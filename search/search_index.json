{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutorials and protocols \u00b6 These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics where they are regularly delivered as in-house workshops. They are designed to be used for self-directed learning. Many of the training materials were developed for use on Galaxy Australia, enabling learners to easily transition from learning to doing their own data analysis.","title":"Home"},{"location":"#tutorials-and-protocols","text":"These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics where they are regularly delivered as in-house workshops. They are designed to be used for self-directed learning. Many of the training materials were developed for use on Galaxy Australia, enabling learners to easily transition from learning to doing their own data analysis.","title":"Tutorials and protocols"},{"location":"guides/bioinfo/","text":"What is bioinformatics? \u00b6 Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites. Getting started \u00b6 Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform. Resources \u00b6 Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published recently in BMC Bioinformatics .","title":"What is Bioinformatics?"},{"location":"guides/bioinfo/#what-is-bioinformatics","text":"Bioinformatics is the analysis of biologial data using computational methods. It includes the study of genes and genomes, RNA, proteins and metabolites.","title":"What is bioinformatics?"},{"location":"guides/bioinfo/#getting-started","text":"Look at our guides on the left for a general introduction to several key topics in bioinformatics. Underneath the guides, there are groups of tutorials for common tasks in bioinformatics. To analyse data, we recommend getting started with Galaxy - a free, web-based bioinformatics platform.","title":"Getting started"},{"location":"guides/bioinfo/#resources","text":"Melbourne Bioinformatics tutorials: see the left hand panel for tutorials covering computing, genomics, RNA-seq, and proteomics. Australian Bioinformatics and Computational Biology Society: www.abacbs.org Student group of ABACBS - COMBINE: www.combine.org.au Winter School in Mathematical and Computational Biology: www.bioinformatics.org.au BioInfo Summer: www.bis.amsi.org.au Software Carpentry: www.software-carpentry.org Bioinformatics problem solving challenges: www.rosalind.info A selelection of key journal papers published recently in BMC Bioinformatics .","title":"Resources"},{"location":"guides/galaxy/","text":"The Galaxy Platform \u00b6 Galaxy is a web platform for bioinformatics analysis. Which Galaxy should I use? \u00b6 There are many different Galaxy servers - each one has a different web address. For researchers based in Australia, we recommend you use Galaxy Australia . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server, you need to register and log in separately for each server. They don\u2019t talk to each other. Tutorials \u00b6 Galaxy Australia tutorials: https://galaxy-au-training.github.io/tutorials/ The Galaxy Training Network also hosts a large collection of useful training material: http://galaxyproject.github.io/training-material/","title":"Introduction to the Galaxy Platform"},{"location":"guides/galaxy/#the-galaxy-platform","text":"Galaxy is a web platform for bioinformatics analysis.","title":"The Galaxy Platform"},{"location":"guides/galaxy/#which-galaxy-should-i-use","text":"There are many different Galaxy servers - each one has a different web address. For researchers based in Australia, we recommend you use Galaxy Australia . The main worldwide Galaxy server is https://usegalaxy.org/ Other Galaxy servers are listed here . Check the policy for the server you are using so that you know how long your data will be kept. If you use more than one Galaxy server, you need to register and log in separately for each server. They don\u2019t talk to each other.","title":"Which Galaxy should I use?"},{"location":"guides/galaxy/#tutorials","text":"Galaxy Australia tutorials: https://galaxy-au-training.github.io/tutorials/ The Galaxy Training Network also hosts a large collection of useful training material: http://galaxyproject.github.io/training-material/","title":"Tutorials"},{"location":"includes/connecting/","text":"Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: 1 2 3 The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer.","title":"Connecting"},{"location":"tutorials/Genome_browsers/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/Genome_browsers/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/","text":"Introduction to Genome Browsers \u00b6 Anticipated workshop duration when delivered to a group of participants is 4 hours . Note that not all the exercises are expected to be completed during the workshop. For queries relating to this workshop, contact Melbourne Bioinformatics at: bioinformatics-training@unimelb.edu.au . Overview \u00b6 This tutorial will introduce you to the genome browser format and illustrate how some freely available genome browsers can be used to interrogate a variety of data types, such as gene expression, genomic variation, methylation and many more. Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with no previous experience of using Genome Browsers and no programming experience. Description \u00b6 Learn how to make the most of Genome Browsers ! By focusing on gene expression, this hands on tutorial will provide beginners with an introduction to both the UCSC Genome browser and IGV (Integrated Genome Viewer). Tools and public datasets will be used to illustrate how the expression of transcript variants can be investigated in different, tissues and cell types using public data, including human RNAseq data from GTEX and mouse cell type RNAseq data from Tabula Muris, as viewed within the UCSC genome browser. A subset of Single cell RNAseq data from the Allen Brain Atlas Celltax study will also be downloaded from SRA and visualised in IGV. The data and genes used in this workshop are taken from the neuroscience field, however the analysis approaches and tools illustrated can be applied to many research areas. This tutorial is in three parts: Section 1 Introduction to the general features of genome browsers. Section 2 Hands on tutorial of the UCSC Genome Browser Section 3 Hands on tutorials of the Integrative Genomics Viewer This tutorial was developed for use as part a series of workshops for neuroscience researchers, hence the data and example genes are drawn from neuroscience field and focused on analysis and visualisation of expression data. However, the skills taught in this tutorial are applicable to all areas of research. Data: GTEX and Tabular Muris data as represented in the UCSC Genome Browser, and Celltax single cell expression atlas data downloaded from SRA Tools: UCSC genome Browser , Integrative Genomics Viewer Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the how some types of genomics and expression data are represented in Genome browsers. Understand gene models and identify differences between transcripts variants. Determine the tissue/cell type expression profiles of a gene of interest in mouse and human expression data. Know some basic files types used in Genome browsers and upload and view local BAM files. Use the \u2018Blat\u2019 tool to locate genomic regions with similarity to a sequence of interest. Create custom interactive views with multiple datatypes to share with colleagues and generate images for publications. Requirements and preparation \u00b6 Attendees are required to provide their own laptop computers. If delivered as a workshop, participants should install the software and data files below prior to the workshop. Ensure that you provide sufficient time to liaise with your own IT support should you encounter any IT problems with installing software. Unless stated otherwise, recommended browsers are Firefox or Chrome. Preparing you and your laptop prior to starting this workshop \u00b6 Required software: Download and install IGV (Free) Ensure that ( Chrome or FireFox are installed and upto date) Create a user account in the UCSC genome browser . Required data is downloaded as part of the tutorial exercises. Required Data \u00b6 No additional data needs to be downloaded prior to this workshop. Mode of Delivery \u00b6 This workshop will be run using freely available Web interfaces and free software using graphical user interfaces. see above. Author(s) and review date \u00b6 Written by: Victoria Perreau | Melbourne Bioinformatics, University of Melbourne. Created: October 2020 Reviewed and revised: June 2021 Genome Browser background \u00b6 Genome browsers are invaluable for viewing and interpreting the many different types of data that can be anchored to genomic positions. These include variation, transcription, the many types regulatory data such as methylation and transcription factor binding, and disease associations. The larger genome browsers serve as data archives for valuable public datasets facilitating visualisation and analysis of different data types. It is also possible to load your own data into some of the public genome browsers. By enabling viewing of one type of data in the context of another, the use of Genome browsers can reveal important information about gene regulation in both normal development and disease, assist hypothesis development relating to genotype phenotype relationships. All researchers are therefore encouraged to become familiar with the use of some of the main browsers such as: The UCSC Genome Browser , (RRID:SCR_005780) ESEMBL Genome Browser , (RRID:SCR_013367) Epigenome browser at WashU , (RRID:SCR_006208) Integrative Genomics Viewer (IGV) , RRID:SCR_011793). They are designed for use by researchers without programming experience and the developers often provide extensive tutorials and cases studies demonstrating the myriad of ways in which data can be loaded and interpreted to assist in develop and supporting your research hypothesis. Many large genomic projects also incorporate genome browsers into their web portals to enable users to easily search and view the data. These include: GTEx gnomAD BDNF and TrkB signalling \u00b6 This tutorial uses the a well known and important signalling pathway in the central nervous system (CNS) to illustrate some of the Genome browser tools and utility. Brain Derived Neurotrophic factor (BDNF) protein is an important neurotrophin responsible for regulating many aspects of growth and development in different cells within the CNS. TrkB is an important receptor that binds extracellular BDNF and propagates the intracellular signalling response via a tyrosine kinase. This TrkB receptor protein is encoded by the NTRK2 gene. The NRK2 gene expresses a number of different transcript variants in different cell types. The most well studied of these is the full length TrkB receptor referred to as TrkB, which is mainly expressed in neuronal cell types. The other transcript variants all express the same exons encoding the extracellular domain of the receptor (shown in the fugure here in green) but have truncated intracellular domains, which do not include the tyrosine kinase domain and thus activate different signalling pathways upon binding to BDNF. None of these truncated protein products have been well studied, but the most highly expressed receptor variant is known as TrkB-T1, and is known to be highly expressed in astocytes. Since the transcript variants are differently expressed in different cell types within the CNS the NTRK2 gene is a very useful example for exploring cell type specific transcript expression in available public data. Major CNS cell types: Neuron (yellow cell in the image below) Astrocyte Oligodendrocyte Microglia Ependymal Section 1: Introduction to Genome Browsers \u00b6 Genome browsers rely on a common reference genome for each species in order to map data from different sources to the correct location. A consortium has agreed on a common numbering for each position on the genome for each species. However, this position will vary based on the version of the genome, as error correction and updates can change the numbering. Therefore it is very important to know which version of the genome your data of interest is aligned to. The sequence for the human reference genome was accumulated up over many years from sequence data from many different sources and does not represent the sequence of one single person. Instead it is a composite of fragments of the genome from many different people. Also, unlike the human genome which is diploid, the human genome is haploid. That is there is only one copy of each chromosome. It therefore does not reflect the variation on the population, or even the most common variants in the human genome. Exploring variation within human genome is very important and facilitated by genome browsers but not covered in this workshop. Genome Build version number- further reading \u00b6 The Genome reference consortium What does the nomenclature mean? For further info on Human Genome version updates I recommend you look at the updates and blog pages on the UCSC genome browser . Section 2: The UCSC genome Browser interface \u00b6 In this section we will become familiar with the web interface of the UCSC genome browser and explore some of the tools and public datasets available. Explore features of particular chromosomal regions Investigate specific genes as well as collections of genes Search for locations of sequences and markers Retrieve annotation information for specific regions or genome-wide View your own data in context of other annotations Compare a region of one genome to genomes of other species Weekly maintenance of the browser is at 5-6 pm Thursdays Pacific time, which is equivalent to 11am-12pm AEST time. During this time the browser may be down for a few minutes. To ensure uninterrupted browser services for your research during UCSC server maintenance and power outages, bookmark one of the mirror sites that replicates the UCSC genome browser. Accessing the tools : Many of the tools that we will explore can be selected via multiple different routes within the browser interface. One way to access many tools is via from the top toolbar on a pull down list, other tools can be accessed from within the browser window. In the following instructions a series of blue boxes is used to indicate successive lower levels from the pull down menu when starting with the top toolbar. For example, the notation below indicates that you should select \u2018Genome Browser\u2019 from the top tool bar and then click on \u2018Reset all user settings\u2019. Toolbar Genome Browser Reset all user settings Accessing help and training : This workshop the UCSC genome browser is supported by rich training resource which has new material added regularly youtube channel . To access training to further develop your skills and go to: Toolbar Help Training Getting started \u00b6 Open the Browser interface: Navigate to the UCSC genome Browser and sign in if you have an account. First reset the browser, so that we all see the same screen: Toolbar Genome Browser Reset all user settings Select and open the human Genome Hg38 at the default position, there are a few different ways to do this Toolbar Genomes (this takes you to the Genome gateway page) Check that GRCh38 is selected in \u2018human assembly\u2019 and click on the blue GO box Toolbar Genomes Human GRCh38hg38 (takes you directly to the genome) You should see this screen, opening at a position on the X chromosome of Human genome version GRCh38 showing the gene model for the ACE2 gene. Familiarise yourself with the main areas of the interface and locate: The main Toolbar Blue bar track collections (data of similar types are collected together under the same \u2018Blue bar\u2019 heading). Scroll down to see additional data collections and which ones are turned on as default. Genome species and version number Position box Navigation tool buttons Chromosome ideogram Genome view window Pre loaded tracks, track titles: The grey bars on the left of the genome view can be used for selecting and configuring the tracks. You can change the order of the tracks by dragging these grey bars up and down. Turn tracks on and off: You can hide tracks by right clicking on the grey bar or by turning them off in the Blue bar collections. You have to click on a \u2018refresh\u2019 button to the changes to be reflected in the the genome view window. View the configuration page specific to a track. The configuration page gives you a lot of information about the data track and its colouring. You can open the configuration page for a track by: clicking on the grey bar for the track or, clicking on the track title in the Blue bar collection. More information and options is usually available by selecting the configuration page for a track via the track title in the Blue Bar collections. Select white \u2018resize\u2019 button to fit the genome view window to your screen Customise your view by using the \u2018Configure\u2019 tool to change the font size to 12. Use either method below to open the Configure tool. Toolbar View Configure browser text size 12 submit or start by clicking on white configure button below the genome view window. Practice navigating around the genome view. move left and right both the navigation buttons and your mouse zoom in and out using navigation buttons zoom in to a region of interest using \u2018Drag-and-select\u2019 : using your mouse select a region of interest by clicking the ruler (position track) at the very top of the genome view window. This is also how to access the \u2018highlight tool\u2019 which you will use in a later exercise to highlight a region of interest. Click on the down arrow next to the highlight colour to select a different colour. Understanding the gene models \u00b6 NTRK2 \u00b6 First we are going to familiarise ourselves with the gene model representation of the different transcripts of NTRK2. Navigate to the NTRK2 gene position in GRCh38 and view the gene models You can navigate to a different region by typing in the position box. If you know the specific location you are interested in type in the location using the format \u201cchr#:1234-1234\u201d. If you have a gene of interest you can type in the gene name (eg: NTRK2). Note the autocompleted suggestions that appear when you start typing. You can select from one of the suggestions or click go and select from a wider range of options. Type (or copy and paste) NTRK2 or chr9:84,665,760-85,030,334 into the position box. Hide all tracks by selecting the white button below the genome view. Turn on only the \u2018Genecode v32 Genemodels\u2019 in \u2018full\u2019 viewing mode by selecting from the blue bar group labelled \u2018Genes and Gene predictions\u2019. Turn on \u2018Conservation\u2019 track to \u2018full\u2019 Dont forget to click refresh . When you have navigated to the NTRK2 gene, zoom out until you can view all of the 5\u2019 UTRs and 3\u2019 UTRs for all transcript variants for this gene. Then drag the view left and right to center (like in Google maps) or \u2018drag and select\u2019 the region to center the gene in the Genome view. You should see something like the image below. Which strand is the gene encoded on / transcribed from? (+ or - strand) Identify the exons, introns and UTRs Do regions of conservation only occur were there are coding regions? How many different transcripts variants are there for this gene? How do they differ? Select a coding region (full height boxes) towards the 3\u2019UTR of the gene. zoom in to the region until you can see the letters of the amino acid sequence. Why are some amino acid boxes red or green? Zoom in again until you can see each amino acid number. Why do different transcripts have different amino acid numbers? Note that one of the transcript names is in white text with a black background, this is the transcript you selected from the autocompleted list or the search results. Change the \u2018view settings\u2019 for the track. Switch between dense squish pack full to see how it changes the representation of the models. Right click on the track grey bar in the left of the genome window to access view settings. Go to the configuration page for the Gencode v32 track and change the gene names to also reveal the \u2018Gencode transcript ID\u2019 in the label. The transcript names are now too long to fit on the screen. Go to the genome view configuration page (like you did to chane the font size at the beginning of the workshop) and change the number of characters in the label so that you can see the entire transcript label. Test your understanding of gene model representation by attempting this 6 questions in this quiz . BDNF \u00b6 Now we look at the gene model for BDNF in the same genome. There are some differences that enable us to demonstrate some more tools. Navigate to the BDNF gene position in GRCh38 and view the gene models Note that there are black transcript models encoded on the + strand and blue BDNS-AS transcript models on the - strand. BDNF-AS is the antisense gene. Colouring info for a track can be obtained form the configuration page, below if the colouring for this track. Since the convention is to display genes in the 5\u2019 to 3\u2019 orientation it can be useful for our own interpretation, and also for presentation purposes, to flip the orientation of a gene when viewing it in a Genome Browser. Use the white reverse button under the genome view window to flip the orientation of the gene. When a gene are many large introns taking up a lot of white space in an image it can be difficult to see if exons in different transcript models or other data tracks align. The \u2018Multi-Region\u2019 view tool can be used to fold the intronic regions in the view out of the view like a concertina. The Broswer seletcs which regiosn to fold out based on the gene model track(s) that you have turned on at the time. Apply the Multi-Region view from the mail tool bar of the white buttons under the Genome window. Toolbar View Multi-Region select \u2018Show exons using GENCODE v32\u2019 It is now a lot easier to view a number of interesting features in the BDNF transcript models: The transcript variants for the BDNF vary mostly in the genomic position of the 5\u2019UTR. The noncoding AS-BDNF gene transcript includes a region that would be antisense to the coding BDNF transcript. You may find that using the multi-region tool facilitates visualisation and interpretation of gene expression data later in the workshop. Blat tool exercise \u00b6 The Blat tool is a sequence similarity tool similar to Blast. It can quickly identify region(s) of homology between a genome and a sequence of interest. Due to the presence of orthologs and paralogs a target sequence may have similarity to more than one region in the genome. In this exercise you will use Blat to map the sequences of two different expression probes to their target regions and determine which gene transcripts the probes are likely to detect in an expression study. Microarray expression data is not commonly used now, but some of the data generated from large well orchestrated studies still provide valuable information to researchers. Microarray probes, like in situ hybridisation probes, target a small region of the RNA and do not measure the whole RNA transcript. If you are measuring gene expression it is important to know exactly which region of the gene you are detecting. IN this exercise we will employ the blat tool to determine which region of the NTRK2 gene the microarray probes in the following study are detecting. The study was the Human Brain gene expression atlas generated by the Allen Institute . Below are sequences of two hybridisation probes that were use in a microarray used to detect expression of the gene NTRK2. These two probes result in very different hybridisation and expression patterns across different regions of the brain. As we observed in the exercise above NTRK2 has a number of different transcript variants. The question we have is whether these probes are detecting different or multiple transcripts of NTRK2, and if so which ones? NTRK2 Probe A_23_P216779 sequence: TTCTATACTCTAATCAGCACTGAATTCAGAGGGTTTGACTTTTTCATCTATAACACAGTG Z score of expression level in Human brain (blue = low expression, red = high expression) NTRK2 Probe A_24_P343559 sequence AAGCTGCTCTCCTTCACTCTGACAGTATTAACATCAAAGACTCCGAGAAGCTCTCGAGGG Z score of expression level in Human brain (blue = low expression, red = high expression) The images above are of one of the six donors included in the atlas, and typical of the expression pattern for NTRK2. These images are taken from the NTRK2 gene page of Human Brain Atlas . Most obvious in the images above is the high level of expression signal using Probe A_23_P216779 and low level for A_24_P343559 in the corpus callosum (CC) which is a region of white matter in the brain with relatively few neurons and relatively high proportion of myelinating oligodendrocytes. This expression profile is reversed in the the cortical regions, eg. frontal lobe (FL) and parietal lobe (PL), which have a relatively high density of neuronal cells. Use Blat tool to find region of homology Select: Toolbar tools blat Copy the sequence of the first probe above and paste into the search box Select the human GRCh38 for \u2018Assembly\u2019 and click Submit A_23_P216779 returns 2 hits for different chromosomes. One of these has 100% homology over the whole 60 base sequence, the other has 87% homology over a 24 base region. Copy and paste the probe name to use as the label for the \u2018Custom track name\u2019 and \u2018Custon track description\u2019 and click on Build a custom track with these results . It\u2019s not necessary to build a custom track, you could simply click on the browser link to view the results, but creating a \u2018custom track\u2019 from the blat result enables you to give it a unique name. This is important if you are doing multiple blat searches as you wontg be able to tell which one is which. Select browser option for the hit with the highest homology to view the result. Repeat for the other probe sequence. It is easy to loose track of a region you are investigating when navigating around the genome. So we are going to highlight each region of probe homology within the NTRK2 gene, using a different colour for each probe. Highlight is also useful if you have lots of different tracks loaded and you want to check that a feature on one track lines up with another. Use the \u2018highlight\u2019 tool to keep track of region of interest in the Genome view Using your mouse select in the position track at the top to activate \u2018Drag-and-select\u2019. Select only the region of homology for each probe within the NTRK2 gene and use a different highlight for each region. Then zoom out to view the whole gene again. Do the probes detect coding regions of the NTRK2 gene? Do the probes detect different transcripts? Use \u2018Multiregion view\u2019 to make it easier to compare coding regions of different transcripts Toolbar View Multi-Region select \u2018Show exons using GENCODE v32\u2019 I have created a \u2018public session\u2019 of the Blat NTRK2 exercise you can view it from the link in the sessions Toolbar My data Public session search for \u201chg38_NTRK2_blat_probes\u201d . Gene expression data \u00b6 Human tissue specific expression data from the GTEX project is available in UCSC genome browser Gene level expression data from GTEx V6 (570) donors, and GTEx V8 (948) donors can be turned on from Blue bar title for more detailed configuration page. These are displayed as coloured bar plots. Transcript level data is also available for GTEx V6, this is also displayed as bar plots. Transcript level expression data for GTEx V6 is available as coverage plots and is accessed from the \u2018Track hubs\u2019 . click on the white \u2018track hubs\u2019 under the genome view window or access from the toolbar under \u2018My data\u2019. scroll down and select \u2018GTEx RNA-seq Signal Hub\u2019. This can be viewed in either hg19 or hg38 so can be compared with a wide variety of other datasets. The default settings are: that all the available data from one individual is loaded. Other samples from other subjects in the study can be loaded. For example you could load all available samples for one tissue region only. the data is \u2018autoscale to data view\u2019 with a track height or 12 pixels for each samples. You can change the height of the track or add a transformation. you can also change it from a points plot into a density plot by clicking on the track title. The FACS derived data from the Tabular Muris cell type data can be visualised as a coverage plot Start at the view of the NTRK2 gene in the human genome and navigate to the Ntrk2 gene in the mouse genome using the \u2018View in other genomes tool\u2019. Toolbar View In Other Genomes,/ss> (Convert) select New Genome:Mouse , New Assembly:GRC38/mm10, click on \u2018Submit\u2019 select the region with the greatest homology Configure the Tabular Muris track by selecting it from the blue bar collection. Hide \u2018Cell expression\u2019 Select \u2018Genome coverage\u2019 to full Select \u2018submit\u2019 This can look like a bit too much data to manage as there are very many tracks and the default track height is set very high. But its easy to simplify it by focusing on cell types of interest. Right click on the grey bar to \u2018configure the track set\u2019. Change track height to 30 for \u2018data view scaling\u2019 select group autoscale clear all the subtracks and then manually select only a few cell types of interest: astrocyte Cv Bergmann glial Cv microglia Cv neuron Cv oligodendrocyte Cv OPC Cv Which cell type has the highest level expression in this dataset? Change the \u2018Data view scaling\u2019 to autoscale to dataview. Export a PDF image of the genome view: Toolbar View PDF/PS select \u2018Download the current browser graphic in PDF\u2019 Which cell type(s) express the long and short transcripts for NTRK2? Mouse CNS cell type expression data can also be validated using an independent single cell dataset of mouse cortex from the Linnarsson lab . The data that is publicly available for viewing in the UCCS genome browser but is not housed in the UCSC genome browser. You must first access it from the the Linnarsson lab data page. This RNAseq data is stranded, meaning you can see if the transcript data is from the + or - strand. Go to the Public data page where you can search for cell expression profiles for individual genes. Click on the \u2018Browse the genome\u2019 blue text near the bottom of the page. This loads 18 different tracks, one for each cell type. The default setting for expression range is quite high and most gene expression cannot be visualised with these settings. Each track must be configured individually rather than as a group, which takes a lot of time. I have created a version of this data as a public session in the UCSC genome browser. Where each track is autoscaled which can make it quicker to determine which expression range would be ideal for visualising the expression of an individual gene. The data is also viewed using \u2018Multi-Region\u2019 which hides the introns in the gene models. THe sessopn is illustrated in a screen shot below and you can access this custom track set by clicking on the title in blue text. Linnarsson lab mouse cortex single cell data as autoscaled datatracks As an exercise, select 2 or three cell types and adjust the scale to best reflect differences in gene expression of Ntrk2 between these cells. Save this session and share it. Section 3: IGV \u00b6 In this section we will download a BAM file of gene expression data from SRA and view it in the Integrated Genome Viewer (IGV). BAM files must first be sorted and indexed before they can be loaded into genome viewers and IGV has tools to do this without having to use command line. The express ion data we are using for this exercise is from the mouse Celltax single cell expression atlas published by the Allen Brain Institute. The cell tax vignette has an expression browser that displays gene level expression as a heat map for any gene of interest, The readsets (fastq files) and aligned data (BAM files) for 1809 runs on single cells are also available for down load from SRA. The SRA study ID for this study is SRP061902 and individual runs from this study are easily selected by viewing the samples in the \u2018RunSelector\u2019. If you wish to identify particular cell types of interest. For this exercise I have already identified a few samples that we will download in order to illustrate navigating in IGV by looking at the expression of NTRK2 in the same cell types we have discussed in earlier exercises. For each cell type we will down load a .BAM file containing only the reads from the chromosome of interest. For each SRA run in the table below open the link to the run to down load the data. Not many raw data sets in SRA have aligned data available for down load but this data set does. Cell type SRA run Vignette Cell ID astrocyte SRR2138962 D1319_V astrocyte SRR2139935 A1643_VL neuron SRR2139989 S467_V4 neuron SRR2140047 S1282_V Download BAM files from SRA Click on the \u2018Alignment\u2019 tab Note that the data is aligned to the mouse GRCm38 genome (mm10). Select the chromosome of interest. For NTRK2 in mouse it is chr13 For \u2018Output this run in:\u2019 select BAM and click on \u2018format to:\u2019 File Rename the downloaded file to include the cell type, to avoid confusion. eg: SRR2138661_astrocyte_chr13.bam Use IGV tools to SORT and INDEX the BAM files Store sorted BAM files and index files in the same folder. Open IGV and select Tools / Run igvtools\u2026 from the pull town menus. Select \u2018Sort\u2019 from the Command options and use the brows options to select the BAM file you just downloaded and click \u2018Run\u2019 Without closing the igvtools window now select the command \u2018Index\u2019 and Browse to find the BAM file you just sorted. It will have the same file name with \u2018sorted\u2019 added to the end. eg SRR2138661_astrocyte_chr13.sorted.bam The resulting index file will have the file name : SRR2138661_astrocyte_chr13.sorted.bam.bai It is essential that the index file for a BAM file has the same name and is located in the same folder as its BAM file. If not the IVG software will not be able to open the BAM file. View the BAM files in IGV Select the Mouse (mm10) genome from the genome box in the top right hand corner. Select File / Load from File\u2026 and select all 4 \u2018_chr13.sorted.bam\u2019 files only (use command to select more then one file at a time). select open - but don\u2019t expect to see any data yet. The genome view window opens on a whole chromosome view as default but it wont show any data until the view region is small enough to show all data in the current view. Type the gene name \u2018NTRK2\u2019 into the search window. Expand the Refseq gene model track by right clicking it to see all the splice variants The gene and thus the genome view is 328kb and the default setting for viewing data is only 100kb. So unless you have already changed your settings alignment data will not get be showing. zoom into the region of a coding exon by selecting in the numbered location track at the top of the genome view. To see the whole gene in the genome window at the same time you may need to change the preferences. Go to View / Preferences and select the \u2018Alignments tab\u2019. Change the visibility range threshold to 400kb. You may need to change this back to a smaller range in the future if you are working with large datasets and/or small amounts of memory on your computer. Export images The Genome view above can be exported by selecting \u2018File / Save image\u2026\u2019 from the tool bar. To export the Sashimi plot below: Right click on one of the junction tracks and select \u2018Sashimi Plot\u2019 from the poll down menu. Select the tracks you want in your final image. There are some data filtering and style adjustments you can make to the Sashimi plot. Right click on each track to access the menu options. Some changes apply to each track individually and some to all tracks. Download and install the Gencode gene model annotation track The refseq gene model track is not as comprehensive as GeneGode gene models. For both Human and Mouse the Gencode gene model gtf annotaion files can be downloaded form Gencode . If you wish to do this be aware that it takes a little time and is not done as part of a workshop. Create a folder called \u2018annotations/Mouse\u2019 in the main \u2018igv\u2019 folder that was installed on your computer when you downloaded IGV. Download the GTF file from the link above and save it in this folder. Unpack and then SORT and INDEX the .gtf file using igvtools. in IGV, before you load you data files, load this annotation file and it will replace the refseq one. Additional reading \u00b6 IGV https://rockefelleruniversity.github.io/IGV_course/presentations/singlepage/IGV.html","title":"Introduction to Genome Browsers"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#introduction-to-genome-browsers","text":"Anticipated workshop duration when delivered to a group of participants is 4 hours . Note that not all the exercises are expected to be completed during the workshop. For queries relating to this workshop, contact Melbourne Bioinformatics at: bioinformatics-training@unimelb.edu.au .","title":"Introduction to Genome Browsers"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#overview","text":"This tutorial will introduce you to the genome browser format and illustrate how some freely available genome browsers can be used to interrogate a variety of data types, such as gene expression, genomic variation, methylation and many more.","title":"Overview"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with no previous experience of using Genome Browsers and no programming experience.","title":"Skill level"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#description","text":"Learn how to make the most of Genome Browsers ! By focusing on gene expression, this hands on tutorial will provide beginners with an introduction to both the UCSC Genome browser and IGV (Integrated Genome Viewer). Tools and public datasets will be used to illustrate how the expression of transcript variants can be investigated in different, tissues and cell types using public data, including human RNAseq data from GTEX and mouse cell type RNAseq data from Tabula Muris, as viewed within the UCSC genome browser. A subset of Single cell RNAseq data from the Allen Brain Atlas Celltax study will also be downloaded from SRA and visualised in IGV. The data and genes used in this workshop are taken from the neuroscience field, however the analysis approaches and tools illustrated can be applied to many research areas. This tutorial is in three parts: Section 1 Introduction to the general features of genome browsers. Section 2 Hands on tutorial of the UCSC Genome Browser Section 3 Hands on tutorials of the Integrative Genomics Viewer This tutorial was developed for use as part a series of workshops for neuroscience researchers, hence the data and example genes are drawn from neuroscience field and focused on analysis and visualisation of expression data. However, the skills taught in this tutorial are applicable to all areas of research. Data: GTEX and Tabular Muris data as represented in the UCSC Genome Browser, and Celltax single cell expression atlas data downloaded from SRA Tools: UCSC genome Browser , Integrative Genomics Viewer","title":"Description"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the how some types of genomics and expression data are represented in Genome browsers. Understand gene models and identify differences between transcripts variants. Determine the tissue/cell type expression profiles of a gene of interest in mouse and human expression data. Know some basic files types used in Genome browsers and upload and view local BAM files. Use the \u2018Blat\u2019 tool to locate genomic regions with similarity to a sequence of interest. Create custom interactive views with multiple datatypes to share with colleagues and generate images for publications.","title":"Learning Objectives"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#requirements-and-preparation","text":"Attendees are required to provide their own laptop computers. If delivered as a workshop, participants should install the software and data files below prior to the workshop. Ensure that you provide sufficient time to liaise with your own IT support should you encounter any IT problems with installing software. Unless stated otherwise, recommended browsers are Firefox or Chrome.","title":"Requirements and preparation"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#preparing-you-and-your-laptop-prior-to-starting-this-workshop","text":"Required software: Download and install IGV (Free) Ensure that ( Chrome or FireFox are installed and upto date) Create a user account in the UCSC genome browser . Required data is downloaded as part of the tutorial exercises.","title":"Preparing you and your laptop prior to starting this workshop"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#required-data","text":"No additional data needs to be downloaded prior to this workshop.","title":"Required Data"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#mode-of-delivery","text":"This workshop will be run using freely available Web interfaces and free software using graphical user interfaces. see above.","title":"Mode of Delivery"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#authors-and-review-date","text":"Written by: Victoria Perreau | Melbourne Bioinformatics, University of Melbourne. Created: October 2020 Reviewed and revised: June 2021","title":"Author(s) and review date"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#genome-browser-background","text":"Genome browsers are invaluable for viewing and interpreting the many different types of data that can be anchored to genomic positions. These include variation, transcription, the many types regulatory data such as methylation and transcription factor binding, and disease associations. The larger genome browsers serve as data archives for valuable public datasets facilitating visualisation and analysis of different data types. It is also possible to load your own data into some of the public genome browsers. By enabling viewing of one type of data in the context of another, the use of Genome browsers can reveal important information about gene regulation in both normal development and disease, assist hypothesis development relating to genotype phenotype relationships. All researchers are therefore encouraged to become familiar with the use of some of the main browsers such as: The UCSC Genome Browser , (RRID:SCR_005780) ESEMBL Genome Browser , (RRID:SCR_013367) Epigenome browser at WashU , (RRID:SCR_006208) Integrative Genomics Viewer (IGV) , RRID:SCR_011793). They are designed for use by researchers without programming experience and the developers often provide extensive tutorials and cases studies demonstrating the myriad of ways in which data can be loaded and interpreted to assist in develop and supporting your research hypothesis. Many large genomic projects also incorporate genome browsers into their web portals to enable users to easily search and view the data. These include: GTEx gnomAD","title":"Genome Browser background"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#bdnf-and-trkb-signalling","text":"This tutorial uses the a well known and important signalling pathway in the central nervous system (CNS) to illustrate some of the Genome browser tools and utility. Brain Derived Neurotrophic factor (BDNF) protein is an important neurotrophin responsible for regulating many aspects of growth and development in different cells within the CNS. TrkB is an important receptor that binds extracellular BDNF and propagates the intracellular signalling response via a tyrosine kinase. This TrkB receptor protein is encoded by the NTRK2 gene. The NRK2 gene expresses a number of different transcript variants in different cell types. The most well studied of these is the full length TrkB receptor referred to as TrkB, which is mainly expressed in neuronal cell types. The other transcript variants all express the same exons encoding the extracellular domain of the receptor (shown in the fugure here in green) but have truncated intracellular domains, which do not include the tyrosine kinase domain and thus activate different signalling pathways upon binding to BDNF. None of these truncated protein products have been well studied, but the most highly expressed receptor variant is known as TrkB-T1, and is known to be highly expressed in astocytes. Since the transcript variants are differently expressed in different cell types within the CNS the NTRK2 gene is a very useful example for exploring cell type specific transcript expression in available public data. Major CNS cell types: Neuron (yellow cell in the image below) Astrocyte Oligodendrocyte Microglia Ependymal","title":"BDNF and TrkB signalling"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#section-1-introduction-to-genome-browsers","text":"Genome browsers rely on a common reference genome for each species in order to map data from different sources to the correct location. A consortium has agreed on a common numbering for each position on the genome for each species. However, this position will vary based on the version of the genome, as error correction and updates can change the numbering. Therefore it is very important to know which version of the genome your data of interest is aligned to. The sequence for the human reference genome was accumulated up over many years from sequence data from many different sources and does not represent the sequence of one single person. Instead it is a composite of fragments of the genome from many different people. Also, unlike the human genome which is diploid, the human genome is haploid. That is there is only one copy of each chromosome. It therefore does not reflect the variation on the population, or even the most common variants in the human genome. Exploring variation within human genome is very important and facilitated by genome browsers but not covered in this workshop.","title":"Section 1: Introduction to Genome Browsers"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#genome-build-version-number-further-reading","text":"The Genome reference consortium What does the nomenclature mean? For further info on Human Genome version updates I recommend you look at the updates and blog pages on the UCSC genome browser .","title":"Genome Build version number- further reading"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#section-2-the-ucsc-genome-browser-interface","text":"In this section we will become familiar with the web interface of the UCSC genome browser and explore some of the tools and public datasets available. Explore features of particular chromosomal regions Investigate specific genes as well as collections of genes Search for locations of sequences and markers Retrieve annotation information for specific regions or genome-wide View your own data in context of other annotations Compare a region of one genome to genomes of other species Weekly maintenance of the browser is at 5-6 pm Thursdays Pacific time, which is equivalent to 11am-12pm AEST time. During this time the browser may be down for a few minutes. To ensure uninterrupted browser services for your research during UCSC server maintenance and power outages, bookmark one of the mirror sites that replicates the UCSC genome browser. Accessing the tools : Many of the tools that we will explore can be selected via multiple different routes within the browser interface. One way to access many tools is via from the top toolbar on a pull down list, other tools can be accessed from within the browser window. In the following instructions a series of blue boxes is used to indicate successive lower levels from the pull down menu when starting with the top toolbar. For example, the notation below indicates that you should select \u2018Genome Browser\u2019 from the top tool bar and then click on \u2018Reset all user settings\u2019. Toolbar Genome Browser Reset all user settings Accessing help and training : This workshop the UCSC genome browser is supported by rich training resource which has new material added regularly youtube channel . To access training to further develop your skills and go to: Toolbar Help Training","title":"Section 2: The UCSC genome Browser interface"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#getting-started","text":"Open the Browser interface: Navigate to the UCSC genome Browser and sign in if you have an account. First reset the browser, so that we all see the same screen: Toolbar Genome Browser Reset all user settings Select and open the human Genome Hg38 at the default position, there are a few different ways to do this Toolbar Genomes (this takes you to the Genome gateway page) Check that GRCh38 is selected in \u2018human assembly\u2019 and click on the blue GO box Toolbar Genomes Human GRCh38hg38 (takes you directly to the genome) You should see this screen, opening at a position on the X chromosome of Human genome version GRCh38 showing the gene model for the ACE2 gene. Familiarise yourself with the main areas of the interface and locate: The main Toolbar Blue bar track collections (data of similar types are collected together under the same \u2018Blue bar\u2019 heading). Scroll down to see additional data collections and which ones are turned on as default. Genome species and version number Position box Navigation tool buttons Chromosome ideogram Genome view window Pre loaded tracks, track titles: The grey bars on the left of the genome view can be used for selecting and configuring the tracks. You can change the order of the tracks by dragging these grey bars up and down. Turn tracks on and off: You can hide tracks by right clicking on the grey bar or by turning them off in the Blue bar collections. You have to click on a \u2018refresh\u2019 button to the changes to be reflected in the the genome view window. View the configuration page specific to a track. The configuration page gives you a lot of information about the data track and its colouring. You can open the configuration page for a track by: clicking on the grey bar for the track or, clicking on the track title in the Blue bar collection. More information and options is usually available by selecting the configuration page for a track via the track title in the Blue Bar collections. Select white \u2018resize\u2019 button to fit the genome view window to your screen Customise your view by using the \u2018Configure\u2019 tool to change the font size to 12. Use either method below to open the Configure tool. Toolbar View Configure browser text size 12 submit or start by clicking on white configure button below the genome view window. Practice navigating around the genome view. move left and right both the navigation buttons and your mouse zoom in and out using navigation buttons zoom in to a region of interest using \u2018Drag-and-select\u2019 : using your mouse select a region of interest by clicking the ruler (position track) at the very top of the genome view window. This is also how to access the \u2018highlight tool\u2019 which you will use in a later exercise to highlight a region of interest. Click on the down arrow next to the highlight colour to select a different colour.","title":"Getting started"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#understanding-the-gene-models","text":"","title":"Understanding the gene models"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#ntrk2","text":"First we are going to familiarise ourselves with the gene model representation of the different transcripts of NTRK2. Navigate to the NTRK2 gene position in GRCh38 and view the gene models You can navigate to a different region by typing in the position box. If you know the specific location you are interested in type in the location using the format \u201cchr#:1234-1234\u201d. If you have a gene of interest you can type in the gene name (eg: NTRK2). Note the autocompleted suggestions that appear when you start typing. You can select from one of the suggestions or click go and select from a wider range of options. Type (or copy and paste) NTRK2 or chr9:84,665,760-85,030,334 into the position box. Hide all tracks by selecting the white button below the genome view. Turn on only the \u2018Genecode v32 Genemodels\u2019 in \u2018full\u2019 viewing mode by selecting from the blue bar group labelled \u2018Genes and Gene predictions\u2019. Turn on \u2018Conservation\u2019 track to \u2018full\u2019 Dont forget to click refresh . When you have navigated to the NTRK2 gene, zoom out until you can view all of the 5\u2019 UTRs and 3\u2019 UTRs for all transcript variants for this gene. Then drag the view left and right to center (like in Google maps) or \u2018drag and select\u2019 the region to center the gene in the Genome view. You should see something like the image below. Which strand is the gene encoded on / transcribed from? (+ or - strand) Identify the exons, introns and UTRs Do regions of conservation only occur were there are coding regions? How many different transcripts variants are there for this gene? How do they differ? Select a coding region (full height boxes) towards the 3\u2019UTR of the gene. zoom in to the region until you can see the letters of the amino acid sequence. Why are some amino acid boxes red or green? Zoom in again until you can see each amino acid number. Why do different transcripts have different amino acid numbers? Note that one of the transcript names is in white text with a black background, this is the transcript you selected from the autocompleted list or the search results. Change the \u2018view settings\u2019 for the track. Switch between dense squish pack full to see how it changes the representation of the models. Right click on the track grey bar in the left of the genome window to access view settings. Go to the configuration page for the Gencode v32 track and change the gene names to also reveal the \u2018Gencode transcript ID\u2019 in the label. The transcript names are now too long to fit on the screen. Go to the genome view configuration page (like you did to chane the font size at the beginning of the workshop) and change the number of characters in the label so that you can see the entire transcript label. Test your understanding of gene model representation by attempting this 6 questions in this quiz .","title":"NTRK2"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#bdnf","text":"Now we look at the gene model for BDNF in the same genome. There are some differences that enable us to demonstrate some more tools. Navigate to the BDNF gene position in GRCh38 and view the gene models Note that there are black transcript models encoded on the + strand and blue BDNS-AS transcript models on the - strand. BDNF-AS is the antisense gene. Colouring info for a track can be obtained form the configuration page, below if the colouring for this track. Since the convention is to display genes in the 5\u2019 to 3\u2019 orientation it can be useful for our own interpretation, and also for presentation purposes, to flip the orientation of a gene when viewing it in a Genome Browser. Use the white reverse button under the genome view window to flip the orientation of the gene. When a gene are many large introns taking up a lot of white space in an image it can be difficult to see if exons in different transcript models or other data tracks align. The \u2018Multi-Region\u2019 view tool can be used to fold the intronic regions in the view out of the view like a concertina. The Broswer seletcs which regiosn to fold out based on the gene model track(s) that you have turned on at the time. Apply the Multi-Region view from the mail tool bar of the white buttons under the Genome window. Toolbar View Multi-Region select \u2018Show exons using GENCODE v32\u2019 It is now a lot easier to view a number of interesting features in the BDNF transcript models: The transcript variants for the BDNF vary mostly in the genomic position of the 5\u2019UTR. The noncoding AS-BDNF gene transcript includes a region that would be antisense to the coding BDNF transcript. You may find that using the multi-region tool facilitates visualisation and interpretation of gene expression data later in the workshop.","title":"BDNF"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#blat-tool-exercise","text":"The Blat tool is a sequence similarity tool similar to Blast. It can quickly identify region(s) of homology between a genome and a sequence of interest. Due to the presence of orthologs and paralogs a target sequence may have similarity to more than one region in the genome. In this exercise you will use Blat to map the sequences of two different expression probes to their target regions and determine which gene transcripts the probes are likely to detect in an expression study. Microarray expression data is not commonly used now, but some of the data generated from large well orchestrated studies still provide valuable information to researchers. Microarray probes, like in situ hybridisation probes, target a small region of the RNA and do not measure the whole RNA transcript. If you are measuring gene expression it is important to know exactly which region of the gene you are detecting. IN this exercise we will employ the blat tool to determine which region of the NTRK2 gene the microarray probes in the following study are detecting. The study was the Human Brain gene expression atlas generated by the Allen Institute . Below are sequences of two hybridisation probes that were use in a microarray used to detect expression of the gene NTRK2. These two probes result in very different hybridisation and expression patterns across different regions of the brain. As we observed in the exercise above NTRK2 has a number of different transcript variants. The question we have is whether these probes are detecting different or multiple transcripts of NTRK2, and if so which ones? NTRK2 Probe A_23_P216779 sequence: TTCTATACTCTAATCAGCACTGAATTCAGAGGGTTTGACTTTTTCATCTATAACACAGTG Z score of expression level in Human brain (blue = low expression, red = high expression) NTRK2 Probe A_24_P343559 sequence AAGCTGCTCTCCTTCACTCTGACAGTATTAACATCAAAGACTCCGAGAAGCTCTCGAGGG Z score of expression level in Human brain (blue = low expression, red = high expression) The images above are of one of the six donors included in the atlas, and typical of the expression pattern for NTRK2. These images are taken from the NTRK2 gene page of Human Brain Atlas . Most obvious in the images above is the high level of expression signal using Probe A_23_P216779 and low level for A_24_P343559 in the corpus callosum (CC) which is a region of white matter in the brain with relatively few neurons and relatively high proportion of myelinating oligodendrocytes. This expression profile is reversed in the the cortical regions, eg. frontal lobe (FL) and parietal lobe (PL), which have a relatively high density of neuronal cells. Use Blat tool to find region of homology Select: Toolbar tools blat Copy the sequence of the first probe above and paste into the search box Select the human GRCh38 for \u2018Assembly\u2019 and click Submit A_23_P216779 returns 2 hits for different chromosomes. One of these has 100% homology over the whole 60 base sequence, the other has 87% homology over a 24 base region. Copy and paste the probe name to use as the label for the \u2018Custom track name\u2019 and \u2018Custon track description\u2019 and click on Build a custom track with these results . It\u2019s not necessary to build a custom track, you could simply click on the browser link to view the results, but creating a \u2018custom track\u2019 from the blat result enables you to give it a unique name. This is important if you are doing multiple blat searches as you wontg be able to tell which one is which. Select browser option for the hit with the highest homology to view the result. Repeat for the other probe sequence. It is easy to loose track of a region you are investigating when navigating around the genome. So we are going to highlight each region of probe homology within the NTRK2 gene, using a different colour for each probe. Highlight is also useful if you have lots of different tracks loaded and you want to check that a feature on one track lines up with another. Use the \u2018highlight\u2019 tool to keep track of region of interest in the Genome view Using your mouse select in the position track at the top to activate \u2018Drag-and-select\u2019. Select only the region of homology for each probe within the NTRK2 gene and use a different highlight for each region. Then zoom out to view the whole gene again. Do the probes detect coding regions of the NTRK2 gene? Do the probes detect different transcripts? Use \u2018Multiregion view\u2019 to make it easier to compare coding regions of different transcripts Toolbar View Multi-Region select \u2018Show exons using GENCODE v32\u2019 I have created a \u2018public session\u2019 of the Blat NTRK2 exercise you can view it from the link in the sessions Toolbar My data Public session search for \u201chg38_NTRK2_blat_probes\u201d .","title":"Blat tool exercise"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#gene-expression-data","text":"Human tissue specific expression data from the GTEX project is available in UCSC genome browser Gene level expression data from GTEx V6 (570) donors, and GTEx V8 (948) donors can be turned on from Blue bar title for more detailed configuration page. These are displayed as coloured bar plots. Transcript level data is also available for GTEx V6, this is also displayed as bar plots. Transcript level expression data for GTEx V6 is available as coverage plots and is accessed from the \u2018Track hubs\u2019 . click on the white \u2018track hubs\u2019 under the genome view window or access from the toolbar under \u2018My data\u2019. scroll down and select \u2018GTEx RNA-seq Signal Hub\u2019. This can be viewed in either hg19 or hg38 so can be compared with a wide variety of other datasets. The default settings are: that all the available data from one individual is loaded. Other samples from other subjects in the study can be loaded. For example you could load all available samples for one tissue region only. the data is \u2018autoscale to data view\u2019 with a track height or 12 pixels for each samples. You can change the height of the track or add a transformation. you can also change it from a points plot into a density plot by clicking on the track title. The FACS derived data from the Tabular Muris cell type data can be visualised as a coverage plot Start at the view of the NTRK2 gene in the human genome and navigate to the Ntrk2 gene in the mouse genome using the \u2018View in other genomes tool\u2019. Toolbar View In Other Genomes,/ss> (Convert) select New Genome:Mouse , New Assembly:GRC38/mm10, click on \u2018Submit\u2019 select the region with the greatest homology Configure the Tabular Muris track by selecting it from the blue bar collection. Hide \u2018Cell expression\u2019 Select \u2018Genome coverage\u2019 to full Select \u2018submit\u2019 This can look like a bit too much data to manage as there are very many tracks and the default track height is set very high. But its easy to simplify it by focusing on cell types of interest. Right click on the grey bar to \u2018configure the track set\u2019. Change track height to 30 for \u2018data view scaling\u2019 select group autoscale clear all the subtracks and then manually select only a few cell types of interest: astrocyte Cv Bergmann glial Cv microglia Cv neuron Cv oligodendrocyte Cv OPC Cv Which cell type has the highest level expression in this dataset? Change the \u2018Data view scaling\u2019 to autoscale to dataview. Export a PDF image of the genome view: Toolbar View PDF/PS select \u2018Download the current browser graphic in PDF\u2019 Which cell type(s) express the long and short transcripts for NTRK2? Mouse CNS cell type expression data can also be validated using an independent single cell dataset of mouse cortex from the Linnarsson lab . The data that is publicly available for viewing in the UCCS genome browser but is not housed in the UCSC genome browser. You must first access it from the the Linnarsson lab data page. This RNAseq data is stranded, meaning you can see if the transcript data is from the + or - strand. Go to the Public data page where you can search for cell expression profiles for individual genes. Click on the \u2018Browse the genome\u2019 blue text near the bottom of the page. This loads 18 different tracks, one for each cell type. The default setting for expression range is quite high and most gene expression cannot be visualised with these settings. Each track must be configured individually rather than as a group, which takes a lot of time. I have created a version of this data as a public session in the UCSC genome browser. Where each track is autoscaled which can make it quicker to determine which expression range would be ideal for visualising the expression of an individual gene. The data is also viewed using \u2018Multi-Region\u2019 which hides the introns in the gene models. THe sessopn is illustrated in a screen shot below and you can access this custom track set by clicking on the title in blue text. Linnarsson lab mouse cortex single cell data as autoscaled datatracks As an exercise, select 2 or three cell types and adjust the scale to best reflect differences in gene expression of Ntrk2 between these cells. Save this session and share it.","title":"Gene expression data"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#section-3-igv","text":"In this section we will download a BAM file of gene expression data from SRA and view it in the Integrated Genome Viewer (IGV). BAM files must first be sorted and indexed before they can be loaded into genome viewers and IGV has tools to do this without having to use command line. The express ion data we are using for this exercise is from the mouse Celltax single cell expression atlas published by the Allen Brain Institute. The cell tax vignette has an expression browser that displays gene level expression as a heat map for any gene of interest, The readsets (fastq files) and aligned data (BAM files) for 1809 runs on single cells are also available for down load from SRA. The SRA study ID for this study is SRP061902 and individual runs from this study are easily selected by viewing the samples in the \u2018RunSelector\u2019. If you wish to identify particular cell types of interest. For this exercise I have already identified a few samples that we will download in order to illustrate navigating in IGV by looking at the expression of NTRK2 in the same cell types we have discussed in earlier exercises. For each cell type we will down load a .BAM file containing only the reads from the chromosome of interest. For each SRA run in the table below open the link to the run to down load the data. Not many raw data sets in SRA have aligned data available for down load but this data set does. Cell type SRA run Vignette Cell ID astrocyte SRR2138962 D1319_V astrocyte SRR2139935 A1643_VL neuron SRR2139989 S467_V4 neuron SRR2140047 S1282_V Download BAM files from SRA Click on the \u2018Alignment\u2019 tab Note that the data is aligned to the mouse GRCm38 genome (mm10). Select the chromosome of interest. For NTRK2 in mouse it is chr13 For \u2018Output this run in:\u2019 select BAM and click on \u2018format to:\u2019 File Rename the downloaded file to include the cell type, to avoid confusion. eg: SRR2138661_astrocyte_chr13.bam Use IGV tools to SORT and INDEX the BAM files Store sorted BAM files and index files in the same folder. Open IGV and select Tools / Run igvtools\u2026 from the pull town menus. Select \u2018Sort\u2019 from the Command options and use the brows options to select the BAM file you just downloaded and click \u2018Run\u2019 Without closing the igvtools window now select the command \u2018Index\u2019 and Browse to find the BAM file you just sorted. It will have the same file name with \u2018sorted\u2019 added to the end. eg SRR2138661_astrocyte_chr13.sorted.bam The resulting index file will have the file name : SRR2138661_astrocyte_chr13.sorted.bam.bai It is essential that the index file for a BAM file has the same name and is located in the same folder as its BAM file. If not the IVG software will not be able to open the BAM file. View the BAM files in IGV Select the Mouse (mm10) genome from the genome box in the top right hand corner. Select File / Load from File\u2026 and select all 4 \u2018_chr13.sorted.bam\u2019 files only (use command to select more then one file at a time). select open - but don\u2019t expect to see any data yet. The genome view window opens on a whole chromosome view as default but it wont show any data until the view region is small enough to show all data in the current view. Type the gene name \u2018NTRK2\u2019 into the search window. Expand the Refseq gene model track by right clicking it to see all the splice variants The gene and thus the genome view is 328kb and the default setting for viewing data is only 100kb. So unless you have already changed your settings alignment data will not get be showing. zoom into the region of a coding exon by selecting in the numbered location track at the top of the genome view. To see the whole gene in the genome window at the same time you may need to change the preferences. Go to View / Preferences and select the \u2018Alignments tab\u2019. Change the visibility range threshold to 400kb. You may need to change this back to a smaller range in the future if you are working with large datasets and/or small amounts of memory on your computer. Export images The Genome view above can be exported by selecting \u2018File / Save image\u2026\u2019 from the tool bar. To export the Sashimi plot below: Right click on one of the junction tracks and select \u2018Sashimi Plot\u2019 from the poll down menu. Select the tracks you want in your final image. There are some data filtering and style adjustments you can make to the Sashimi plot. Right click on each track to access the menu options. Some changes apply to each track individually and some to all tracks. Download and install the Gencode gene model annotation track The refseq gene model track is not as comprehensive as GeneGode gene models. For both Human and Mouse the Gencode gene model gtf annotaion files can be downloaded form Gencode . If you wish to do this be aware that it takes a little time and is not done as part of a workshop. Create a folder called \u2018annotations/Mouse\u2019 in the main \u2018igv\u2019 folder that was installed on your computer when you downloaded IGV. Download the GTF file from the link above and save it in this folder. Unpack and then SORT and INDEX the .gtf file using igvtools. in IGV, before you load you data files, load this annotation file and it will replace the refseq one.","title":"Section 3: IGV"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro/#additional-reading","text":"IGV https://rockefelleruniversity.github.io/IGV_course/presentations/singlepage/IGV.html","title":"Additional reading"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/","text":"Introduction to Genome Browsers \u00b6 Anticipated workshop duration when delivered to a group of participants is 1 hour . This is an abbreviated version of a more extensive Genome Browser workshop . For queries relating to this workshop, contact Melbourne Bioinformatics at: bioinformatics-training@unimelb.edu.au . Overview \u00b6 This tutorial will introduce you to the genome browser format and illustrate how some freely available genome browsers can be used to interrogate a variety of data types, such as gene expression, genomic variation, methylation and many more. Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with no previous experience of using Genome Browsers and no programming experience. Description \u00b6 Learn how to make the most of Genome Browsers ! By focusing on gene expression, this hands on tutorial will provide beginners with an introduction to both the UCSC Genome browser and IGV (Integrated Genome Viewer). Tools and public datasets will be used to illustrate how the expression of transcript variants can be investigated in different, tissues and cell types using public data, including human RNAseq data from GTEX and mouse cell type RNAseq data from Tabula Muris, as viewed within the UCSC genome browser. A subset of Single cell RNAseq data from the Allen Brain Atlas Celltax study will also be downloaded from SRA and visualised in IGV. The data and genes used in this workshop are taken from the neuroscience field, however the analysis approaches and tools illustrated can be applied to many research areas. This tutorial is in three parts: Section 1 Introduction to the general features of genome browsers. Section 2 Hands on tutorial of the UCSC Genome Browser Section 3 Hands on tutorials of the Integrative Genomics Viewer This tutorial was developed for use as part a series of workshops for neuroscience researchers, hence the data and example genes are drawn from neuroscience field and focused on analysis and visualisation of expression data. However, the skills taught in this tutorial are applicable to all areas of research. Data: Single cell mouse cortex data from the Linnarsson lab Tabular Muris data as represented in the UCSC Genome Browser Celltax single cell expression atlas data downloaded from SRA Tools: UCSC genome Browser Integrative Genomics Viewer Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the how some types of genomics and expression data are represented in Genome browsers. Understand gene models and identify differences between transcripts variants. Examine the tissue/cell type expression profiles of a gene of interest in expression data. Know some basic files types used in Genome browsers and upload and view local BAM files. Requirements and preparation \u00b6 Attendees are required to provide their own laptop computers. If delivered as a workshop, participants should install the software and data files below prior to the workshop. Ensure that you provide sufficient time to liaise with your own IT support should you encounter any IT problems with installing software. Unless stated otherwise, recommended browsers are Firefox or Chrome. Preparing you and your laptop prior to starting this workshop \u00b6 Recomended software: Download and install IGV (Free) Ensure that ( Chrome or FireFox are installed and upto date) Create a user account in the UCSC genome browser . Required data is downloaded as part of the tutorial exercises. Required Data \u00b6 No additional data needs to be downloaded prior to this workshop. Author(s) and review date \u00b6 Written by: Victoria Perreau | Melbourne Bioinformatics, University of Melbourne. Created: October 2020 Reviewed and revised: March 2021 Genome Browser background \u00b6 Genome browsers are invaluable for viewing and interpreting the many different types of data that can be anchored to genomic positions. These include variation, transcription, the many types regulatory data such as methylation and transcription factor binding, and disease associations. The larger genome browsers serve as data archives for valuable public datasets facilitating visualisation and analysis of different data types. It is also possible to load your own data into some of the public genome browsers. By enabling viewing of one type of data in the context of another, the use of Genome browsers can reveal important information about gene regulation in both normal development and disease, assist hypothesis development relating to genotype phenotype relationships. All researchers are therefore encouraged to become familiar with the use of some of the main browsers such as: The UCSC Genome Browser , (RRID:SCR_005780) ESEMBL Genome Browser , (RRID:SCR_013367) Epigenome browser at WashU , (RRID:SCR_006208) Integrative Genomics Viewer (IGV) , RRID:SCR_011793). They are designed for use by researchers without programming experience and the developers often provide extensive tutorials and cases studies demonstrating the myriad of ways in which data can be loaded and interpreted to assist in develop and supporting your research hypothesis. The UCSC Genome Browser Youtube channel Ensemble Browser webinar course Many large genomic projects also incorporate genome browsers into their web portals to enable users to easily search and view the data. These include: GTEx gnomAD BDNF and TrkB signalling \u00b6 This tutorial uses the a well known and important signalling pathway in the central nervous system (CNS) to illustrate some of the Genome browser tools and utility. Brain Derived Neurotrophic factor (BDNF) protein is an important neurotrophin responsible for regulating many aspects of growth and development in different cells within the CNS. TrkB is an important receptor that binds extracellular BDNF and propagates the intracellular signalling response via a tyrosine kinase. This TrkB receptor protein is encoded by the NTRK2 gene. The NRK2 gene expresses a number of different transcript variants in different cell types. The most well studied of these is the full length TrkB receptor referred to as TrkB, which is mainly expressed in neuronal cell types. The other transcript variants all express the same exons encoding the extracellular domain of the receptor (shown in the fugure here in green) but have truncated intracellular domains, which do not include the tyrosine kinase domain and thus activate different signalling pathways upon binding to BDNF. None of these truncated protein products have been well studied, but the most highly expressed receptor variant is known as TrkB-T1, and is known to be highly expressed in astocytes. Since the transcript variants are differently expressed in different cell types within the CNS the NTRK2 gene is a very useful example for exploring cell type specific transcript expression in available public data. Major CNS cell types: Neuron (yellow cell in the image below) Astrocyte Oligodendrocyte Microglia Ependymal Section 1: Introduction to Genome Browsers \u00b6 Genome browsers rely on a common reference genome for each species in order to map data from different sources to the correct location. A consortium has agreed on a common numbering for each position on the genome for each species. However, this position will vary based on the version of the genome, as error correction and updates can change the numbering. Therefore it is very important to know which version of the genome your data of interest is aligned to. The sequence for the human reference genome was accumulated up over many years from sequence data from many different sources and does not represent the sequence of one single person. Instead it is a composite of fragments of the genome from many different people. Also, unlike the human genome which is diploid, the human genome is haploid. That is there is only one copy of each chromosome. It therefore does not reflect the variation on the population, or even the most common variants in the human genome. Exploring variation within human genome is very important and facilitated by genome browsers but not covered in this workshop. Genome Build version number- further reading \u00b6 The Genome reference consortium What does the nomenclature mean? For further info on Human Genome version updates I recommend you look at the updates and blog pages on the UCSC genome browser . Section 2: The UCSC genome Browser interface \u00b6 In this section we will become familiar with the web interface of the UCSC genome browser and explore some of the tools and public datasets available. Explore features of particular chromosomal regions Investigate specific genes as well as collections of genes Search for locations of sequences and markers Retrieve annotation information for specific regions or genome-wide View your own data in context of other annotations Compare a region of one genome to genomes of other species Weekly maintenance of the browser is at 5-6 pm Thursdays Pacific time, which is equivalent to 11am-12pm AEST time. During this time the browser may be down for a few minutes. To ensure uninterrupted browser services for your research during UCSC server maintenance and power outages, bookmark one of the mirror sites that replicates the UCSC genome browser. Accessing the tools : Many of the tools that we will explore can be selected via multiple different routes within the browser interface. One way to access many tools is via from the top toolbar on a pull down list, other tools can be accessed from within the browser window. In the following instructions a series of blue boxes is used to indicate successive lower levels from the pull down menu when starting with the top toolbar. For example, the notation below indicates that you should select \u2018Genome Browser\u2019 from the top tool bar and then click on \u2018Reset all user settings\u2019. Toolbar Genome Browser Reset all user settings Accessing help and training : This workshop the UCSC genome browser is supported by rich training resource which has new material added regularly youtube channel . To access training to further develop your skills and go to: Toolbar Help Training Getting started \u00b6 Open the Browser interface: Navigate to the UCSC genome Browser and sign in if you have an account. First reset the browser, so that we all see the same screen: Toolbar Genome Browser Reset all user settings Select and open the human Genome Hg38 at the default position, there are a few different ways to do this Toolbar Genomes (this takes you to the Genome gateway page) Check that GRCh38 is selected in \u2018human assembly\u2019 and click on the blue GO box Toolbar Genomes Human GRCh38hg38 (takes you directly to the genome) You should see this screen, opening at a position on the X chromosome of Human genome version GRCh38 showing the gene model for the ACE2 gene. Familiarise yourself with the main areas of the interface and locate: The main Toolbar Blue bar track collections (data of similar types are collected together under the same \u2018Blue bar\u2019 heading). Scroll down to see additional data collections and which ones are turned on as default. Genome species and version number Position box Navigation tool buttons Chromosome ideogram Genome view window Pre loaded tracks, track titles: The grey bars on the left of the genome view can be used for selecting and configuring the tracks. You can change the order of the tracks by dragging these grey bars up and down. Turn tracks on and off: You can hide tracks by right clicking on the grey bar or by turning them off in the Blue bar collections. You have to click on a \u2018refresh\u2019 button to the changes to be reflected in the the genome view window. View the configuration page specific to a track. The configuration page gives you a lot of information about the data track and its colouring. You can open the configuration page for a track by: clicking on the grey bar for the track or, clicking on the track title in the Blue bar collection. More information and options is usually available by selecting the configuration page for a track via the track title in the Blue Bar collections. Select white \u2018resize\u2019 button to fit the genome view window to your screen Customise your view by using the \u2018Configure\u2019 tool to change the font size to 12. Use either method below to open the Configure tool. Toolbar View Configure browser text size 12 submit or start by clicking on white configure button below the genome view window. Practice navigating around the genome view. move left and right both the navigation buttons and your mouse zoom in and out using navigation buttons zoom in to a region of interest using \u2018Drag-and-select\u2019 : using your mouse select a region of interest by clicking the ruler (position track) at the very top of the genome view window. This is also how to access the \u2018highlight tool\u2019 which you will use in a later exercise to highlight a region of interest. Click on the down arrow next to the highlight colour to select a different colour. Understanding the gene models \u00b6 NTRK2 \u00b6 First we are going to familiarise ourselves with the gene model representation of the different transcripts of NTRK2. Navigate to the NTRK2 gene position in GRCh38 and view the gene models You can navigate to a different region by typing in the position box. If you know the specific location you are interested in type in the location using the format \u201cchr#:1234-1234\u201d. If you have a gene of interest you can type in the gene name (eg: NTRK2). Note the autocompleted suggestions that appear when you start typing. You can select from one of the suggestions or click go and select from a wider range of options. Type (or copy and paste) NTRK2 or chr9:84,665,760-85,030,334 into the position box. Hide all tracks by selecting the white button below the genome view. Turn on only the \u2018Genecode v32 Genemodels\u2019 in \u2018full\u2019 viewing mode by selecting from the blue bar group labelled \u2018Genes and Gene predictions\u2019. Turn on \u2018Conservation\u2019 track to \u2018full\u2019 Dont forget to click refresh . When you have navigated to the NTRK2 gene, zoom out until you can view all of the 5\u2019 UTRs and 3\u2019 UTRs for all transcript variants for this gene. Then drag the view left and right to center (like in Google maps) or \u2018drag and select\u2019 the region to center the gene in the Genome view. You should see something like the image below. Which strand is the gene encoded on / transcribed from? (+ or - strand) Identify the exons, introns and UTRs Do regions of conservation only occur were there are coding regions? How many different transcripts variants are there for this gene? How do they differ? Select a coding region (full height boxes) towards the 3\u2019UTR of the gene. zoom in to the region until you can see the letters of the amino acid sequence. Why are some amino acid boxes red or green? Zoom in again until you can see each amino acid number. Why do different transcripts have different amino acid numbers? Note that one of the transcript names is in white text with a black background, this is the transcript you selected from the autocompleted list or the search results. Change the \u2018view settings\u2019 for the track. Switch between dense squish pack full to see how it changes the representation of the models. Right click on the track grey bar in the left of the genome window to access view settings. Go to the configuration page for the Gencode v32 track and change the gene names to also reveal the \u2018Gencode transcript ID\u2019 in the label. The transcript names are now too long to fit on the screen. Go to the genome view configuration page (like you did to chane the font size at the beginning of the workshop) and change the number of characters in the label so that you can see the entire transcript label. Gene model revision Quiz \u00b6 Test your understanding of gene model representation by attempting this 6 questions in this quiz . Gene expression data \u00b6 The FACS derived data from the Tabular Muris cell type data is also availble in the UCSC genome browser and can be visualised as a coverage plot Start at the view of the NTRK2 gene in the human genome and navigate to the Ntrk2 gene in the mouse genome using the \u2018View in other genomes tool\u2019. Toolbar View In Other Genomes (Convert) select New Genome:Mouse , New Assembly:GRC38/mm10, click on \u2018Submit\u2019 select the region with the greatest homology Configure the Tabular Muris track by selecting it from the blue bar collection. Hide \u2018Cell expression\u2019 Select \u2018Genome coverage\u2019 to full Select \u2018submit\u2019 This can look like a bit too much data to manage as there are very many tracks and the default track height is set very high. But its easy to simplify it by filtering to show only a few cell types of interest. Right click on the grey bar to \u2018configure the track set\u2019. Change track height to 30 for \u2018data view scaling\u2019 select group autoscale clear all the subtracks and then manually select only a few cell types of interest: astrocyte Cv Bergmann glial Cv microglia Cv neuron Cv oligodendrocyte Cv OPC Cv Which cell type has the highest level expression in this dataset? Change the \u2018Data view scaling\u2019 to autoscale to dataview. Export a PDF image of the genome view: Toolbar View PDF/PS select \u2018Download the current browser graphic in PDF\u2019 Which cell type(s) express the long and short transcripts for NTRK2? We are going to examine the expression of transcript variants of NTRK2 in different cell types in the mouse brain cortex Linnarsson lab The data that is publicly available for viewing in the UCCS genome browser but is not housed in the UCSC genome browser. You must first access it from the the Linnarsson lab data page. This RNAseq data is stranded, meaning you can see if the transcript data is from the + or - strand. Go to the Public data page where you can search for cell expression profiles for individual genes. Click on the \u2018Browse the genome\u2019 blue text near the bottom of the page. This loads 18 different tracks, one for each cell type. The default setting for expression range is quite high and most gene expression cannot be visualised with these settings. Each track must be also be configured individually rather than as a group, which I will demonstrate but it takes a lot of time. I have created a version of this data where each track autoscales which can make it quicker to determine what expression range would be ideal for visualising the expression of an individual gene. You can access this custom track set at this link . For an exercise, select 2 or three cell types and adjust the scale to best reflect differences in gene expression of Ntrk2 between these cells. Save this session and share it. Section 3: IGV \u00b6 In this section we will download a BAM file of gene expression data from SRA and view it in the Integrated Genome Viewer (IGV) . For simplicity, and in order to save time if you have not yet installed IGV on your computer, you can view the BAM files the IVG web browser application. However you cannot generate the Sashimi plot view in the web browser version of IGV. BAM files must first be sorted and indexed before they can be loaded into genome viewers and IGV has tools to do this in the desktop app without having to use command line. However these tools are not available in the IGV web app . Therefore I have also provided the sorted BAM and index files for download, but I encourage you to download them yourself from SRA and sort and index them using the instructions below when you have time. BAM and Bai file download link The expression data we are using for this exercise is from the mouse Celltax single cell expression atlas published by the Allen Brain Institute. The cell tax vignette has an expression browser that displays gene level expression as a heat map for any gene of interest, The readsets (fastq files) and aligned data (BAM files) for 1809 runs on single cells are also available for down load from SRA. The SRA study ID for this study is SRP061902 and individual runs from this study are easily selected by viewing the samples in the \u2018RunSelector\u2019, if you wish to identify particular cell types of interest. For this exercise, I have already identified a few samples that we will download in order to illustrate navigating data in IGV by looking at the expression of NTRK2 in the same cell types we have discussed in earlier exercises. For each cell type we will down load a .BAM file containing only the reads from a single chromosome of interest. Using a reduced dataset for demonstrations cuts down on data transfer and processing time. For each SRA run in the table below open the link in a new tab to down load the data. Not many readsets in SRA have aligned data available for down load but this data set does. Cell type SRA run Vignette Cell ID astrocyte SRR2138962 D1319_V astrocyte SRR2139935 A1643_VL neuron SRR2139989 S467_V4 neuron SRR2140047 S1282_V Download BAM files from SRA Click on the \u2018Alignment\u2019 tab Note that the data is aligned to the mouse GRCm38 genome (mm10). Select the chromosome of interest. For NTRK2 in mouse it is chr13 For \u2018Output this run in:\u2019 select BAM and click on \u2018format to:\u2019 File Rename the downloaded file to include the cell type, to avoid confusion. eg: SRR2138661_astrocyte_chr13.bam Use IGV tools to SORT and INDEX the BAM files Store sorted BAM files and index files in the same folder. Open IGV and select Tools / Run igvtools\u2026 from the pull town menus. Select \u2018Sort\u2019 from the Command options and use the brows options to select the BAM file you just downloaded and click \u2018Run\u2019 Without closing the igvtools window now select the command \u2018Index\u2019 and Browse to find the BAM file you just sorted. It will have the same file name with \u2018sorted\u2019 added to the end. eg SRR2138661_astrocyte_chr13.sorted.bam The resulting index file will have the file name : SRR2138661_astrocyte_chr13.sorted.bam.bai It is essential that the index file for a BAM file has the same name and is located in the same folder as its BAM file. If not the IVG software will not be able to open the BAM file. View the BAM files in IGV Select the Mouse (mm10) genome from the genome box in the top right hand corner. Select File / Load from File\u2026 and select all 4 \u2018_chr13.sorted.bam\u2019 files only (use command to select more then one file at a time). select open - but don\u2019t expect to see any data yet. The genome view window opens on a whole chromosome view as default but it wont show any data until the view region is small enough to show all data in the current view. Type the gene name \u2018NTRK2\u2019 into the search window. Expand the Refseq gene model track by right clicking it to see all the splice variants The gene and thus the genome view is 328kb and the default setting for viewing data is only 100kb. So unless you have already changed your settings alignment data will not get be showing. zoom into the region of a coding exon by selecting in the numbered location track at the top of the genome view. To see the whole gene in the genome window at the same time you may need to change the preferences. Go to View / Preferences and select the \u2018Alignments tab\u2019. Change the visibility range threshold to 400kb. You may need to change this back to a smaller range in the future if you are working with large datasets and/or small amounts of memory on your computer. Export images The Genome view above can be exported by selecting \u2018File / Save image\u2026\u2019 from the tool bar. To export the Sashimi plot below: Right click on one of the junction tracks and select \u2018Sashimi Plot\u2019 from the poll down menu. Select the tracks you want in your final image. There are some data filtering and style adjustments you can make to the Sashimi plot. Right click on each track to access the menu options. Some changes apply to each track individually and some to all tracks. * Additional reading \u00b6 IGV https://rockefelleruniversity.github.io/IGV_course/presentations/singlepage/IGV.html","title":"GenomeBrowsers Intro short"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#introduction-to-genome-browsers","text":"Anticipated workshop duration when delivered to a group of participants is 1 hour . This is an abbreviated version of a more extensive Genome Browser workshop . For queries relating to this workshop, contact Melbourne Bioinformatics at: bioinformatics-training@unimelb.edu.au .","title":"Introduction to Genome Browsers"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#overview","text":"This tutorial will introduce you to the genome browser format and illustrate how some freely available genome browsers can be used to interrogate a variety of data types, such as gene expression, genomic variation, methylation and many more.","title":"Overview"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with no previous experience of using Genome Browsers and no programming experience.","title":"Skill level"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#description","text":"Learn how to make the most of Genome Browsers ! By focusing on gene expression, this hands on tutorial will provide beginners with an introduction to both the UCSC Genome browser and IGV (Integrated Genome Viewer). Tools and public datasets will be used to illustrate how the expression of transcript variants can be investigated in different, tissues and cell types using public data, including human RNAseq data from GTEX and mouse cell type RNAseq data from Tabula Muris, as viewed within the UCSC genome browser. A subset of Single cell RNAseq data from the Allen Brain Atlas Celltax study will also be downloaded from SRA and visualised in IGV. The data and genes used in this workshop are taken from the neuroscience field, however the analysis approaches and tools illustrated can be applied to many research areas. This tutorial is in three parts: Section 1 Introduction to the general features of genome browsers. Section 2 Hands on tutorial of the UCSC Genome Browser Section 3 Hands on tutorials of the Integrative Genomics Viewer This tutorial was developed for use as part a series of workshops for neuroscience researchers, hence the data and example genes are drawn from neuroscience field and focused on analysis and visualisation of expression data. However, the skills taught in this tutorial are applicable to all areas of research. Data: Single cell mouse cortex data from the Linnarsson lab Tabular Muris data as represented in the UCSC Genome Browser Celltax single cell expression atlas data downloaded from SRA Tools: UCSC genome Browser Integrative Genomics Viewer","title":"Description"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the how some types of genomics and expression data are represented in Genome browsers. Understand gene models and identify differences between transcripts variants. Examine the tissue/cell type expression profiles of a gene of interest in expression data. Know some basic files types used in Genome browsers and upload and view local BAM files.","title":"Learning Objectives"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#requirements-and-preparation","text":"Attendees are required to provide their own laptop computers. If delivered as a workshop, participants should install the software and data files below prior to the workshop. Ensure that you provide sufficient time to liaise with your own IT support should you encounter any IT problems with installing software. Unless stated otherwise, recommended browsers are Firefox or Chrome.","title":"Requirements and preparation"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#preparing-you-and-your-laptop-prior-to-starting-this-workshop","text":"Recomended software: Download and install IGV (Free) Ensure that ( Chrome or FireFox are installed and upto date) Create a user account in the UCSC genome browser . Required data is downloaded as part of the tutorial exercises.","title":"Preparing you and your laptop prior to starting this workshop"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#required-data","text":"No additional data needs to be downloaded prior to this workshop.","title":"Required Data"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#authors-and-review-date","text":"Written by: Victoria Perreau | Melbourne Bioinformatics, University of Melbourne. Created: October 2020 Reviewed and revised: March 2021","title":"Author(s) and review date"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#genome-browser-background","text":"Genome browsers are invaluable for viewing and interpreting the many different types of data that can be anchored to genomic positions. These include variation, transcription, the many types regulatory data such as methylation and transcription factor binding, and disease associations. The larger genome browsers serve as data archives for valuable public datasets facilitating visualisation and analysis of different data types. It is also possible to load your own data into some of the public genome browsers. By enabling viewing of one type of data in the context of another, the use of Genome browsers can reveal important information about gene regulation in both normal development and disease, assist hypothesis development relating to genotype phenotype relationships. All researchers are therefore encouraged to become familiar with the use of some of the main browsers such as: The UCSC Genome Browser , (RRID:SCR_005780) ESEMBL Genome Browser , (RRID:SCR_013367) Epigenome browser at WashU , (RRID:SCR_006208) Integrative Genomics Viewer (IGV) , RRID:SCR_011793). They are designed for use by researchers without programming experience and the developers often provide extensive tutorials and cases studies demonstrating the myriad of ways in which data can be loaded and interpreted to assist in develop and supporting your research hypothesis. The UCSC Genome Browser Youtube channel Ensemble Browser webinar course Many large genomic projects also incorporate genome browsers into their web portals to enable users to easily search and view the data. These include: GTEx gnomAD","title":"Genome Browser background"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#bdnf-and-trkb-signalling","text":"This tutorial uses the a well known and important signalling pathway in the central nervous system (CNS) to illustrate some of the Genome browser tools and utility. Brain Derived Neurotrophic factor (BDNF) protein is an important neurotrophin responsible for regulating many aspects of growth and development in different cells within the CNS. TrkB is an important receptor that binds extracellular BDNF and propagates the intracellular signalling response via a tyrosine kinase. This TrkB receptor protein is encoded by the NTRK2 gene. The NRK2 gene expresses a number of different transcript variants in different cell types. The most well studied of these is the full length TrkB receptor referred to as TrkB, which is mainly expressed in neuronal cell types. The other transcript variants all express the same exons encoding the extracellular domain of the receptor (shown in the fugure here in green) but have truncated intracellular domains, which do not include the tyrosine kinase domain and thus activate different signalling pathways upon binding to BDNF. None of these truncated protein products have been well studied, but the most highly expressed receptor variant is known as TrkB-T1, and is known to be highly expressed in astocytes. Since the transcript variants are differently expressed in different cell types within the CNS the NTRK2 gene is a very useful example for exploring cell type specific transcript expression in available public data. Major CNS cell types: Neuron (yellow cell in the image below) Astrocyte Oligodendrocyte Microglia Ependymal","title":"BDNF and TrkB signalling"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#section-1-introduction-to-genome-browsers","text":"Genome browsers rely on a common reference genome for each species in order to map data from different sources to the correct location. A consortium has agreed on a common numbering for each position on the genome for each species. However, this position will vary based on the version of the genome, as error correction and updates can change the numbering. Therefore it is very important to know which version of the genome your data of interest is aligned to. The sequence for the human reference genome was accumulated up over many years from sequence data from many different sources and does not represent the sequence of one single person. Instead it is a composite of fragments of the genome from many different people. Also, unlike the human genome which is diploid, the human genome is haploid. That is there is only one copy of each chromosome. It therefore does not reflect the variation on the population, or even the most common variants in the human genome. Exploring variation within human genome is very important and facilitated by genome browsers but not covered in this workshop.","title":"Section 1: Introduction to Genome Browsers"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#genome-build-version-number-further-reading","text":"The Genome reference consortium What does the nomenclature mean? For further info on Human Genome version updates I recommend you look at the updates and blog pages on the UCSC genome browser .","title":"Genome Build version number- further reading"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#section-2-the-ucsc-genome-browser-interface","text":"In this section we will become familiar with the web interface of the UCSC genome browser and explore some of the tools and public datasets available. Explore features of particular chromosomal regions Investigate specific genes as well as collections of genes Search for locations of sequences and markers Retrieve annotation information for specific regions or genome-wide View your own data in context of other annotations Compare a region of one genome to genomes of other species Weekly maintenance of the browser is at 5-6 pm Thursdays Pacific time, which is equivalent to 11am-12pm AEST time. During this time the browser may be down for a few minutes. To ensure uninterrupted browser services for your research during UCSC server maintenance and power outages, bookmark one of the mirror sites that replicates the UCSC genome browser. Accessing the tools : Many of the tools that we will explore can be selected via multiple different routes within the browser interface. One way to access many tools is via from the top toolbar on a pull down list, other tools can be accessed from within the browser window. In the following instructions a series of blue boxes is used to indicate successive lower levels from the pull down menu when starting with the top toolbar. For example, the notation below indicates that you should select \u2018Genome Browser\u2019 from the top tool bar and then click on \u2018Reset all user settings\u2019. Toolbar Genome Browser Reset all user settings Accessing help and training : This workshop the UCSC genome browser is supported by rich training resource which has new material added regularly youtube channel . To access training to further develop your skills and go to: Toolbar Help Training","title":"Section 2: The UCSC genome Browser interface"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#getting-started","text":"Open the Browser interface: Navigate to the UCSC genome Browser and sign in if you have an account. First reset the browser, so that we all see the same screen: Toolbar Genome Browser Reset all user settings Select and open the human Genome Hg38 at the default position, there are a few different ways to do this Toolbar Genomes (this takes you to the Genome gateway page) Check that GRCh38 is selected in \u2018human assembly\u2019 and click on the blue GO box Toolbar Genomes Human GRCh38hg38 (takes you directly to the genome) You should see this screen, opening at a position on the X chromosome of Human genome version GRCh38 showing the gene model for the ACE2 gene. Familiarise yourself with the main areas of the interface and locate: The main Toolbar Blue bar track collections (data of similar types are collected together under the same \u2018Blue bar\u2019 heading). Scroll down to see additional data collections and which ones are turned on as default. Genome species and version number Position box Navigation tool buttons Chromosome ideogram Genome view window Pre loaded tracks, track titles: The grey bars on the left of the genome view can be used for selecting and configuring the tracks. You can change the order of the tracks by dragging these grey bars up and down. Turn tracks on and off: You can hide tracks by right clicking on the grey bar or by turning them off in the Blue bar collections. You have to click on a \u2018refresh\u2019 button to the changes to be reflected in the the genome view window. View the configuration page specific to a track. The configuration page gives you a lot of information about the data track and its colouring. You can open the configuration page for a track by: clicking on the grey bar for the track or, clicking on the track title in the Blue bar collection. More information and options is usually available by selecting the configuration page for a track via the track title in the Blue Bar collections. Select white \u2018resize\u2019 button to fit the genome view window to your screen Customise your view by using the \u2018Configure\u2019 tool to change the font size to 12. Use either method below to open the Configure tool. Toolbar View Configure browser text size 12 submit or start by clicking on white configure button below the genome view window. Practice navigating around the genome view. move left and right both the navigation buttons and your mouse zoom in and out using navigation buttons zoom in to a region of interest using \u2018Drag-and-select\u2019 : using your mouse select a region of interest by clicking the ruler (position track) at the very top of the genome view window. This is also how to access the \u2018highlight tool\u2019 which you will use in a later exercise to highlight a region of interest. Click on the down arrow next to the highlight colour to select a different colour.","title":"Getting started"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#understanding-the-gene-models","text":"","title":"Understanding the gene models"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#ntrk2","text":"First we are going to familiarise ourselves with the gene model representation of the different transcripts of NTRK2. Navigate to the NTRK2 gene position in GRCh38 and view the gene models You can navigate to a different region by typing in the position box. If you know the specific location you are interested in type in the location using the format \u201cchr#:1234-1234\u201d. If you have a gene of interest you can type in the gene name (eg: NTRK2). Note the autocompleted suggestions that appear when you start typing. You can select from one of the suggestions or click go and select from a wider range of options. Type (or copy and paste) NTRK2 or chr9:84,665,760-85,030,334 into the position box. Hide all tracks by selecting the white button below the genome view. Turn on only the \u2018Genecode v32 Genemodels\u2019 in \u2018full\u2019 viewing mode by selecting from the blue bar group labelled \u2018Genes and Gene predictions\u2019. Turn on \u2018Conservation\u2019 track to \u2018full\u2019 Dont forget to click refresh . When you have navigated to the NTRK2 gene, zoom out until you can view all of the 5\u2019 UTRs and 3\u2019 UTRs for all transcript variants for this gene. Then drag the view left and right to center (like in Google maps) or \u2018drag and select\u2019 the region to center the gene in the Genome view. You should see something like the image below. Which strand is the gene encoded on / transcribed from? (+ or - strand) Identify the exons, introns and UTRs Do regions of conservation only occur were there are coding regions? How many different transcripts variants are there for this gene? How do they differ? Select a coding region (full height boxes) towards the 3\u2019UTR of the gene. zoom in to the region until you can see the letters of the amino acid sequence. Why are some amino acid boxes red or green? Zoom in again until you can see each amino acid number. Why do different transcripts have different amino acid numbers? Note that one of the transcript names is in white text with a black background, this is the transcript you selected from the autocompleted list or the search results. Change the \u2018view settings\u2019 for the track. Switch between dense squish pack full to see how it changes the representation of the models. Right click on the track grey bar in the left of the genome window to access view settings. Go to the configuration page for the Gencode v32 track and change the gene names to also reveal the \u2018Gencode transcript ID\u2019 in the label. The transcript names are now too long to fit on the screen. Go to the genome view configuration page (like you did to chane the font size at the beginning of the workshop) and change the number of characters in the label so that you can see the entire transcript label.","title":"NTRK2"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#gene-model-revision-quiz","text":"Test your understanding of gene model representation by attempting this 6 questions in this quiz .","title":"Gene model revision Quiz"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#gene-expression-data","text":"The FACS derived data from the Tabular Muris cell type data is also availble in the UCSC genome browser and can be visualised as a coverage plot Start at the view of the NTRK2 gene in the human genome and navigate to the Ntrk2 gene in the mouse genome using the \u2018View in other genomes tool\u2019. Toolbar View In Other Genomes (Convert) select New Genome:Mouse , New Assembly:GRC38/mm10, click on \u2018Submit\u2019 select the region with the greatest homology Configure the Tabular Muris track by selecting it from the blue bar collection. Hide \u2018Cell expression\u2019 Select \u2018Genome coverage\u2019 to full Select \u2018submit\u2019 This can look like a bit too much data to manage as there are very many tracks and the default track height is set very high. But its easy to simplify it by filtering to show only a few cell types of interest. Right click on the grey bar to \u2018configure the track set\u2019. Change track height to 30 for \u2018data view scaling\u2019 select group autoscale clear all the subtracks and then manually select only a few cell types of interest: astrocyte Cv Bergmann glial Cv microglia Cv neuron Cv oligodendrocyte Cv OPC Cv Which cell type has the highest level expression in this dataset? Change the \u2018Data view scaling\u2019 to autoscale to dataview. Export a PDF image of the genome view: Toolbar View PDF/PS select \u2018Download the current browser graphic in PDF\u2019 Which cell type(s) express the long and short transcripts for NTRK2? We are going to examine the expression of transcript variants of NTRK2 in different cell types in the mouse brain cortex Linnarsson lab The data that is publicly available for viewing in the UCCS genome browser but is not housed in the UCSC genome browser. You must first access it from the the Linnarsson lab data page. This RNAseq data is stranded, meaning you can see if the transcript data is from the + or - strand. Go to the Public data page where you can search for cell expression profiles for individual genes. Click on the \u2018Browse the genome\u2019 blue text near the bottom of the page. This loads 18 different tracks, one for each cell type. The default setting for expression range is quite high and most gene expression cannot be visualised with these settings. Each track must be also be configured individually rather than as a group, which I will demonstrate but it takes a lot of time. I have created a version of this data where each track autoscales which can make it quicker to determine what expression range would be ideal for visualising the expression of an individual gene. You can access this custom track set at this link . For an exercise, select 2 or three cell types and adjust the scale to best reflect differences in gene expression of Ntrk2 between these cells. Save this session and share it.","title":"Gene expression data"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#section-3-igv","text":"In this section we will download a BAM file of gene expression data from SRA and view it in the Integrated Genome Viewer (IGV) . For simplicity, and in order to save time if you have not yet installed IGV on your computer, you can view the BAM files the IVG web browser application. However you cannot generate the Sashimi plot view in the web browser version of IGV. BAM files must first be sorted and indexed before they can be loaded into genome viewers and IGV has tools to do this in the desktop app without having to use command line. However these tools are not available in the IGV web app . Therefore I have also provided the sorted BAM and index files for download, but I encourage you to download them yourself from SRA and sort and index them using the instructions below when you have time. BAM and Bai file download link The expression data we are using for this exercise is from the mouse Celltax single cell expression atlas published by the Allen Brain Institute. The cell tax vignette has an expression browser that displays gene level expression as a heat map for any gene of interest, The readsets (fastq files) and aligned data (BAM files) for 1809 runs on single cells are also available for down load from SRA. The SRA study ID for this study is SRP061902 and individual runs from this study are easily selected by viewing the samples in the \u2018RunSelector\u2019, if you wish to identify particular cell types of interest. For this exercise, I have already identified a few samples that we will download in order to illustrate navigating data in IGV by looking at the expression of NTRK2 in the same cell types we have discussed in earlier exercises. For each cell type we will down load a .BAM file containing only the reads from a single chromosome of interest. Using a reduced dataset for demonstrations cuts down on data transfer and processing time. For each SRA run in the table below open the link in a new tab to down load the data. Not many readsets in SRA have aligned data available for down load but this data set does. Cell type SRA run Vignette Cell ID astrocyte SRR2138962 D1319_V astrocyte SRR2139935 A1643_VL neuron SRR2139989 S467_V4 neuron SRR2140047 S1282_V Download BAM files from SRA Click on the \u2018Alignment\u2019 tab Note that the data is aligned to the mouse GRCm38 genome (mm10). Select the chromosome of interest. For NTRK2 in mouse it is chr13 For \u2018Output this run in:\u2019 select BAM and click on \u2018format to:\u2019 File Rename the downloaded file to include the cell type, to avoid confusion. eg: SRR2138661_astrocyte_chr13.bam Use IGV tools to SORT and INDEX the BAM files Store sorted BAM files and index files in the same folder. Open IGV and select Tools / Run igvtools\u2026 from the pull town menus. Select \u2018Sort\u2019 from the Command options and use the brows options to select the BAM file you just downloaded and click \u2018Run\u2019 Without closing the igvtools window now select the command \u2018Index\u2019 and Browse to find the BAM file you just sorted. It will have the same file name with \u2018sorted\u2019 added to the end. eg SRR2138661_astrocyte_chr13.sorted.bam The resulting index file will have the file name : SRR2138661_astrocyte_chr13.sorted.bam.bai It is essential that the index file for a BAM file has the same name and is located in the same folder as its BAM file. If not the IVG software will not be able to open the BAM file. View the BAM files in IGV Select the Mouse (mm10) genome from the genome box in the top right hand corner. Select File / Load from File\u2026 and select all 4 \u2018_chr13.sorted.bam\u2019 files only (use command to select more then one file at a time). select open - but don\u2019t expect to see any data yet. The genome view window opens on a whole chromosome view as default but it wont show any data until the view region is small enough to show all data in the current view. Type the gene name \u2018NTRK2\u2019 into the search window. Expand the Refseq gene model track by right clicking it to see all the splice variants The gene and thus the genome view is 328kb and the default setting for viewing data is only 100kb. So unless you have already changed your settings alignment data will not get be showing. zoom into the region of a coding exon by selecting in the numbered location track at the top of the genome view. To see the whole gene in the genome window at the same time you may need to change the preferences. Go to View / Preferences and select the \u2018Alignments tab\u2019. Change the visibility range threshold to 400kb. You may need to change this back to a smaller range in the future if you are working with large datasets and/or small amounts of memory on your computer. Export images The Genome view above can be exported by selecting \u2018File / Save image\u2026\u2019 from the tool bar. To export the Sashimi plot below: Right click on one of the junction tracks and select \u2018Sashimi Plot\u2019 from the poll down menu. Select the tracks you want in your final image. There are some data filtering and style adjustments you can make to the Sashimi plot. Right click on each track to access the menu options. Some changes apply to each track individually and some to all tracks. *","title":"Section 3: IGV"},{"location":"tutorials/Genome_browsers/GenomeBrowsers_Intro_short/#additional-reading","text":"IGV https://rockefelleruniversity.github.io/IGV_course/presentations/singlepage/IGV.html","title":"Additional reading"},{"location":"tutorials/Genome_browsers/formatting_template/","text":"Formats to use \u00b6 Text formatting \u00b6 Bold Italics Bold Italics Headings (This is 2 nd level) \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Code Blocks and inline code \u00b6 They can be added like this. Many different languages are supported. Blocks \u00b6 1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf Inline code \u00b6 Code can also be shown as an inline snippet like this import tensorflow as tf . Lists \u00b6 If you need to add a list: Unordered Lists \u00b6 Some information Some more information Ordered Lists \u00b6 Some point Another point Subpoint Sub-subpoint Images \u00b6 How to add an image: Tables \u00b6 Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit Questions and Answers \u00b6 It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: Line Breaks \u00b6 To create a line break (\\<br>), end a line with two or more spaces, and then type return. Links \u00b6 Please see the link . Blockquotes \u00b6 This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote. Text including commands to type \u00b6 Type ls and press ENTER . When referring to a filename \u00b6 canu.contigs.fasta contains the assembled sequences. Showing that a button needs clicking \u00b6 Click Start Highlighting text \u00b6 This text is highlighted. Equations \u00b6 Equations can be added as a block or inline. Block equations \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline equations \u00b6 This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} To do \u00b6 Pdf printing Survey Slides?","title":"Formats to use"},{"location":"tutorials/Genome_browsers/formatting_template/#formats-to-use","text":"","title":"Formats to use"},{"location":"tutorials/Genome_browsers/formatting_template/#text-formatting","text":"Bold Italics Bold Italics","title":"Text formatting"},{"location":"tutorials/Genome_browsers/formatting_template/#headings-this-is-2nd-level","text":"","title":"Headings (This is 2nd level)"},{"location":"tutorials/Genome_browsers/formatting_template/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"tutorials/Genome_browsers/formatting_template/#the-4th-level","text":"","title":"The 4th level"},{"location":"tutorials/Genome_browsers/formatting_template/#the-5th-level","text":"","title":"The 5th level"},{"location":"tutorials/Genome_browsers/formatting_template/#the-6th-level","text":"","title":"The 6th level"},{"location":"tutorials/Genome_browsers/formatting_template/#code-blocks-and-inline-code","text":"They can be added like this. Many different languages are supported.","title":"Code Blocks and inline code"},{"location":"tutorials/Genome_browsers/formatting_template/#blocks","text":"1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf","title":"Blocks"},{"location":"tutorials/Genome_browsers/formatting_template/#inline-code","text":"Code can also be shown as an inline snippet like this import tensorflow as tf .","title":"Inline code"},{"location":"tutorials/Genome_browsers/formatting_template/#lists","text":"If you need to add a list:","title":"Lists"},{"location":"tutorials/Genome_browsers/formatting_template/#unordered-lists","text":"Some information Some more information","title":"Unordered Lists"},{"location":"tutorials/Genome_browsers/formatting_template/#ordered-lists","text":"Some point Another point Subpoint Sub-subpoint","title":"Ordered Lists"},{"location":"tutorials/Genome_browsers/formatting_template/#images","text":"How to add an image:","title":"Images"},{"location":"tutorials/Genome_browsers/formatting_template/#tables","text":"Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit","title":"Tables"},{"location":"tutorials/Genome_browsers/formatting_template/#questions-and-answers","text":"It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this:","title":"Questions and Answers"},{"location":"tutorials/Genome_browsers/formatting_template/#line-breaks","text":"To create a line break (\\<br>), end a line with two or more spaces, and then type return.","title":"Line Breaks"},{"location":"tutorials/Genome_browsers/formatting_template/#links","text":"Please see the link .","title":"Links"},{"location":"tutorials/Genome_browsers/formatting_template/#blockquotes","text":"This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote.","title":"Blockquotes"},{"location":"tutorials/Genome_browsers/formatting_template/#text-including-commands-to-type","text":"Type ls and press ENTER .","title":"Text including commands to type"},{"location":"tutorials/Genome_browsers/formatting_template/#when-referring-to-a-filename","text":"canu.contigs.fasta contains the assembled sequences.","title":"When referring to a filename"},{"location":"tutorials/Genome_browsers/formatting_template/#showing-that-a-button-needs-clicking","text":"Click Start","title":"Showing that a button needs clicking"},{"location":"tutorials/Genome_browsers/formatting_template/#highlighting-text","text":"This text is highlighted.","title":"Highlighting text"},{"location":"tutorials/Genome_browsers/formatting_template/#equations","text":"Equations can be added as a block or inline.","title":"Equations"},{"location":"tutorials/Genome_browsers/formatting_template/#block-equations","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Block equations"},{"location":"tutorials/Genome_browsers/formatting_template/#inline-equations","text":"This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline equations"},{"location":"tutorials/Genome_browsers/formatting_template/#to-do","text":"Pdf printing Survey Slides?","title":"To do"},{"location":"tutorials/alignment/","text":"NB: This tutorial is from a lab and probably obsolete; alignment is covered by variant calling workshops. PR reviewers and advice: Clare Sloggett Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/alignment/alignment/","text":"Alignment Tutorial \u00b6 In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We\u2019ll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials. Section 1: Alignment using Galaxy \u00b6 Preparation \u00b6 Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There\u2019s only one input file, so instead of importing a History, let\u2019s import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ . Alignment \u00b6 Run BWA \u00b6 For our alignment, we will use the tool BWA, which stands for \u201cBurrows-Wheeler Aligner\u201d. You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \u201c@\u201d, followed by one alignment row per read. Refer to the docs above to understand what you\u2019re seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \u201c>\u201d lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that\u2019s aligned to a particular region of the genome. Section 2: View the alignment \u00b6 Load the VNC interface \u00b6 We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer\u2019s VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You\u2019ll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer\u2019s visual desktop. You\u2019ll notice two icons on the desktop. A terminal and a shortcut to IGV. Download the data \u00b6 The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You\u2019ll need to download both the BAM file and BAM index file. View the BAM file in IGV \u00b6 Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user\u2019s Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \u201cAll\u201d, instead select \u201c22\u201d. Zoom in. If you\u2019re still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \u201cGo\u201d. At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV\u2019s features - try right-clicking on a read to change the display options. Section 3: Alignment using the command line \u00b6 In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty. Get the data \u00b6 Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : 1 wget https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_377 / public / variantCalling_BASIC / NA12878 . GAIIx . exome_chr22 . 1 E6reads . 76 bp . fastq This will download the datafile to the current directory. Set up the environment \u00b6 Now you\u2019ll need to load the bwa and samtools modules. Try 1 module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: 1 2 module load bwa module load samtools / 1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands. Perform the alignment \u00b6 The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won\u2019t have time to index it during the lab, but your instance should already have access to an indexed genome. Try 1 2 3 4 ls galaxy_genomes ls galaxy_genomes / hg19 ls galaxy_genomes / hg19 / bwa_mem_index ls galaxy_genomes / hg19 / bwa_mem_index / hg19 / Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: 1 bwa mem galaxy_genomes / hg19 / bwa_mem_index / hg19 / hg19 . fa NA12878 . GAIIx . exome_chr22 . 1 E6reads . 76 bp . fastq > aligned_reads . sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it. Make the BAM file \u00b6 Try to convert your SAM file to a BAM file using samtools view. The command you\u2019ll need is: 1 samtools view - b - h aligned_reads . sam > aligned_reads . bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation. Other SAM tools commands \u00b6 Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You\u2019ll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV\u2026 That\u2019s it, I hope you enjoyed the tutorial!","title":"Alignment"},{"location":"tutorials/alignment/alignment/#alignment-tutorial","text":"In this tutorial we will be performing some alignments of short reads to a longer reference (as outlined in earlier lectures.) It is split into two sections. You will perform the same analysis in both sections. The first is Alignment using the Galaxy bioinformatics workflow environment, the second is Alignment using the Unix/Linux command line. Both sections use the same tools. We\u2019ll be using some data and steps from the Genomics Virtual Lab Variant Detection tutorials. You can follow these links if interested to see the full Introductory Variant Detection and Advanced Variant Detection tutorials.","title":"Alignment Tutorial"},{"location":"tutorials/alignment/alignment/#section-1-alignment-using-galaxy","text":"","title":"Section 1: Alignment using Galaxy"},{"location":"tutorials/alignment/alignment/#preparation","text":"Open Galaxy using the IP address of your Cloud instance in Firefox or Chrome (whichever) Unless you already have, create a log-in for yourself On the top menu select: User -> Register Enter your email address, choose a password, repeat it and add an (all lower case) one word name Click Submit. If you have already registered previously, just log in. Create a new history and load the required data files Click the icon in the History pane and Create New . There\u2019s only one input file, so instead of importing a History, let\u2019s import the file directly: From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Once the file is imported, you may want to give it a shorter name by clicking the icon on the dataset in the History pane. Examine the input data. In the History pane on the right, you should see a FASTQ file in the History. Click the icon to view the contents. The data for this workshop is the same as that used in the GVL Introductory Variant Detection tutorial. It is short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ .","title":"Preparation"},{"location":"tutorials/alignment/alignment/#alignment","text":"","title":"Alignment"},{"location":"tutorials/alignment/alignment/#run-bwa","text":"For our alignment, we will use the tool BWA, which stands for \u201cBurrows-Wheeler Aligner\u201d. You can see its website, if interested, at http://bio-bwa.sourceforge.net/ . We will be using the bwa mem variant. Hopefully, everything will be working fine, so the first thing we need to do is run BWA.. To do this, use the following steps. In the Galaxy tool pane on the left, under NGS: Mapping select the tool Map with BWA-MEM . We want to align this data to the human genome. Have a look under Using reference genome . Choose hg19 . This is Human reference genome 19 ( UCSC hg19 ). Set Single or Paired-end reads to Single Specify the FASTQ file as input - it may be automatically selected already. For our alignment, it will be ok to use the default alignment parameters. However, have a look at the options you can change by setting Select analysis mode to Full list of options . Look through the parameters and see if you understand from lectures what they are for. You can also read documentation on these options by scrolling down the Galaxy BWA tool page. Put the Select analysis mode back to Simple Illumina Mode Click Execute and wait for your job to run. This may take a few minutes and will produce a BAM file. While waiting you can have a look at the SAM/BAM format specification. Look at this document , which gives you information on FASTQ, SAM, pileup and BED formats. You can also view the SAM/BAM format specification itself at http://samtools.github.io/hts-specs/SAMv1.pdf . Now we will convert the compressed BAM file to an uncompressed SAM file so we can see what it looks like. In the Galaxy tool pane on the left, under NGS: SAM tools select the tool BAM to SAM . For BAM File to Convert select your BAM file. Click Execute . Browse the resulting SAM file. To view the data click on the icon next to the \u201cBAM to SAM on \u2026 converted SAM\u201d dataset in the History panel on the right-hand-side of the display. You should see header lines which begin with \u201c@\u201d, followed by one alignment row per read. Refer to the docs above to understand what you\u2019re seeing. One thing to notice is that the SAM file might be sorted in different ways, or might not be sorted at all. Two common ways of sorting a SAM or BAM file are: Alphabetically by the names of the reads. The read names are the first column of the SAM file, and come from the \u201c>\u201d lines in the FASTQ file. In order of genomic position, so that all the reads aligned to chromosome 1 are listed first (in coordinate order), followed by reads aligned to chromosome 2, etc. This information is in the third and fourth columns of the SAM file. This sort order is usually more useful for algorithms which need to access data that\u2019s aligned to a particular region of the genome.","title":"Run BWA"},{"location":"tutorials/alignment/alignment/#section-2-view-the-alignment","text":"","title":"Section 2: View the alignment"},{"location":"tutorials/alignment/alignment/#load-the-vnc-interface","text":"We will use a Genome Browser called IGV to view the BAM alignment. It is installed on your cloud computer. To use it, we need to log in to your cloud computer\u2019s VNC interface. Point your web browser at http://your-ip-address This is the GVL dashboard. Half way down the page there is a link to the Lubuntu desktop. It is, http://your-ip-address/vnc , click on it. You\u2019ll need to log in to the web page using the username: Ubuntu , password: SummerCamp2016 . This will display the desktop of the cloud computer in your web browser. (You may need to reload the page for it to work properly.) Login to the desktop as the researcher user (not ubuntu) using the same password. Now we have logged into the cloud computer\u2019s visual desktop. You\u2019ll notice two icons on the desktop. A terminal and a shortcut to IGV.","title":"Load the VNC interface"},{"location":"tutorials/alignment/alignment/#download-the-data","text":"The next thing we need to do is get the BAM files out of Galaxy and into the normal unix filesystem. To do this we need to load Galaxy within the VNC interface. From the Linux menu button on the bottom left of the vnc interface, select Internet->Firefox Point the resultant web browser (within the VNC interface) at http://localhost/galaxy Login to this Galaxy interface via the User menu using the same login details you set up earlier. You should now see the history that you hve been working on. Click on the file name of the BAM file in your history. This will expand it to show some additional details. Click on the Download ( ) button. You\u2019ll need to download both the BAM file and BAM index file.","title":"Download the data"},{"location":"tutorials/alignment/alignment/#view-the-bam-file-in-igv","text":"Now we will open IGV to view the bam file and browse the alignment. Double click the IGV icon on the VNC desktop. Note that it will take some time to open. We need to make sure we have the correct reference genome loaded that matches the one we used in our alignment. Make sure that the dropdown box on the top-left shows hg19. Load the BAM you downloaded from Galaxy. Select File -> Load from File and supply the BAM file. (It is in the researcher user\u2019s Downloads directory.) Now explore the visualisation of the BAM file! Recall that our example data is from chromosome 22 only. In the contig drop-down menu, which says \u201cAll\u201d, instead select \u201c22\u201d. Zoom in. If you\u2019re still having trouble finding the area the reads are mapped to, paste these coordinates into the box in the top bar of IGV: 22:17,432,499-17,502,283 . Then click \u201cGo\u201d. At the bottom, you should see an annotation track that is loaded into IGV by default, showing transcripts. Right click on the \u201cGenes\u201d track and select \u201cExpanded\u201d. Zoom in to look at individual reads. Compare them to the gene annotation track, do you understand what is going on? Try zooming in far enough to see the reference genome sequence. Browse around and see if you can find some errors in the alignments. You can also explore IGV\u2019s features - try right-clicking on a read to change the display options.","title":"View the BAM file in IGV"},{"location":"tutorials/alignment/alignment/#section-3-alignment-using-the-command-line","text":"In this section of the tutorial we will perform the same alignment as before on the command line. We will use the same data and tools. To use the command-line on your Research Cloud instance, ssh in to your instance as researcher@ using ssh or Putty.","title":"Section 3: Alignment using the command line"},{"location":"tutorials/alignment/alignment/#get-the-data","text":"Once you are in the unix shell, you can get the input data for the tutorial using a command like wget or curl : 1 wget https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_377 / public / variantCalling_BASIC / NA12878 . GAIIx . exome_chr22 . 1 E6reads . 76 bp . fastq This will download the datafile to the current directory.","title":"Get the data"},{"location":"tutorials/alignment/alignment/#set-up-the-environment","text":"Now you\u2019ll need to load the bwa and samtools modules. Try 1 module avail This command will show you a list of command line tool modules available to you. To use any of the tools, you need to load them into the current compute environment. You use the module load command for this. We need bwa and samtools so the commands are: 1 2 module load bwa module load samtools / 1.2 Then, run bwa with the command: bwa to see some information on BWA usage. This will list BWA commands, such as bwa index , which indexes a reference genome, and bwa mem , which performs alignment. Type a command to see information on its usage. Try bwa mem to see how to run bwa mem . Similarly, try just samtools to see a list of samtools commands.","title":"Set up the environment"},{"location":"tutorials/alignment/alignment/#perform-the-alignment","text":"The Galaxy BWA tool actually wraps three commands; bwa mem , which performs alignment and produces a SAM file; samtools view which produces a BAM file from the SAM file and finally samtools sort which sorts the bam file so it is in reference position order. We will replicate that process here. How can you get an indexed reference genome? You could download a FASTA file and index it with bwa index . The human genome is fairly large and we won\u2019t have time to index it during the lab, but your instance should already have access to an indexed genome. Try 1 2 3 4 ls galaxy_genomes ls galaxy_genomes / hg19 ls galaxy_genomes / hg19 / bwa_mem_index ls galaxy_genomes / hg19 / bwa_mem_index / hg19 / Under galaxy_genomes are directories for different reference genomes, and each has pre-built indices for various tools including BWA. You may also need galaxy_genomes/hg19/sam_index when using some samtools commands. You need to give BWA the prefix of the reference genome index files, which is galaxy_genomes/hg19/bwa_index/hg19/hg19.fa So as the genome has already been indexed for us, we just need to point bwa at it. So, now we can run bwa to align the reads to the reference. We use the form of the command bwa mem index_file reads_file > aligned_reads.sam To do this from your home directory, run the command: 1 bwa mem galaxy_genomes / hg19 / bwa_mem_index / hg19 / hg19 . fa NA12878 . GAIIx . exome_chr22 . 1 E6reads . 76 bp . fastq > aligned_reads . sam This will load the reference index, align the reads and produce a SAM file called aligned_reads.sam . Use less to examine it.","title":"Perform the alignment"},{"location":"tutorials/alignment/alignment/#make-the-bam-file","text":"Try to convert your SAM file to a BAM file using samtools view. The command you\u2019ll need is: 1 samtools view - b - h aligned_reads . sam > aligned_reads . bam You can look up the effect of all these command line flags by typing the view command by itself, or by reading the samtools documentation.","title":"Make the BAM file"},{"location":"tutorials/alignment/alignment/#other-sam-tools-commands","text":"Try some other samtools commands on your BAM file. Again, find out how to use them by typing them by themselves, or by reading the documentation. Some commands are: samtools sort to sort the BAM file samtools index to produce the index (.bai) file. You need this to visualise the BAM file in IGV. samtools flagstat to get some basic information on mapped reads samtools view to see the BAM files contents; this effectively converts it back to SAM Now go back to the viewing BAM alignments section above. You\u2019ll need to sort the BAM file and create an index (.bai) file before you load the BAM file into IGV\u2026 That\u2019s it, I hope you enjoyed the tutorial!","title":"Other SAM tools commands"},{"location":"tutorials/assembly/","text":"PR reviewers and advice: Simon Gladman, Torsten Seemann, Dieter Bulach, Anna Syme Current slides: in folder https://drive.google.com/drive/u/0/folders/0B2iomOA3e6SuZHNxNjlPWG9hdTQ Other slides: Markdown slides at galaxyproject.github.io : http://galaxyproject.github.io/training-material/topics/assembly/","title":"Home"},{"location":"tutorials/assembly/assembly-background/","text":"De novo genome assembly using Velvet \u00b6 Background \u00b6 Introduction to de novo assembly \u00b6 DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \u201cThe problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\u201d \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system. The Galaxy workflow platform \u00b6 What is Galaxy? \u00b6 Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right. De novo assembly with Velvet and the Velvet Optimiser. \u00b6 Velvet \u00b6 Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically. de Bruijn graphs \u00b6 A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others. The Velvet algorithm \u00b6 Step 1: Hashing the reads. \u00b6 Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite. Step 2: Constructing the de Bruijn graph. \u00b6 Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.) Step 3: Simplification of the graph. \u00b6 Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.) Step 4: Read off the contigs. \u00b6 Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one. K-mer size and coverage cutoff values \u00b6 The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software. Velvet Optimiser \u00b6 The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time). References \u00b6 http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"Introduction to de novo assembly with Velvet"},{"location":"tutorials/assembly/assembly-background/#de-novo-genome-assembly-using-velvet","text":"","title":"De novo genome assembly using Velvet"},{"location":"tutorials/assembly/assembly-background/#background","text":"","title":"Background"},{"location":"tutorials/assembly/assembly-background/#introduction-to-de-novo-assembly","text":"DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated. The sequence assembly issue was neatly summed up by the following quote: \u201cThe problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces. The book may have many repeated paragraphs, and some shreds may be modified to have typos. Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\u201d \u2013 Wikipedia: Sequence assembly. An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known. This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system.","title":"Introduction to de novo assembly"},{"location":"tutorials/assembly/assembly-background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/assembly/assembly-background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface Tools on the left, data in the middle, analysis workflow on the right.","title":"What is Galaxy?"},{"location":"tutorials/assembly/assembly-background/#de-novo-assembly-with-velvet-and-the-velvet-optimiser","text":"","title":"De novo assembly with Velvet and the Velvet Optimiser."},{"location":"tutorials/assembly/assembly-background/#velvet","text":"Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.","title":"Velvet"},{"location":"tutorials/assembly/assembly-background/#de-bruijn-graphs","text":"A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence. Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0. From: https://cameroncounts.wordpress.com/2015/02/28/1247/ Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters. Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.","title":"de Bruijn graphs"},{"location":"tutorials/assembly/assembly-background/#the-velvet-algorithm","text":"","title":"The Velvet algorithm"},{"location":"tutorials/assembly/assembly-background/#step-1-hashing-the-reads","text":"Velvet breaks up each read into k-mers of length k. A k-mer is a k length subsequence of the read. A 36 base pair long read would have 6 different 31-mers. The k-mers and their reverse complements are added to a hash table to categorize them. Each k-mer is stored once but the number of times it appears is also recorded. This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.","title":"Step 1: Hashing the reads."},{"location":"tutorials/assembly/assembly-background/#step-2-constructing-the-de-bruijn-graph","text":"Velvet adds the k-mers one-by-one to the graph. Adjacent k-mers overlap by k-1 nucleotides. A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node. Each node stores the average number of times its k-mers appear in the hash table. Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5. Different sequences can be read off the graph by following a different path through it. (Figure 3) Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph. (Base figure Zerbino et al 2008.)","title":"Step 2: Constructing the de Bruijn graph."},{"location":"tutorials/assembly/assembly-background/#step-3-simplification-of-the-graph","text":"Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes. Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path. Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences. Velvet compares the paths using dynamic programming. If they are highly similar, the paths are merged. Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes. Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs. Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph. (Base figure Zerbino et al 2008.)","title":"Step 3: Simplification of the graph."},{"location":"tutorials/assembly/assembly-background/#step-4-read-off-the-contigs","text":"Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs. Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.","title":"Step 4: Read off the contigs."},{"location":"tutorials/assembly/assembly-background/#k-mer-size-and-coverage-cutoff-values","text":"The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down. The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data. Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.","title":"K-mer size and coverage cutoff values"},{"location":"tutorials/assembly/assembly-background/#velvet-optimiser","text":"The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies. It uses different optimisation functions for k and c and these can be user controlled. It requires the user to input a range of k values to search (to cut down on running time).","title":"Velvet Optimiser"},{"location":"tutorials/assembly/assembly-background/#references","text":"http://en.wikipedia.org/wiki/Sequence_assembly Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829 Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407. Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.","title":"References"},{"location":"tutorials/assembly/assembly-protocol/","text":"De novo Genome Assembly for Illumina Data \u00b6 Protocol \u00b6 Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Protocol Overview / Introduction \u00b6 In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes. What is de novo genome assembly? \u00b6 Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1] Why do we want to assemble an organism\u2019s DNA? \u00b6 Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2]. The protocol in a nutshell: \u00b6 Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol. Raw read sequence file formats. \u00b6 Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth. Bioinformatics tools for this protocol. \u00b6 There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type. Genomics Virtual Laboratory resources for this protocol. \u00b6 Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources. Section 1: Read Quality Control \u00b6 Purpose: \u00b6 The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities. Steps involved and suggested tools: \u00b6 Examine the quality of your raw read files. \u00b6 For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s. Quality trimming/cleanup of read files. \u00b6 Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java - cp < path to trimmomatic jar > org . usadellab . trimmomatic . TrimmomaticPE for Paired End Files java - cp < path to trimmomatic jar > org . usadellab . trimmomatic . TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly. Possible alternate tools: \u00b6 Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml Section 2: Assembly \u00b6 Purpose: \u00b6 The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d Steps involved and suggested tools: \u00b6 Assembly of the reads. \u00b6 The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff \u00a9. Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser . pl - f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" - s 63 - e 75 - d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range. Examine the draft contigs and assessment of the assembly quality. \u00b6 The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ). Possible alternative software: \u00b6 Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs. Section 3: What next? \u00b6 Purpose: \u00b6 Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data? Possible tools for improving your assemblies: \u00b6 Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"Introduction to de novo genome assembly for Illumina reads"},{"location":"tutorials/assembly/assembly-protocol/#de-novo-genome-assembly-for-illumina-data","text":"","title":"De novo Genome Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly-protocol/#protocol","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Protocol"},{"location":"tutorials/assembly/assembly-protocol/#protocol-overview-introduction","text":"In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.","title":"Protocol Overview / Introduction"},{"location":"tutorials/assembly/assembly-protocol/#what-is-de-novo-genome-assembly","text":"Genome assembly refers to the process of taking a large number of short DNA sequences and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used. Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found here . Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process. The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose. The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See this document for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]","title":"What is de novo genome assembly?"},{"location":"tutorials/assembly/assembly-protocol/#why-do-we-want-to-assemble-an-organisms-dna","text":"Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].","title":"Why do we want to assemble an organism\u2019s DNA?"},{"location":"tutorials/assembly/assembly-protocol/#the-protocol-in-a-nutshell","text":"Obtain sequence read file(s) from sequencing machine(s). Look at the reads - get an understanding of what you\u2019ve got and what the quality is like. Raw data cleanup/quality trimming if necessary. Choose an appropriate assembly parameter set. Assemble the data into contigs/scaffolds. Examine the output of the assembly and assess assembly quality. Figure 1: Flowchart of de novo assembly protocol.","title":"The protocol in a nutshell:"},{"location":"tutorials/assembly/assembly-protocol/#raw-read-sequence-file-formats","text":"Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a Fasta file or with their qualities as a FastQ file. They can also be stored as alignments to references in other formats such as SAM or its binary compressed implementation BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.) The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.","title":"Raw read sequence file formats."},{"location":"tutorials/assembly/assembly-protocol/#bioinformatics-tools-for-this-protocol","text":"There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.","title":"Bioinformatics tools for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#genomics-virtual-laboratory-resources-for-this-protocol","text":"Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources. You can use Galaxy-tut or your own GVL server. All of the suggested tools for this protocol are installed and available. If you\u2019re happy and comfortable using the command line, you can do this with your own GVL Linux instance on the NeCTAR Research Cloud . Most of the suggested tools are available on the command line as environment modules. Enter module avail at a command prompt on your instance for details. You can also use your own computing resources.","title":"Genomics Virtual Laboratory resources for this protocol."},{"location":"tutorials/assembly/assembly-protocol/#section-1-read-quality-control","text":"","title":"Section 1: Read Quality Control"},{"location":"tutorials/assembly/assembly-protocol/#purpose","text":"The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#examine-the-quality-of-your-raw-read-files","text":"For FastQ files (the most common), the suggested tool is FastQC . Details can be found here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.) FastQC on any GVL Galaxy is located in: NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC Command line: fastqc Details on installation and use can be found here . Some of the important outputs of FastQC for our purposes are: Read length - Will be important in setting maximum k-mer size value for assembly Quality encoding type - Important for quality trimming software % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution. Total number of reads - Gives you an idea of coverage.. Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run. Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc. Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.","title":"Examine the quality of your raw read files."},{"location":"tutorials/assembly/assembly-protocol/#quality-trimmingcleanup-of-read-files","text":"Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called Trimmomatic . Details on Trimmomatic can be found here . Trimmomatic on GVL systems: NGS: QC and Manipulation -> Trimmomatic Command line: details and examples here . java - cp < path to trimmomatic jar > org . usadellab . trimmomatic . TrimmomaticPE for Paired End Files java - cp < path to trimmomatic jar > org . usadellab . trimmomatic . TrimmomaticSE for Single End Files Trimmomatic can perform many read trimming functions sequentially. Suggested Trimmomatic functions to use: Adapter trimming This function trims adapters, barcodes and other contaminants from the reads. You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions. The default quality settings are sensible. This should always be the first trimming step if it is used. Sliding window trimming This function uses a sliding window to measure average quality and trims accordingly. The default quality parameters are sensible for this step. Trailing bases quality trimming This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases. Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point. Leading bases quality trimming This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads. Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary. Minimum read length Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file. The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph. Things to look for in the output: Number of reads orphaned by the trimming / cleanup process. Number of pairs lost totally. Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.","title":"Quality trimming/cleanup of read files."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternate-tools","text":"Read quality trimming: nesoni clip , part of the nesoni suite of bioinformatics tools. Available at http://www.bioinformatics.net.au/software.shtml","title":"Possible alternate tools:"},{"location":"tutorials/assembly/assembly-protocol/#section-2-assembly","text":"","title":"Section 2: Assembly"},{"location":"tutorials/assembly/assembly-protocol/#purpose_1","text":"The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method. You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools_1","text":"","title":"Steps involved and suggested tools:"},{"location":"tutorials/assembly/assembly-protocol/#assembly-of-the-reads","text":"The suggested assembly software for this protocol is the Velvet Optimiser which wraps the Velvet Assembler. The Velvet assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see here for details). Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously. The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff \u00a9. Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line. In Galaxy: NGS-Assembly \u2192 Velvet Optimiser Command line: details and examples here . Example command line for paired end reads in read files reads_R1.fq and reads_R2.fq using a kmer-size search range of 63 - 75 . VelvetOptimiser . pl - f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" - s 63 - e 75 - d output_directory The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.","title":"Assembly of the reads."},{"location":"tutorials/assembly/assembly-protocol/#examine-the-draft-contigs-and-assessment-of-the-assembly-quality","text":"The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file. The assembly parameters used in the final assembly can also be found as part of the last entry in the log file. The contig_stats.txt file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc. More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ).","title":"Examine the draft contigs and assessment of the assembly quality."},{"location":"tutorials/assembly/assembly-protocol/#possible-alternative-software","text":"Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include: Spades SOAP-denovo MIRA ALLPATHS See here for a comprehensive list of - and links to - short read assembly programs.","title":"Possible alternative software:"},{"location":"tutorials/assembly/assembly-protocol/#section-3-what-next","text":"","title":"Section 3: What next?"},{"location":"tutorials/assembly/assembly-protocol/#purpose_2","text":"Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now. Some things to remember about the contigs you have just produced: They\u2019re draft contigs. They may contain some gaps or regions of \u201cN\u201ds. There may be some mis-assemblies. What happens with your contigs next is determined by what you need them for: You only want to look at certain loci or genes in your genome Check and see if the regions of interest have been assembled in their entirety. If they have then just use the contigs of interest. If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions. Performing an automatic annotation on your draft contigs can help with this. You want to perform comparative genomics analyses with your contigs Do your contigs cover all of the regions you are interested in? Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats. Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below. You want to \u201cfinish\u201d the genome and publish it in genbank. Can your assembly be improved with more and/or different read data? Can you use other tools to improve your assembly with your current read data?","title":"Purpose:"},{"location":"tutorials/assembly/assembly-protocol/#possible-tools-for-improving-your-assemblies","text":"Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied! Mis-assembly checking and assembly metric tools: QUAST - Quality assessment tool for genome assembly http://bioinf.spbau.ru/quast Mauve assembly metrics - http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve InGAP-SV - https://sites.google.com/site/nextgengenomics/ingap and http://ingap.sourceforge.net/ inGAP is also useful for finding structural variants between genomes from read mappings. Genome finishing tools: Semi-automated gap fillers: Gap filler - http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/ IMAGE (V2) - http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page Genome visualisers and editors Artemis - http://www.sanger.ac.uk/resources/software/artemis/ IGV - http://www.broadinstitute.org/igv/ Geneious - http://www.geneious.com/ CLC BioWorkbench - http://www.clcbio.com/products/clc-genomics-workbench/ Automated and semi automated annotation tools Prokka - https://github.com/tseemann/prokka RAST - http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer JCVI Annotation Service - http://www.jcvi.org/cms/research/projects/annotation-service/","title":"Possible tools for improving your assemblies:"},{"location":"tutorials/assembly/assembly/","text":"Microbial de novo Assembly for Illumina Data \u00b6 Introductory Tutorial \u00b6 Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Tutorial Overview \u00b6 In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies Background \u00b6 Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) ( http://www.ncbi.nlm.nih.gov/sra/ ). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information. Preparation \u00b6 Login to Galaxy \u00b6 Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well. Import the DNA read data for the tutorial. \u00b6 You can do this in a few ways. If you\u2019re using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \u201c Microbial_assembly_input_data \u201d. Then click \u2018Import History\u2019 at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it\u2019s type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it\u2019s type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format View the fastq files \u00b6 Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019 Section 1: Quality control \u00b6 The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here. Run FastQC on both input read files \u00b6 From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \u201cFASTQ reads\u201d: ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them. Figure 1: Screenshot of FastQC interface in Galaxy \u00b6 Examine the FastQC output \u00b6 You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the \u201832\u2019 mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality. Figure 2: Screenshot of FastQC output in Galaxy \u00b6 Quality trim the reads using Trimmomatic. \u00b6 From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \u201cInput FASTQ file (R1/first of pair)\u201d: ERR048396_1.fastq \u201cInput FASTQ file (R2/second of pair)\u201d: ERR048396_2.fastq \u201cPerform initial ILLUMINACLIP step?\u201d: Yes \u201cAdapter sequences to use\u201d: TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \u201cHow accurate \u2026 read alignment\u201d: 40 \u201cHow accurate \u2026 against a read\u201d: 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases \u2026 (LEADING) \u201cMinimum quality required to keep a base\u201d: 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute Figure 3: Screenshot of Trimmomatic inputs in Galaxy \u00b6 Examine the Trimmomatic output FastQ files. \u00b6 You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed. Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser \u00b6 The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser De novo assembly of the reads into contigs \u00b6 From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \u201cStart k-mer value\u201d: 55 \u201cEnd k-mer value\u201d: 69 In the input files section: \u201cSelect first set of reads\u201d: Trimmomatic on data 2 and data 1 (R1 paired) \u201cSelect second set of reads\u201d: Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \u201cSingle or paired end reads\u201d: Single \u201cSelect the reads\u201d: Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute . Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy \u00b6 Examine assembly output \u00b6 Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.) Figure 5: Screenshot of assembled contigs (a) and contig stats (b) \u00b6 a \u00b6 b \u00b6 Calculate some statistics on the assembled contigs \u00b6 From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \u201cFasta or multifasta file\u201d: Velvet Optimiser \u2026 Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them. Section 3: Extension. \u00b6 Examine the contig coverage depth and blast a high coverage contig against a protein database. Examine the contig coverage depth. \u00b6 Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \u201cFilter\u201d: Velvet Optimiser on data 8, data 7 and others: Contig stats \u201cWith the following condition\u201d: c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is. Extract a single sequence from the contigs file. \u00b6 Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \u201cFasta or multifasta file\u201d: Velvet Optimiser \u2026 : Contigs \u201cSequence ID (or partial): NODE_1_\u2026 (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format. Blast sequence to determine what it contains. \u00b6 We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage. Figure 6: Screenshot of the output from the NCBI Blast website \u00b6","title":"de novo assembly of Illumina reads using Velvet (Galaxy)"},{"location":"tutorials/assembly/assembly/#microbial-de-novo-assembly-for-illumina-data","text":"","title":"Microbial de novo Assembly for Illumina Data"},{"location":"tutorials/assembly/assembly/#introductory-tutorial","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introductory Tutorial"},{"location":"tutorials/assembly/assembly/#tutorial-overview","text":"In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism. What\u2019s not covered This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads. It does not cover more complicated aspects of assembly such as: Incorporation of other raw data types (454 reads, Sanger reads) Gap filling techniques for \u201cfinishing\u201d an assembly Measuring the accuracy of assemblies","title":"Tutorial Overview"},{"location":"tutorials/assembly/assembly/#background","text":"Read the background to the workshop here Where is the data in this tutorial from? The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the NCBI Short Read Archive (SRA) ( http://www.ncbi.nlm.nih.gov/sra/ ). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396. We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced. We will use software called Velvet (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above. The protocol: We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the Velvet Optimiser (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly. Follow this link for an overview of the protocol The protocol in a nutshell: Input: Raw reads from sequencer run on microbial DNA sample. Output: File of assembled scaffolds/contigs and associated information.","title":"Background"},{"location":"tutorials/assembly/assembly/#preparation","text":"","title":"Preparation"},{"location":"tutorials/assembly/assembly/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. (what is Galaxy ?) You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Register as a new user if you don\u2019t already have an account on that particular server NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.","title":"Login to Galaxy"},{"location":"tutorials/assembly/assembly/#import-the-dna-read-data-for-the-tutorial","text":"You can do this in a few ways. If you\u2019re using galaxy-tut.genome.edu.au : Go to Shared Data -> Published Histories and click on \u201c Microbial_assembly_input_data \u201d. Then click \u2018Import History\u2019 at top right, wait for the history to be imported to your account, and then \u2018start using this history\u2019 . This will create a new Galaxy history in your account with all of the required data files. Proceed to step 4. If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs. On the Galaxy tools panel, click on Get data -> Upload File . Click on the Paste/Fetch Data button. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz into the text box. Change the type to fastqsanger (Not fastqcsanger ). Click on the Paste/Fetch Data button again. Paste the URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz into the text box and change it\u2019s type to fastqsanger as well. Repeat the process for the last URL: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna , but make it\u2019s type fasta Click on the Start button. Once all of the uploads are at 100%, click on the Close button. When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the icon to the top right of the file name in the right hand Galaxy panel (the history panel) You should now have the following files in your Galaxy history: ERR048396_1.fastq - forward reads in fastq format ERR048396_2.fastq - reverse reads in fastq format illumina_adapters.fa - Illumina adapter sequences in fasta format","title":"Import the DNA read data for the tutorial."},{"location":"tutorials/assembly/assembly/#view-the-fastq-files","text":"Click on the icon to the top right of each fastq file to view the first part of the file If you\u2019re not familiar with the FASTQ format, click here for an overview NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019","title":"View the fastq files"},{"location":"tutorials/assembly/assembly/#section-1-quality-control","text":"The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.) More detailed description of FastQC quality analysis can be found here. More detailed description of Trimmomatic read quality filtering can be found here.","title":"Section 1: Quality control"},{"location":"tutorials/assembly/assembly/#run-fastqc-on-both-input-read-files","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > FastQC: Comprehensive QC (down the bottom of this category) and run with these parameters: \u201cFASTQ reads\u201d: ERR048396_1.fastq Use default for other fields Click Execute Now repeat the above process on the second read file: ERR048396_2.fastq Note: This may take a few minutes, depending on how busy Galaxy is. It is important to do both read files as the quality can be very different between them.","title":"Run FastQC on both input read files"},{"location":"tutorials/assembly/assembly/#figure-1-screenshot-of-fastqc-interface-in-galaxy","text":"","title":"Figure 1: Screenshot of FastQC interface in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-fastqc-output","text":"You should have two output objects from the first step: FastQC_ERR048396_1.fastqc.html FastQC_ERR048396_2.fastqc.html These are a html outputs which show the results of all of the tests FastQC performed on the read files. Click on the icon of each of these objects in turn to see the FastQC output. The main parts of the output to evaluate are: Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%. Per base sequence quality. In the plot you should see that most of the early bases are up around the \u201832\u2019 mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.","title":"Examine the FastQC output"},{"location":"tutorials/assembly/assembly/#figure-2-screenshot-of-fastqc-output-in-galaxy","text":"","title":"Figure 2: Screenshot of FastQC output in Galaxy"},{"location":"tutorials/assembly/assembly/#quality-trim-the-reads-using-trimmomatic","text":"From the tools menu in the left hand panel of Galaxy, select NGS QC and manipulation > Trimmomatic and run with these parameters (only the non-default selections are listed here): \u201cInput FASTQ file (R1/first of pair)\u201d: ERR048396_1.fastq \u201cInput FASTQ file (R2/second of pair)\u201d: ERR048396_2.fastq \u201cPerform initial ILLUMINACLIP step?\u201d: Yes \u201cAdapter sequences to use\u201d: TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq) \u201cHow accurate \u2026 read alignment\u201d: 40 \u201cHow accurate \u2026 against a read\u201d: 15 We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations. Click Insert Trimmomatic Operation Add Cut bases \u2026 (LEADING) \u201cMinimum quality required to keep a base\u201d: 15 Repeat the Insert Trimmomatic Operation for: Trim trailing bases, minimum quality: 15 Minimum length read: 35 Click Execute","title":"Quality trim the reads using Trimmomatic."},{"location":"tutorials/assembly/assembly/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy","text":"","title":"Figure 3: Screenshot of Trimmomatic inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-the-trimmomatic-output-fastq-files","text":"You should have 4 new objects in your history from the output of Trimmomatic: Trimmomatic on data 2 and data 1 (R1 Paired) Trimmomatic on data 2 and data 1 (R1 Unpaired) Trimmomatic on data 2 and data 1 (R2 Paired) Trimmomatic on data 2 and data 1 (R2 Unpaired) Click on the on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.","title":"Examine the Trimmomatic output FastQ files."},{"location":"tutorials/assembly/assembly/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser","text":"The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser. We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process. Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser","title":"Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"},{"location":"tutorials/assembly/assembly/#de-novo-assembly-of-the-reads-into-contigs","text":"From the tools menu in the left hand panel of Galaxy, select NGS: Assembly -> Velvet Optimiser and run with these parameters (only the non-default selections are listed here): \u201cStart k-mer value\u201d: 55 \u201cEnd k-mer value\u201d: 69 In the input files section: \u201cSelect first set of reads\u201d: Trimmomatic on data 2 and data 1 (R1 paired) \u201cSelect second set of reads\u201d: Trimmomatic on data 2 and data 1 (R2 paired) Click the Insert Input Files button and add the following: \u201cSingle or paired end reads\u201d: Single \u201cSelect the reads\u201d: Trimmomatic on data 2 and data 1 (R1 unpaired) Repeat the above process to add the other unpaired read set Trimmomatic on data 2 and data 1 (R2 unpaired) as well. Click Execute .","title":"De novo assembly of the reads into contigs"},{"location":"tutorials/assembly/assembly/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy","text":"","title":"Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"},{"location":"tutorials/assembly/assembly/#examine-assembly-output","text":"Once step 1 is complete, you should now have 2 new objects in your history: * VelvetOptimiser on data 9, data 7, and others: Contigs * VelvetOptimiser on data 9, data 7, and others: Contig Stats Click on the icon of the various objects. Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.) Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)","title":"Examine assembly output"},{"location":"tutorials/assembly/assembly/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b","text":"","title":"Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"},{"location":"tutorials/assembly/assembly/#a","text":"","title":"a"},{"location":"tutorials/assembly/assembly/#b","text":"","title":"b"},{"location":"tutorials/assembly/assembly/#calculate-some-statistics-on-the-assembled-contigs","text":"From the tools menu in the left hand panel of Galaxy, select FASTA Manipulation -> Fasta Statistics and run with these parameters: \u201cFasta or multifasta file\u201d: Velvet Optimiser \u2026 Contigs Click Execute Examine the Fasta Stats output You should now have one more object in your history: Fasta Statistics on data 10: Fasta summary stats Click on the icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.","title":"Calculate some statistics on the assembled contigs"},{"location":"tutorials/assembly/assembly/#section-3-extension","text":"Examine the contig coverage depth and blast a high coverage contig against a protein database.","title":"Section 3: Extension."},{"location":"tutorials/assembly/assembly/#examine-the-contig-coverage-depth","text":"Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1). We can easily filter out these short contigs from this information list by using the Filter and Sort -> Filter tool. Set the following: \u201cFilter\u201d: Velvet Optimiser on data 8, data 7 and others: Contig stats \u201cWith the following condition\u201d: c2 > 100 Click Execute The new data object in the history is called: Filter on data 11 . Click on its icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32. There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times! Lets have a look at one of these contigs and see if we can find out what it is.","title":"Examine the contig coverage depth."},{"location":"tutorials/assembly/assembly/#extract-a-single-sequence-from-the-contigs-file","text":"Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily. To do this we will use the tool: Fasta manipulation -> Fasta Extract Sequence Set the following: \u201cFasta or multifasta file\u201d: Velvet Optimiser \u2026 : Contigs \u201cSequence ID (or partial): NODE_1_\u2026 (for example) Click Execute The new data object in the history is called: Fasta Extract Sequence on data 10: Fasta . Click on its icon to view it. It is a single sequence in fasta format.","title":"Extract a single sequence from the contigs file."},{"location":"tutorials/assembly/assembly/#blast-sequence-to-determine-what-it-contains","text":"We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this: Bring up the sequence of the contig into the main window of the browser by clicking on the icon if it isn\u2019t already. Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser. Copy the selected sequence to the clipboard. Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi Under the BASIC BLAST section, click \u201cblastx\u201d. Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s). Change the Genetic code to: Bacteria and Archaea (11) Click the button labelled: BLAST After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.","title":"Blast sequence to determine what it contains."},{"location":"tutorials/assembly/assembly/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website","text":"","title":"Figure 6: Screenshot of the output from the NCBI Blast website"},{"location":"tutorials/assembly/spades/","text":"Assembly using Spades \u00b6 Background \u00b6 Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link . Learning objectives \u00b6 At the end of this tutorial you should be able to: assemble the reads using Spades, and examine the output assembly. Import and view data \u00b6 Galaxy \u00b6 If you are using Galaxy-Mel or Galaxy-Qld, import the files: In your browser, go to Galaxy-Mel or Galaxy-Qld In the top Galaxy panel, go to User and log in (or register, and then log in) In the top Galaxy panel, go to Shared Data and click on the drop down arrow Click on Histories Click on Genomics-workshop and then (over in the top right) Import history The files will now be listed in the right hand panel (your current history). (Alternatively, see here for information about how to start with Galaxy, and here for the link to import the Galaxy history for this tutorial, if you don\u2019t already have them in your history.) The data \u00b6 The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don\u2019t need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files. Assemble reads with Spades \u00b6 We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to Tools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files \u2192 Forward reads: mutant_R1.fastq Files \u2192 Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute Examine the output \u00b6 Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this:","title":"de novo assembly of Illumina reads using Spades (Galaxy)"},{"location":"tutorials/assembly/spades/#assembly-using-spades","text":"","title":"Assembly using Spades"},{"location":"tutorials/assembly/spades/#background","text":"Spades is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this link .","title":"Background"},{"location":"tutorials/assembly/spades/#learning-objectives","text":"At the end of this tutorial you should be able to: assemble the reads using Spades, and examine the output assembly.","title":"Learning objectives"},{"location":"tutorials/assembly/spades/#import-and-view-data","text":"","title":"Import and view data"},{"location":"tutorials/assembly/spades/#galaxy","text":"If you are using Galaxy-Mel or Galaxy-Qld, import the files: In your browser, go to Galaxy-Mel or Galaxy-Qld In the top Galaxy panel, go to User and log in (or register, and then log in) In the top Galaxy panel, go to Shared Data and click on the drop down arrow Click on Histories Click on Genomics-workshop and then (over in the top right) Import history The files will now be listed in the right hand panel (your current history). (Alternatively, see here for information about how to start with Galaxy, and here for the link to import the Galaxy history for this tutorial, if you don\u2019t already have them in your history.)","title":"Galaxy"},{"location":"tutorials/assembly/spades/#the-data","text":"The read set for today is from an imaginary Staphylococcus aureus bacterium with a miniature genome. The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument. The files we need for assembly are the mutant_R1.fastq and mutant_R2.fastq . (We don\u2019t need the reference genome sequences for this tutorial). The reads are paired-end. Each read is 150 bases long. The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!). Click on the View Data button (the ) next to one of the FASTQ sequence files.","title":"The data"},{"location":"tutorials/assembly/spades/#assemble-reads-with-spades","text":"We will perform a de novo assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.) Go to Tools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades Set the following parameters (leave other settings as they are): Run only Assembly : Yes [the Yes button should be darker grey] Kmers to use separated by commas: 33,55,91 [note: no spaces] Coverage cutoff: auto Files \u2192 Forward reads: mutant_R1.fastq Files \u2192 Reverse reads: mutant_R2.fastq Your tool interface should look like this: Click Execute","title":"Assemble reads with Spades"},{"location":"tutorials/assembly/spades/#examine-the-output","text":"Galaxy is now running Spades on the reads for you. When it is finished, you will have five (or more) new files in your history, including: two FASTA files of the resulting contigs and scaffolds two files for statistics about these the Spades logfile Click on the View Data button on each of the files. Note that the short reads have been assembled into much longer contigs. (However, in this case, the contigs have not been assembled into larger scaffolds.) The stats files will give you the length of each of the contigs, and the file should look something like this:","title":"Examine the output"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/","text":"Workshop title \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with \u2026 Description \u00b6 Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026 Requirements and preparation \u00b6 Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Preparing your laptop prior to starting this workshop \u00b6 Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026 Required Software \u00b6 IGV Software x or No additional software needs to be installed for this workshop. Required Data \u00b6 Data file or No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019 Background \u00b6 Little bit of history and context. Why is this important/useful\u2026 Section 1: Title of section 1 \u00b6 In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc. Section 2: Title of section 2 \u00b6 In this section we will \u2026 Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials.","title":"Bfb intro"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#workshop-title","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Workshop title"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#overview","text":"","title":"Overview"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with \u2026","title":"Skill level"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#description","text":"Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional.","title":"Description"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026","title":"Learning Objectives"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#requirements-and-preparation","text":"Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#required-software","text":"IGV Software x or No additional software needs to be installed for this workshop.","title":"Required Software"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#required-data","text":"Data file or No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#author-information","text":"Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019","title":"Author Information"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#background","text":"Little bit of history and context. Why is this important/useful\u2026","title":"Background"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#section-1-title-of-section-1","text":"In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc.","title":"Section 1: Title of section 1"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#section-2-title-of-section-2","text":"In this section we will \u2026","title":"Section 2: Title of section 2"},{"location":"tutorials/bioinformatics-bench-scientists/bfb-intro/bfb-intro/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials.","title":"Additional reading"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/","text":"Workshop title \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with \u2026 Description \u00b6 Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026 Requirements and preparation \u00b6 Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Preparing your laptop prior to starting this workshop \u00b6 Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026 Required Software \u00b6 IGV Software x or No additional software needs to be installed for this workshop. Required Data \u00b6 Data file or No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019 Background \u00b6 Little bit of history and context. Why is this important/useful\u2026 Section 1: Title of section 1 \u00b6 In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc. Section 2: Title of section 2 \u00b6 In this section we will \u2026 Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials.","title":"Cell type expression"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#workshop-title","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Workshop title"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#overview","text":"","title":"Overview"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with \u2026","title":"Skill level"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#description","text":"Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional.","title":"Description"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026","title":"Learning Objectives"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#requirements-and-preparation","text":"Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#required-software","text":"IGV Software x or No additional software needs to be installed for this workshop.","title":"Required Software"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#required-data","text":"Data file or No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#author-information","text":"Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019","title":"Author Information"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#background","text":"Little bit of history and context. Why is this important/useful\u2026","title":"Background"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#section-1-title-of-section-1","text":"In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc.","title":"Section 1: Title of section 1"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#section-2-title-of-section-2","text":"In this section we will \u2026","title":"Section 2: Title of section 2"},{"location":"tutorials/bioinformatics-bench-scientists/cell-type-expression/cell-type-expression/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials.","title":"Additional reading"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/","text":"Workshop title \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with \u2026 Description \u00b6 Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026 Requirements and preparation \u00b6 Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Preparing your laptop prior to starting this workshop \u00b6 Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026 Required Software \u00b6 IGV Software x or No additional software needs to be installed for this workshop. Required Data \u00b6 Data file or No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019 Background \u00b6 Little bit of history and context. Why is this important/useful\u2026 Section 1: Title of section 1 \u00b6 In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc. Section 2: Title of section 2 \u00b6 In this section we will \u2026 Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials.","title":"Temporal spatial gene expression"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#workshop-title","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Workshop title"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#overview","text":"","title":"Overview"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with \u2026","title":"Skill level"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#description","text":"Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional.","title":"Description"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026","title":"Learning Objectives"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#requirements-and-preparation","text":"Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#required-software","text":"IGV Software x or No additional software needs to be installed for this workshop.","title":"Required Software"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#required-data","text":"Data file or No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#author-information","text":"Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019","title":"Author Information"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#background","text":"Little bit of history and context. Why is this important/useful\u2026","title":"Background"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#section-1-title-of-section-1","text":"In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc.","title":"Section 1: Title of section 1"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#section-2-title-of-section-2","text":"In this section we will \u2026","title":"Section 2: Title of section 2"},{"location":"tutorials/bioinformatics-bench-scientists/temporal-spatial-gene-expression/temporal-spatial-gene-expression/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials.","title":"Additional reading"},{"location":"tutorials/bionitio/bionitio/","text":"Command-line software development \u00b6 To set up a new bioinformatics tool, we recommend that you follow best practice in software development. Many of these best-practice features are implemented in a template called \u201cBionitio\u201d. You can set up the template in one of many programming languages and then modify/extend your tool. Link to Bionitio: https://github.com/bionitio-team/bionitio See the README for an overview, and see the Wiki for a step-by-step guide to project setup.","title":"Command-line software development"},{"location":"tutorials/bionitio/bionitio/#command-line-software-development","text":"To set up a new bioinformatics tool, we recommend that you follow best practice in software development. Many of these best-practice features are implemented in a template called \u201cBionitio\u201d. You can set up the template in one of many programming languages and then modify/extend your tool. Link to Bionitio: https://github.com/bionitio-team/bionitio See the README for an overview, and see the Wiki for a step-by-step guide to project setup.","title":"Command-line software development"},{"location":"tutorials/cwl/cwl/","text":"Summary \u00b6 Common Workflow Language (or CWL), is a growing language for defining workflows in a cross-platform and cross-domain manner. In biology in particular, we need workflows to automate complex analyses such as DNA variant calling, RNA sequencing, and genome assembly. CWL provides a simple and well-defined format for automating these analysis by specifying their stages and connections using readable CWL documents. CWL makes use of a number of existing standards, including support for cluster computing using SLURM or PBS, containerisation using Docker, and deployment using common packaging formats. In addition, the CWL ecosystem has grown to include workflow visualisation tools, graphical workflow editors, libraries for interacting with CWL programatically and tools that convert to and from CWL and other workflow formats. Outcomes \u00b6 At the end of the course, you will be able to: Find and use CWL tool definitions online Use the Rabix Composer, a graphical editor for CWL Understand how to write CWL tool definitions for command line tools Use Docker with CWL to provide software dependencies and ensure reproducibility Join CWL tools into a workflow Read and write CWL files written in YAML Understand advanced CWL features like secondary files, parameter references and subworkflows Run CWL workflows on local and HPC systems Requirements \u00b6 General \u00b6 This workshop is aimed at anyone with basic Unix command-line experience Attendees are required to bring their own laptop computers Software \u00b6 All of this software is free, and should run on any operating system (Mac, Windows, or Linux): Rabix Composer: A GUI for writing CWL https://github.com/rabix/composer/releases Download the .dmg (Mac), .exe (Windows) or .AppImage (Linux) Docker A system for managing containers https://store.docker.com/search?type=edition&offering=community Python 2.7 or above If you don\u2019t have any version of python installed, Python 3.6 is preferable https://www.python.org/downloads/ cwltool A command-line executor for CWL This has to be installed using the command line https://github.com/common-workflow-language/cwltool#install A text editor for code If you don\u2019t already have a favourite, I recommend Atom https://atom.io/ Slides \u00b6 Workshop Slides (use the arrow keys to navigate) Part 1: Introduction Part 2: Tools Part 3: Writing Workflows Part 4: YAML","title":"Common Workflow Language for Bioinformatics"},{"location":"tutorials/cwl/cwl/#summary","text":"Common Workflow Language (or CWL), is a growing language for defining workflows in a cross-platform and cross-domain manner. In biology in particular, we need workflows to automate complex analyses such as DNA variant calling, RNA sequencing, and genome assembly. CWL provides a simple and well-defined format for automating these analysis by specifying their stages and connections using readable CWL documents. CWL makes use of a number of existing standards, including support for cluster computing using SLURM or PBS, containerisation using Docker, and deployment using common packaging formats. In addition, the CWL ecosystem has grown to include workflow visualisation tools, graphical workflow editors, libraries for interacting with CWL programatically and tools that convert to and from CWL and other workflow formats.","title":"Summary"},{"location":"tutorials/cwl/cwl/#outcomes","text":"At the end of the course, you will be able to: Find and use CWL tool definitions online Use the Rabix Composer, a graphical editor for CWL Understand how to write CWL tool definitions for command line tools Use Docker with CWL to provide software dependencies and ensure reproducibility Join CWL tools into a workflow Read and write CWL files written in YAML Understand advanced CWL features like secondary files, parameter references and subworkflows Run CWL workflows on local and HPC systems","title":"Outcomes"},{"location":"tutorials/cwl/cwl/#requirements","text":"","title":"Requirements"},{"location":"tutorials/cwl/cwl/#general","text":"This workshop is aimed at anyone with basic Unix command-line experience Attendees are required to bring their own laptop computers","title":"General"},{"location":"tutorials/cwl/cwl/#software","text":"All of this software is free, and should run on any operating system (Mac, Windows, or Linux): Rabix Composer: A GUI for writing CWL https://github.com/rabix/composer/releases Download the .dmg (Mac), .exe (Windows) or .AppImage (Linux) Docker A system for managing containers https://store.docker.com/search?type=edition&offering=community Python 2.7 or above If you don\u2019t have any version of python installed, Python 3.6 is preferable https://www.python.org/downloads/ cwltool A command-line executor for CWL This has to be installed using the command line https://github.com/common-workflow-language/cwltool#install A text editor for code If you don\u2019t already have a favourite, I recommend Atom https://atom.io/","title":"Software"},{"location":"tutorials/cwl/cwl/#slides","text":"Workshop Slides (use the arrow keys to navigate) Part 1: Introduction Part 2: Tools Part 3: Writing Workflows Part 4: YAML","title":"Slides"},{"location":"tutorials/docker/docker/","text":"Overview \u00b6 Containerisation is a method of bundling an application or pipeline with all its dependencies, from language runtimes like Python and R to the operating system itself. This technology has already revolutionised web development by providing a simple way to run web applications a in precisely controlled environment, regardless of which computer system they are running on. This workshop will explain how these advantages can be easily applied to bioinformatics analysis, to ensure 100% reproducibility of your work, along with easy distribution of your pipelines to other users without the need for complex installation. Learning Objectives \u00b6 At the end of the course, you will be able to: understand what containerisation is, and why you might use it in bioinformatics be familiar with some common containerisation tools are, and when to use each of them find and run containers built by other people run containers on HPC systems (like Melbourne Bioinformatics) build your own application into a container (containerisation) use containers as elements of a bioinformatics pipeline, and distribute your container online. Requirements \u00b6 This workshop is aimed at anyone with basic Unix command-line experience. Attendees are required to bring their own laptop computers. Windows users should have PuTTY installed Slides \u00b6 Workshop Slides (use the arrow keys to navigate) Part 1: Docker and Containers Part 2: Running Containers Part 3: Making your Own Image Part 4: Docker on HPC","title":"Containerized Bioinformatics"},{"location":"tutorials/docker/docker/#overview","text":"Containerisation is a method of bundling an application or pipeline with all its dependencies, from language runtimes like Python and R to the operating system itself. This technology has already revolutionised web development by providing a simple way to run web applications a in precisely controlled environment, regardless of which computer system they are running on. This workshop will explain how these advantages can be easily applied to bioinformatics analysis, to ensure 100% reproducibility of your work, along with easy distribution of your pipelines to other users without the need for complex installation.","title":"Overview"},{"location":"tutorials/docker/docker/#learning-objectives","text":"At the end of the course, you will be able to: understand what containerisation is, and why you might use it in bioinformatics be familiar with some common containerisation tools are, and when to use each of them find and run containers built by other people run containers on HPC systems (like Melbourne Bioinformatics) build your own application into a container (containerisation) use containers as elements of a bioinformatics pipeline, and distribute your container online.","title":"Learning Objectives"},{"location":"tutorials/docker/docker/#requirements","text":"This workshop is aimed at anyone with basic Unix command-line experience. Attendees are required to bring their own laptop computers. Windows users should have PuTTY installed","title":"Requirements"},{"location":"tutorials/docker/docker/#slides","text":"Workshop Slides (use the arrow keys to navigate) Part 1: Docker and Containers Part 2: Running Containers Part 3: Making your Own Image Part 4: Docker on HPC","title":"Slides"},{"location":"tutorials/galaxy-workflows/","text":"PR reviewers and advice: Simon Gladman, Clare Sloggett, Anna Syme Current slides: https://docs.google.com/presentation/d/1jOp5hH-NHRZahcUkPEQ3jNv-MSr0wSGpL4UsBqZECC4 (Simon\u2019s) Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Galaxy Workflows \u00b6 Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Background \u00b6 This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow Section 1: Preparation. \u00b6 The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy Australia Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don\u2019t use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button. Section 2: Create and run a workflow. \u00b6 This section will show you two different methods to create a workflow and then how to run one. Import the workflow history \u00b6 In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn\u2019t have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \u201cSaved Histories.\u201d From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \u201c workflow_finished \u201d and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right. Workflow creation: Method 1 \u00b6 We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn\u2019t import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows. Some discussion \u00b6 Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor.. Workflow Creation: Method 2 \u00b6 We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file. Step 1: Create a workflow name and edit space. \u00b6 Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \u201cWorkflow Name\u201d text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid. Step 2: Open the editor and place component tools \u00b6 Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \u201cReference data\u201d output to reference to \u201cUse the following dataset as the reference sequence\u201d \u201cReads 1\u201d output to \u201cSelect first set of reads\u201d \u201cReads 2\u201d output to \u201cSelect second set of reads\u201d Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \u201cChoose the source for the reference list:\u201d to History Connect \u201cMap with BWA\u201d bam output to \u201cFreebayes\u2019\u201d bam input. Connect the \u201cReference data\u201d output to \u201cFreebayes\u2019\u201d Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \u201cChoose parameter selection level\u201d: Complete list of all options \u201cPopulation model options\u201d: Set population model options \u201cSet ploidy for the analysis\u201d: 1 \u201cInput filters\u201d: Set input filters \u201cExclude alignments from analysis if they have a mapping quality less than\u201d: 20 \u201cExclude alleles from analysis if their supporting base quality is less than\u201d: 20 \u201cRequire at least this fraction of observations \u2026 to evaluate the position\u201d: 0.9 \u201cRequire at least this count of observations .. to evaluate the position\u201d: 10 \u201cPopulation and mappability priors\u201d: Set population and mappability priors \u201cDisable incorporation of prior expectations about observations\u201d: Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \u201cFreebayes\u2019\u201d output_vcf to the \u201cfilter\u201d input. In the right hand pane, change the following: \u201cWith the following condition\u201d: c6 > 500 \u201cNumber of header lines to skip\u201d: 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \u201cMap with BWA\u2019s\u201d bam file output. Click on the star next to \u201cFilter\u2019s\u201d output vcf. Step 3: Save it! \u00b6 Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it! Running the workflow \u00b6 We will now make a new history called \u201cTest\u201d and run the workflow on it\u2019s data. Create the new history \u00b6 From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \u201cNew History Named:\u201d Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen Run the workflow \u00b6 On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli \u2026 .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn. What now? \u00b6 Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"Introduction to Galaxy Workflows"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#galaxy-workflows","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Galaxy Workflows"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#background","text":"This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics: Logging in to the server How to construct and use a workflow by various methods How to share a workflow","title":"Background"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: Launch a GVL Galaxy Instance or you can use the public Galaxy Australia Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don\u2019t use IE or Safari.) If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click the Submit button If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click on the Submit button.","title":"Section 1: Preparation."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#section-2-create-and-run-a-workflow","text":"This section will show you two different methods to create a workflow and then how to run one.","title":"Section 2: Create and run a workflow."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#import-the-workflow-history","text":"In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn\u2019t have the appropriate history, there are instructions to create it here . Alternatively, you can extract a workflow from any history you have in your \u201cSaved Histories.\u201d From the menu at the top of the Galaxy window, click Shared Data -> Histories Find the history called \u201c workflow_finished \u201d and click on it. Then click on Import history at the top right of the screen. Change the name if you wish and then click Import This history should now be in your history pane on the right.","title":"Import the workflow history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-1","text":"We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow. Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn\u2019t import it, you should be able to complete this step with any suitable history. Now we will create the workflow. Click on the histories menu button at the top of the history pane. Click Extract Workflow You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and: Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d Click Create Workflow The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.","title":"Workflow creation: Method 1"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#some-discussion","text":"Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence. More interesting though is to: Click on the Workflows link in the top menu Click on the down arrow on your workflow\u2019s button. Click Edit A visualisation of your workflow will appear. Note the connections and the steps. Next we\u2019ll go through how to create this workflow using the editor..","title":"Some discussion"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-2","text":"We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.","title":"Workflow Creation: Method 2"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-1-create-a-workflow-name-and-edit-space","text":"Click on Workflow in Galaxy\u2019s menu. Click on the Create New Workflow button. In the \u201cWorkflow Name\u201d text box type: Variants from scratch Click the Create button. You should now be presented with a blank workflow editing grid.","title":"Step 1: Create a workflow name and edit space."},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-2-open-the-editor-and-place-component-tools","text":"Add three input datafiles. In the Workflow control section of the tool pane, click on Inputs -> Input dataset three times. Spread them out towards the left hand side of the workflow grid by clicking and dragging them around. For each one, change their name. Click on each input box in turn In the right hand pane (where the history usually is), change the name to: Reference data Reads 1 Reads 2 - respectively. If you click on Input dataset in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press Enter to make the change. Add in the BWA mapping step. Click on NGS: Mapping -> Map with BWA in the tool pane. BWA will be added to the workflow grid. In the right hand pane (where the history usually is), change the following parameters. Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to Use a genome from history. Note that the BWA box on the grid changes to match these settings. Connect the tools together. Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.) \u201cReference data\u201d output to reference to \u201cUse the following dataset as the reference sequence\u201d \u201cReads 1\u201d output to \u201cSelect first set of reads\u201d \u201cReads 2\u201d output to \u201cSelect second set of reads\u201d Add in the Freebayes (Variant Calling step.) Click on NGS: Variant Calling -> Freebayes In the right hand pane, change the following: \u201cChoose the source for the reference list:\u201d to History Connect \u201cMap with BWA\u201d bam output to \u201cFreebayes\u2019\u201d bam input. Connect the \u201cReference data\u201d output to \u201cFreebayes\u2019\u201d Use the following dataset as the reference sequence input. If you\u2019re keen - Note: this is optional. Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes. \u201cChoose parameter selection level\u201d: Complete list of all options \u201cPopulation model options\u201d: Set population model options \u201cSet ploidy for the analysis\u201d: 1 \u201cInput filters\u201d: Set input filters \u201cExclude alignments from analysis if they have a mapping quality less than\u201d: 20 \u201cExclude alleles from analysis if their supporting base quality is less than\u201d: 20 \u201cRequire at least this fraction of observations \u2026 to evaluate the position\u201d: 0.9 \u201cRequire at least this count of observations .. to evaluate the position\u201d: 10 \u201cPopulation and mappability priors\u201d: Set population and mappability priors \u201cDisable incorporation of prior expectations about observations\u201d: Yes Add in the Filter step. Click on Filter and Sort - > Filter Connect \u201cFreebayes\u2019\u201d output_vcf to the \u201cfilter\u201d input. In the right hand pane, change the following: \u201cWith the following condition\u201d: c6 > 500 \u201cNumber of header lines to skip\u201d: 56 Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore: Select workflow outputs. Click on the star next to \u201cMap with BWA\u2019s\u201d bam file output. Click on the star next to \u201cFilter\u2019s\u201d output vcf.","title":"Step 2: Open the editor and place component tools"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#step-3-save-it","text":"Click on the at the top of the workflow grid and select Save . Congratulations. You\u2019ve just created a Galaxy workflow. Now to run it!","title":"Step 3: Save it!"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#running-the-workflow","text":"We will now make a new history called \u201cTest\u201d and run the workflow on it\u2019s data.","title":"Running the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#create-the-new-history","text":"From the Histories Menu, select Copy Datasets Select the 2 x fastq files and the Ecoli .fna file. Under destination history, enter Test into \u201cNew History Named:\u201d Click Copy History Items button Click on the link to the new history in the green bar at the top of the screen","title":"Create the new history"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#run-the-workflow","text":"On the tools pane, click All Workflows Select the Variants from scratch workflow (or whatever you called it.) Give it the correct files. Ecoli \u2026 .fna for Reference data bacterial_std_err_1.fastq for Reads 1 bacterial_std_err_2.fastq for Reads 2 Click Run Workflow Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.","title":"Run the workflow"},{"location":"tutorials/galaxy-workflows/galaxy-workflows/#what-now","text":"Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.","title":"What now?"},{"location":"tutorials/galaxy-workflows/history_creation/","text":"History creation instructions for Workflow tutorial \u00b6 Use this set of instructions to create the base history for the \u201cExtract Workflow\u201d section of the workflows tutorial. Step 1: Import the raw datafiles \u00b6 Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url\u2019s (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file. Step 2: Run BWA \u00b6 Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \u201cWill you select a reference genome from your history or use a built-in index?\u201d: Use a genome from history and build index \u201cUse the following dataset as the reference sequence\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \u201cSelect first set of reads\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \u201cSelect second set of reads\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference. Step 3: Run Freebayes \u00b6 Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \u201cChoose the source for the reference genome\u201d: History \u201cBAM file\u201d: Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \u201cUse the following dataset as the reference sequence\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf). Step 4: Filter the VCF file. \u00b6 Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \u201cFilter\u201d: FreeBayes on data 3 and data 4 (variants) \u201cWith following condition\u201d: c6 > 500 \u201cNumber of header lines to skip\u201d: 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"History creation instructions for Workflow tutorial"},{"location":"tutorials/galaxy-workflows/history_creation/#history-creation-instructions-for-workflow-tutorial","text":"Use this set of instructions to create the base history for the \u201cExtract Workflow\u201d section of the workflows tutorial.","title":"History creation instructions for Workflow tutorial"},{"location":"tutorials/galaxy-workflows/history_creation/#step-1-import-the-raw-datafiles","text":"Create a new blank history by clicking on the history menu , then Create New Use the upload data tool to upload the data files from a remote repository.. Click Get Data -> Upload File Click Paste/Fetch data In the box paste the following two url\u2019s (one per line): https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz and https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz Change the Type to fastqsanger Click the Paste/Fetch data button again. Paste https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Change the Type on this file to fasta Click the Start button. Click the Close button. After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.","title":"Step 1: Import the raw datafiles"},{"location":"tutorials/galaxy-workflows/history_creation/#step-2-run-bwa","text":"Now we will run BWA on these files to map the reads to the reference. In the tools menu, click NGS: Mapping -> Map with BWA Set the following in the tool interface: \u201cWill you select a reference genome from your history or use a built-in index?\u201d: Use a genome from history and build index \u201cUse the following dataset as the reference sequence\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna \u201cSelect first set of reads\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq \u201cSelect second set of reads\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq Click Execute This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.","title":"Step 2: Run BWA"},{"location":"tutorials/galaxy-workflows/history_creation/#step-3-run-freebayes","text":"Now we will run Freebayes to call variants in out reads compared with the reference. In the tools menu, click NGS: Variant Analysis -> Freebayes Set the following in the tool interface: \u201cChoose the source for the reference genome\u201d: History \u201cBAM file\u201d: Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format) \u201cUse the following dataset as the reference sequence\u201d: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna Click Execute This will run Freebayes on your BAM file and will result in a variant calling format file (vcf).","title":"Step 3: Run Freebayes"},{"location":"tutorials/galaxy-workflows/history_creation/#step-4-filter-the-vcf-file","text":"Now we will filter the vcf file to something sensible. In the tools menu, click Filter and Sort -> Filter Set the following in the tool interface: \u201cFilter\u201d: FreeBayes on data 3 and data 4 (variants) \u201cWith following condition\u201d: c6 > 500 \u201cNumber of header lines to skip\u201d: 56 Click Execute This will filter out VCF file. You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial. Return to it here","title":"Step 4: Filter the VCF file."},{"location":"tutorials/galaxy_101/","text":"PR reviewers and advice: Simon Gladman, Gayle Philip, Clare Sloggett, Jessica Chung, Anna Syme Current slides: https://docs.google.com/presentation/d/1dzHagGkswjH7MOZ7OACVXGU-riBs33K3J5lWpnCpPhs (Simon\u2019s) Other slides: None yet","title":"Home"},{"location":"tutorials/galaxy_101/galaxy_101/","text":".image-header-gvl { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; -webkit-column-gap: 140px; -moz-column-gap: 140px; column-gap: 140px; } Introduction to Galaxy \u00b6 Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI) Overview \u00b6 This beginners tutorial will introduce Galaxy\u2019s interface, tool use, histories, and get new users of the Genomics Virtual Laboratory up and running. You can follow this tutorial with the Galaxy Workflows tutorial to learn about workflows. Galaxy is an open source, web-based platform for accessible, reproducible, and transparent computational biomedical research. It allows users without programming experience to easily specify parameters and run individual tools as well as larger workflows. It also captures run information so that any user can repeat and understand a complete computational analysis. Finally, it allows users to share and publish analyses via the web. Learning Objectives \u00b6 At the end of the course, you will be able to: login to a Galaxy server. upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output. Requirements \u00b6 This is a hands-on workshop and attendees should bring their own laptops. Background \u00b6 Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools Section 1: Preparation. \u00b6 The purpose of this section is to get you to log in to the server. Open your browser. We recommend Firefox or Chrome (please don\u2019t use Internet Explorer or Safari). Go to the Galaxy Australia server. Alternatively, you can use a different Galaxy server - a list of available servers is here . If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit Section 2: Getting data into Galaxy \u00b6 There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \u201cFile Type\u201d built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it. Method 1: Upload a file from your own computer \u00b6 With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz (To download this file, copy the link into a new browser tab, and press enter. The file should now download.) From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \u201cType\u201d (= file format) to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history. Method 2: Upload a file from a URL \u00b6 If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it. Method 2 (again): Get data from a Data Library \u00b6 Now we are going to get another file from a shared Data Library. Go to the menu at the top of the screen and click Shared Data -> Data Libraries . Click on the Library: \u201cGalaxy Australia Training Material\u201d then \u201cGalaxy_101\u201d To add the MRSA0252.fa file to our history click on the checkbox next to it Then click the \u201cTo History\u201d button at the top of the page and select \u201cAs Datasets\u201d Click the \u201cImport\u201d button Finally, click \u201cAnalyse Data\u201d in the menu at the top of the screen to return to your history. The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this. The data \u00b6 Though we aren\u2019t going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illumina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format. Section 3: Play with the tools \u00b6 The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \u201cName\u201d text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools. Example 1: Histogram and summary statistics \u00b6 The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \u201cCut Columns\u201d to: c1,c6 \u201cDelimited by\u201d: Tab \u201cCut from\u201d: Contig_stats.txt Click Execute Examine the new file by clicking on its icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \u201cRemove First\u201d: 1 \u201cfrom\u201d: Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \u201cDataset\u201d: Remove beginning on Data X \u201cNumerical column for X axis\u201d: c2 \u201cNumber of breaks\u201d: 25 \u201cPlot title\u201d: Histogram of Contig Coverage \u201cLabel for X axis\u201d: Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \u201cSummary statistics on\u201d: Remove beginning on Data X \u201cColumn or expression\u201d: c2 Click Execute Example 2: Convert Fastq to Fasta \u00b6 This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \u201cFASTQ file to convert\u201d: Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2. Example 3: Find Ribosomal RNA Features in a DNA Sequence \u00b6 This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA\u2019s in a sequence From the tool panel, click on NGS: Annotation -> barrnap and set the following: \u201cFasta file\u201d: MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the icon.) Now let\u2019s say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \u201cSelect lines from\u201d: (whatever you called the barrnap gff3 output) \u201cthat\u201d: Matching \u201cthe pattern\u201d: 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations! What now? \u00b6 Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That\u2019s it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"Galaxy 101"},{"location":"tutorials/galaxy_101/galaxy_101/#introduction-to-galaxy","text":"Written and maintained by Simon Gladman - Melbourne Bioinformatics (formerly VLSCI)","title":"Introduction to Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#overview","text":"This beginners tutorial will introduce Galaxy\u2019s interface, tool use, histories, and get new users of the Genomics Virtual Laboratory up and running. You can follow this tutorial with the Galaxy Workflows tutorial to learn about workflows. Galaxy is an open source, web-based platform for accessible, reproducible, and transparent computational biomedical research. It allows users without programming experience to easily specify parameters and run individual tools as well as larger workflows. It also captures run information so that any user can repeat and understand a complete computational analysis. Finally, it allows users to share and publish analyses via the web.","title":"Overview"},{"location":"tutorials/galaxy_101/galaxy_101/#learning-objectives","text":"At the end of the course, you will be able to: login to a Galaxy server. upload data to a Galaxy server from: A file on your local computer A file on a remote datastore with an accessible URL. use tools in Galaxy by: Accessing the tool via the tool menu Using the tool interface to run the particular tool Viewing/accessing the tool output.","title":"Learning Objectives"},{"location":"tutorials/galaxy_101/galaxy_101/#requirements","text":"This is a hands-on workshop and attendees should bring their own laptops.","title":"Requirements"},{"location":"tutorials/galaxy_101/galaxy_101/#background","text":"Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc. There are some introductory slides available here . Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial. This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics: Logging in to the server Getting data into galaxy How to access the tools Using to use some common tools","title":"Background"},{"location":"tutorials/galaxy_101/galaxy_101/#section-1-preparation","text":"The purpose of this section is to get you to log in to the server. Open your browser. We recommend Firefox or Chrome (please don\u2019t use Internet Explorer or Safari). Go to the Galaxy Australia server. Alternatively, you can use a different Galaxy server - a list of available servers is here . If you have previously registered on this server just log in: On the top menu select: User -> Login Enter your password Click Submit If you haven\u2019t registered on this server, you\u2019ll need to now. On the top menu select: User -> Register Enter your email, choose a password, repeat it and add a (all lower case) one word name Click Submit","title":"Section 1: Preparation."},{"location":"tutorials/galaxy_101/galaxy_101/#section-2-getting-data-into-galaxy","text":"There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop. Start a new history for this workshop. To do this: Click on the history menu button (the icon) at the top of the Histories panel. Select Create New It is important to note that Galaxy has the concept of \u201cFile Type\u201d built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc. We will tell Galaxy what type of file each one is as we upload it.","title":"Section 2: Getting data into Galaxy"},{"location":"tutorials/galaxy_101/galaxy_101/#method-1-upload-a-file-from-your-own-computer","text":"With this method you can get most of the files on your own computer into Galaxy. (there is a size limit) Download the following file to your computer: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz (To download this file, copy the link into a new browser tab, and press enter. The file should now download.) From the Galaxy tool panel, click on Get Data -> Upload File Click the Choose File button Find and select the Contig_stats.txt.gz file you downloaded and click Open Set the \u201cType\u201d (= file format) to tabular Click the Start button Once the progress bar reaches 100%, click the Close button The file will now upload to your current history.","title":"Method 1: Upload a file from your own computer"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-upload-a-file-from-a-url","text":"If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy. From the tool panel, click on Get Data -> Upload File Click on the Paste/Fetch Data button Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz Set the file format to fastqsanger (not fastqcsanger) Click Start Once the progress bar has reached 100%, click Close Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.","title":"Method 2: Upload a file from a URL"},{"location":"tutorials/galaxy_101/galaxy_101/#method-2-again-get-data-from-a-data-library","text":"Now we are going to get another file from a shared Data Library. Go to the menu at the top of the screen and click Shared Data -> Data Libraries . Click on the Library: \u201cGalaxy Australia Training Material\u201d then \u201cGalaxy_101\u201d To add the MRSA0252.fa file to our history click on the checkbox next to it Then click the \u201cTo History\u201d button at the top of the page and select \u201cAs Datasets\u201d Click the \u201cImport\u201d button Finally, click \u201cAnalyse Data\u201d in the menu at the top of the screen to return to your history. The DNA sequence of Staphlococcus aureus MRSA252 will be loaded into your history as a fasta file. Your history should now look like this.","title":"Method 2 (again): Get data from a Data Library"},{"location":"tutorials/galaxy_101/galaxy_101/#the-data","text":"Though we aren\u2019t going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one. Contigs_stats.txt this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. ) The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly. bacterial_std_err_1.fastq.gz This file contains sequence reads as they would come off an Illumina sequencing machine. They are in fastq format. MRSA0252.fna This file contains the genome sequence of Staphylococcus aureus MRSA252 . It is in fasta format.","title":"The data"},{"location":"tutorials/galaxy_101/galaxy_101/#section-3-play-with-the-tools","text":"The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools. Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So: Click on the icon (edit) next to the file in the history called: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq In the \u201cName\u201d text box, give it a new name. Call it: Typical Fastq File Click the Save button. Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date. Ok, back to the tools.","title":"Section 3: Play with the tools"},{"location":"tutorials/galaxy_101/galaxy_101/#example-1-histogram-and-summary-statistics","text":"The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools. Click on the icon of the Contig_stats.txt file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this: 1. Cut out column 1 and column 6. From the tool panel, click on Text Manipulation -> Cut and set the following: Set \u201cCut Columns\u201d to: c1,c6 \u201cDelimited by\u201d: Tab \u201cCut from\u201d: Contig_stats.txt Click Execute Examine the new file by clicking on its icon. We now have 2 columns instead of the 18 in the original file. 2. Remove the Header lines of the new file. From the tool panel, click on Text Manipulation -> Remove beginning and set the following: \u201cRemove First\u201d: 1 \u201cfrom\u201d: Cut on data1 click Execute Note the the new file is the same as the previous one without the header line. 3. Make a histogram. From the tool panel, click on Graph/Display Data -> Histogram and set the following: \u201cDataset\u201d: Remove beginning on Data X \u201cNumerical column for X axis\u201d: c2 \u201cNumber of breaks\u201d: 25 \u201cPlot title\u201d: Histogram of Contig Coverage \u201cLabel for X axis\u201d: Coverage depth Click Execute Click on the icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs. 4. Calculate summary statistics for contig coverage depth. From the tool panel, click on Statistics -> Summary Statisitics and set the following: \u201cSummary statistics on\u201d: Remove beginning on Data X \u201cColumn or expression\u201d: c2 Click Execute","title":"Example 1: Histogram and summary statistics"},{"location":"tutorials/galaxy_101/galaxy_101/#example-2-convert-fastq-to-fasta","text":"This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data. Converter tool From the tool panel, click on Convert Formats -> FASTQ to FASTA and set the following: \u201cFASTQ file to convert\u201d: Typical Fastq File Click Execute This will have created a new Fasta file called FASTQ to FASTA on data 2.","title":"Example 2: Convert Fastq to Fasta"},{"location":"tutorials/galaxy_101/galaxy_101/#example-3-find-ribosomal-rna-features-in-a-dna-sequence","text":"This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence. 1. Find all of the ribosomal RNA\u2019s in a sequence From the tool panel, click on NGS: Annotation -> barrnap and set the following: \u201cFasta file\u201d: MRSA252.fna Click Execute A new file called barrnap on data 3 will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the icon.) Now let\u2019s say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool. 2. Filter the annotations to get the 23S RNAs From the tool panel, click on Filter and Sort -> Select and set the following: \u201cSelect lines from\u201d: (whatever you called the barrnap gff3 output) \u201cthat\u201d: Matching \u201cthe pattern\u201d: 23S (this will look for all the lines in the file that contain \u201c23S\u201d) Click Execute Now you have a gff3 file with just the 23S annotations!","title":"Example 3: Find Ribosomal RNA Features in a DNA Sequence"},{"location":"tutorials/galaxy_101/galaxy_101/#what-now","text":"Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server. That\u2019s it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn","title":"What now?"},{"location":"tutorials/genomespace/","text":"PR reviewers and advice: Yousef Kowsar Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/genomespace/genomespace/","text":"What is GenomeSpace? \u00b6 GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au . Prerequisites \u00b6 GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain. Registering a GenomeSpace account \u00b6 To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \u201cRegister new GenomeSpace user\u201d link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website. Making a swift container \u00b6 (These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven\u2019t used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \u201cObject Store\u201d and then \u201cContainers\u201d. To make a container, click \u201cCreate Container\u201d. Mounting a swift container \u00b6 Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to. Basic file manipulation \u00b6 Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \u201cCreate Subdirectory\u201d. You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \u201cpreview\u201d option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \u201cview file\u201d. Adding a Galaxy service to your account: \u00b6 PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \u201cAdd new\u201d button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName. File transfer to/from Galaxy \u00b6 PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \u201cSend to GenomeSpace\u201d. A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"Genomespace"},{"location":"tutorials/genomespace/genomespace/#what-is-genomespace","text":"GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer. GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next. The GVL GenomeSpace can be found at https://genomespace.genome.edu.au .","title":"What is GenomeSpace?"},{"location":"tutorials/genomespace/genomespace/#prerequisites","text":"GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.","title":"Prerequisites"},{"location":"tutorials/genomespace/genomespace/#registering-a-genomespace-account","text":"To register a GenomeSpace account you need to have a valid email address and do the following: Go to https://genomespace.genome.edu.au/ and click on the \u201cRegister new GenomeSpace user\u201d link: Enter your preferred username, password and the valid email address and click the Sign up button. Note: You will receive an error if the username has already been taken. If everything goes right you will see the following page. Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active. Go to https://genomespace.genome.edu.au and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace. In a few seconds you will be redirected to your Home page. On this page you can find the following items: Your username in the top right corner The menu bar The application bar Your home directory The directory under the name Shared to \u201cyour username\u201d contains any folders that have been shared to you through the GenomeSpace website. The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.","title":"Registering a GenomeSpace account"},{"location":"tutorials/genomespace/genomespace/#making-a-swift-container","text":"(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.) NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably. If you haven\u2019t used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ . Go to the NeCTAR dashboard at https://dashboard.rc.nectar.org.au . On the left hand side of the dashboard click on \u201cObject Store\u201d and then \u201cContainers\u201d. To make a container, click \u201cCreate Container\u201d.","title":"Making a swift container"},{"location":"tutorials/genomespace/genomespace/#mounting-a-swift-container","text":"Containers can be found under the Object Store link in NeCTAR\u2019s dashboard. To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container. You will see a new page as follows: To fill out this form you need the following parameters: OpenStack EndPoint: The default value should be correct for NeCTAR: https://keystone.rc.nectar.org.au:5000/v2.0/tokens User Name: This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below: Password: This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key: Login to your NeCTAR dashboard account. On the top right hand side of your Home click on the setting link Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.) Tenancy name: Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. Container name: The name of the container that you want to connect to.","title":"Mounting a swift container"},{"location":"tutorials/genomespace/genomespace/#basic-file-manipulation","text":"Under the containers directory you can perform basic file manipulation as follows: Creating a directory: To create a directory under another directory, right click on the source directory and select \u201cCreate Subdirectory\u201d. You will be asked for a name and in a few seconds your target directory will be created. Uploading a file into a directory: Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green. Deleting a file: To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds. Previewing a file: Under the right click menu you will find the \u201cpreview\u201d option, which will show the first 5000 bytes of a file. Downloading a file: To download a file simply right click on the file and select download. Your download will be started in a few seconds. Creating a public link: Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.) Creating a private link: Right click on the file and select the \u201cview file\u201d.","title":"Basic file manipulation"},{"location":"tutorials/genomespace/genomespace/#adding-a-galaxy-service-to-your-account","text":"PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first. The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows: From the Menu bar go to the manage menu and select Private Tool. From the opened window press the \u201cAdd new\u201d button. In the new window fill out the form as follows: Give a name to your Galaxy Give a description (Optional) Tool provider GVL (Optional) Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer Parameter name: URL Required: Ticked Allow multiple files: Ticked Multiple file Delimiter: , Select the files\u2019 types that you want your galaxy to work on Upload an image as an Icon for Galaxy (Optional) Press the save button. In a few seconds your Galaxy instance will be added to the Application bar. A sample page can be seen in the following image: Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch. Galaxy will be opened in a new window. (Note: Your browser may block the pop-up. Allow the pop-up accordingly). From the opened Galaxy login and under your username go to the preferences options and select the Manage OpenIDs links: From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL. From now on your Galaxy can talk to GenomeSpace under your UserName.","title":"Adding a Galaxy service to your account:"},{"location":"tutorials/genomespace/genomespace/#file-transfer-tofrom-galaxy","text":"PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ). Sending a file: From your Galaxy instance: Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy. From GenomeSpace: You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have connected Galaxy and GenomeSpace ). Receiving a file: Under each file click on the store icon and choose \u201cSend to GenomeSpace\u201d. A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.","title":"File transfer to/from Galaxy"},{"location":"tutorials/gvl_launch/","text":"PR reviewers and advice: Clare Sloggett, Simon Gladman, Nuwan Goonasekera Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/gvl_launch/gvl_launch/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } Launching a Personal GVL Server on the NeCTAR Research Cloud \u00b6 Tutorial Overview \u00b6 This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down Background \u00b6 What is the GVL? \u00b6 The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Australia , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources. What is NeCTAR? \u00b6 The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free). Section 1: Access the NeCTAR dashboard \u00b6 Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose \u2018The University of Melbourne\u2019 and not \u2018The University of Melbourne (with ECP)\u2019. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here . Section 2: Get your cloud credentials \u00b6 Launching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher to create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top \u2018API Access\u2019 tab. Click on the \u2018Download OpenStack RC File\u2019 button on the top right. A file containing your credentials will be saved to your downloads folder. This file will be needed later in the launch process. Next you must obtain your OpenStack password and record it securely for future use. If you have ever done this step before, you should reuse your previously saved password. To obtain your OpenStack password, click on your username on the top right hand corner and go to settings as shown below. Click on the reset password link. Once reset, your password will be displayed. Record this securely for all future GVL launches. Section 3: Launch your personal GVL instance \u00b6 In a new browser tab, go to launch.usegalaxy.org You will see the screen below. Select the first option from the list - \u201cGenomics Virtual Lab\u201d. You will be asked to login with your preferred social network account. Once logged in, perform the following steps. Select NeCTAR for the question \u201cOn which cloud would you like to launch your appliance\u201d Click \u201cload credentials from file\u201d and provide file you downloaded in Section 2.3 Provide the OpenStack password you obtained in Section 2.5. Click \u201cTest and Use these Credentials\u201d. The Next button will now be activated. Click the next button, and provide the following options. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Optional advanced options Toggle the \u2018Advanced cloudlaunch options\u2019 option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Deployment name: You can override the name with a name of your choice. It is recommended you choose a unique name if you launch multiple instances. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Root Volume Storage: Keep the default: Instance storage Placement Zone: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Click \u2018Launch\u2019 to launch a GVL. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced cloudlaunch options -> Placement Zone field. Section 4: Access your GVL instance \u00b6 Once your instance has finished launching, click on the Access address to access your GVL dashboard. If you accidentally closed the launch page, you can find your access address at any time by logging back into launch.usegalaxy.org and navigating to the \u201cMy Appliances\u201d section through the menu bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL. Section 5: GVL services \u00b6 Listed below are short descriptions of the services the GVL provides. Galaxy \u00b6 Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar. CloudMan \u00b6 CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username \u2018ubuntu\u2019 and your cluster password. You can also shut down your instance (permanently) with CloudMan. Lubuntu Desktop \u00b6 Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username \u2018ubuntu\u2019 and your cluster password. SSH \u00b6 Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username \u2018ubuntu\u2019 or the username \u2018researcher\u2019 and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software). JupyterHub \u00b6 JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username \u2018researcher\u2019 and your cluster password. You may need to install Python packages you intend to use via the command line beforehand. RStudio \u00b6 RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username \u2018researcher\u2019 and your cluster password. Public HTML \u00b6 This is a shared web-accessible folder. Any files you place in the directory / home / researcher / public_html will be publicly accessible. PacBio SMRT Portal \u00b6 PacBio\u2019s SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on \u2018Admin\u2019 in the top navigation bar. Log in with the username \u2018ubuntu\u2019 and your cluster password. The screen will now show all the tools available to be installed. Scroll down to \u2018SMRT Analysis\u2019, and click \u2018install\u2019. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user. Section 6: Shutting your machine down \u00b6 There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username \u2018ubuntu\u2019 and your cluster password. Click the Shut down\u2026 button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Gvl launch"},{"location":"tutorials/gvl_launch/gvl_launch/#launching-a-personal-gvl-server-on-the-nectar-research-cloud","text":"","title":"Launching a Personal GVL Server on the NeCTAR Research Cloud"},{"location":"tutorials/gvl_launch/gvl_launch/#tutorial-overview","text":"This document guides you to launch your own GVL Analysis platform, with Galaxy, using your default NeCTAR allocation. The tutorial will go over: Accessing the NeCTAR dashboard using Australian Access Federation (AAF) credentials Getting your NeCTAR Research Cloud credentials Launching the GVL image Accessing your GVL instance GVL services Shutting your machine down","title":"Tutorial Overview"},{"location":"tutorials/gvl_launch/gvl_launch/#background","text":"","title":"Background"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-the-gvl","text":"The Genomics Virtual Laboratory (GVL) is a research platform that can be deployed on the cloud. A private GVL server is a virtual machine running on the cloud and contains a pre-installed suite of tools for performing bioinformatics analyses. It differs from public GVL servers (such as the Galaxy Tutorial Server , Galaxy Australia , and Galaxy Queensland ) by providing full administrative access to the server, as well as the full suite of GVL services, whereas public GVL servers provide restricted access for security reasons. For example, public GVL servers do not provide access to the Ubuntu desktop, the Linux command line or JupyterHub at present. Accessing the GVL server is completely free on the Australian NeCTAR Research Cloud, provided that you have a account with NeCTAR with allocated resources.","title":"What is the GVL?"},{"location":"tutorials/gvl_launch/gvl_launch/#what-is-nectar","text":"The National eResearch Collaboration Tools and Resources project (NeCTAR) is an Australian programme that provides computing infrastructure and services to Australian researchers. The NeCTAR Cloud allows us to deploy virtual machines as a platform for research. While it is possible to launch the GVL on Amazon , you may have to pay Amazon usage charges (the GVL software itself is free).","title":"What is NeCTAR?"},{"location":"tutorials/gvl_launch/gvl_launch/#section-1-access-the-nectar-dashboard","text":"Login into the NeCTAR dashboard at dashboard.rc.nectar.org.au . Only members of the Australian Access Federation (AAF) can access Australian Research Cloud resources. Most Australian universities are members of the AAF, however, if you belong to an institution that is not a member of AAF, GVL Help may be able to provide you with credentials. You also have the option of using a commercial cloud provider such as AWS to host your server. Choose your organisation from the list and login using your credentials. If you are from the University of Melbourne, choose \u2018The University of Melbourne\u2019 and not \u2018The University of Melbourne (with ECP)\u2019. Login with your institutional username and password. If this is the first time you have accessed the Australian Research Cloud, you must agree to some terms and conditions. When you log in for the first time, you are automatically allocated a trial project which lasts for 3 months. This trial project allows you to launch a medium instance (with 2 cores) which is sufficient for launching a GVL instance. If you have projects which require more compute resources, you can apply for more allocation here .","title":"Section 1: Access the NeCTAR dashboard"},{"location":"tutorials/gvl_launch/gvl_launch/#section-2-get-your-cloud-credentials","text":"Launching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher to create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process. From the NeCTAR dashboard, on the left sidebar, navigate to Project > Compute > Access & Security Click on the top \u2018API Access\u2019 tab. Click on the \u2018Download OpenStack RC File\u2019 button on the top right. A file containing your credentials will be saved to your downloads folder. This file will be needed later in the launch process. Next you must obtain your OpenStack password and record it securely for future use. If you have ever done this step before, you should reuse your previously saved password. To obtain your OpenStack password, click on your username on the top right hand corner and go to settings as shown below. Click on the reset password link. Once reset, your password will be displayed. Record this securely for all future GVL launches.","title":"Section 2: Get your cloud credentials"},{"location":"tutorials/gvl_launch/gvl_launch/#section-3-launch-your-personal-gvl-instance","text":"In a new browser tab, go to launch.usegalaxy.org You will see the screen below. Select the first option from the list - \u201cGenomics Virtual Lab\u201d. You will be asked to login with your preferred social network account. Once logged in, perform the following steps. Select NeCTAR for the question \u201cOn which cloud would you like to launch your appliance\u201d Click \u201cload credentials from file\u201d and provide file you downloaded in Section 2.3 Provide the OpenStack password you obtained in Section 2.5. Click \u201cTest and Use these Credentials\u201d. The Next button will now be activated. Click the next button, and provide the following options. Password: Choose a strong password and remember it. This is the password you will use later to log into your instance. Optional advanced options Toggle the \u2018Advanced cloudlaunch options\u2019 option to see more options. For this tutorial, it is not necessary to modify any of the advanced options. Deployment name: You can override the name with a name of your choice. It is recommended you choose a unique name if you launch multiple instances. Instance type: Keep the default: Medium (2 vcpu / 8GB RAM) Root Volume Storage: Keep the default: Instance storage Placement Zone: You can also choose the region of where your server is hosted. If you are doing lots of data transfer, it may be beneficial to pick a location close to your physical location. More information about zones can be found here . Key pair: Key pairs are SSH credentials that can be used to access your instance. You can create and import key pairs in the NeCTAR dashboard by navigating to Project > Compute > Access & Security > Key Pairs and creating or importing a key pair. Click \u2018Launch\u2019 to launch a GVL. The launch process takes 2-5 minutes to start the machine and another 5 minutes to start and configure Galaxy. If an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try selecting a different availability zone under the advanced cloudlaunch options -> Placement Zone field.","title":"Section 3: Launch your personal GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-4-access-your-gvl-instance","text":"Once your instance has finished launching, click on the Access address to access your GVL dashboard. If you accidentally closed the launch page, you can find your access address at any time by logging back into launch.usegalaxy.org and navigating to the \u201cMy Appliances\u201d section through the menu bar. Explore the GVL dashboard. Have a read through of the services provided by the GVL.","title":"Section 4: Access your GVL instance"},{"location":"tutorials/gvl_launch/gvl_launch/#section-5-gvl-services","text":"Listed below are short descriptions of the services the GVL provides.","title":"Section 5: GVL services"},{"location":"tutorials/gvl_launch/gvl_launch/#galaxy","text":"Galaxy is a web-based platform for computational biomedical research. The GVL has a number of Galaxy tutorials available here . To begin using Galaxy, register as a user by navigating to User > Register on the top Galaxy bar.","title":"Galaxy"},{"location":"tutorials/gvl_launch/gvl_launch/#cloudman","text":"CloudMan is a cloud manager that manages services on your GVL instance. Use Cloudman to start and manage your Galaxy service and to add additional nodes to your compute cluster (if you have enough resources). You can log into CloudMan by using the username \u2018ubuntu\u2019 and your cluster password. You can also shut down your instance (permanently) with CloudMan.","title":"CloudMan"},{"location":"tutorials/gvl_launch/gvl_launch/#lubuntu-desktop","text":"Lubuntu is a lightweight desktop environment through which you can run desktop applications on your virtual machine through your web browser. You can also access the GVL command line utilities through the desktop. You can log into Lubuntu Desktop using the username \u2018ubuntu\u2019 and your cluster password.","title":"Lubuntu Desktop"},{"location":"tutorials/gvl_launch/gvl_launch/#ssh","text":"Secure Shell (SSH) is a network protocol that allows us to connect to a remotely machine. You can login to your virtual machine remotely through an SSH client. If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using OSX, open up a Terminal window. If you are unfamiliar with the command line and UNIX, many tutorials on UNIX can be found online . You can ssh into your machine using the either the username \u2018ubuntu\u2019 or the username \u2018researcher\u2019 and using your cluster password. It is recommended to use the researcher account when you are doing your computational research and use the ubuntu account when you need administrative powers (such as installing software).","title":"SSH"},{"location":"tutorials/gvl_launch/gvl_launch/#jupyterhub","text":"JupyterHub is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. Currently, JupyterHub can connect to Python2 and Python3 kernals. If you are unfamiliar with Python, there are many tutorials available online . You can log into JupyterHub with the username \u2018researcher\u2019 and your cluster password. You may need to install Python packages you intend to use via the command line beforehand.","title":"JupyterHub"},{"location":"tutorials/gvl_launch/gvl_launch/#rstudio","text":"RStudio Server gives browser-based access to RStudio, the popular programming and analysis environment for the R programming language. You can find out more about RStudio here , and the R programming language here . You can log into RStudio with the username \u2018researcher\u2019 and your cluster password.","title":"RStudio"},{"location":"tutorials/gvl_launch/gvl_launch/#public-html","text":"This is a shared web-accessible folder. Any files you place in the directory / home / researcher / public_html will be publicly accessible.","title":"Public HTML"},{"location":"tutorials/gvl_launch/gvl_launch/#pacbio-smrt-portal","text":"PacBio\u2019s SMRT Portal is an open source software suite for the analysis of single molecule, real-time sequencing data. Before you use SMRT Portal, you need to firstly install it through the Admin console. Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package. To install SMRT Portal from the GVL dashboard: Click on \u2018Admin\u2019 in the top navigation bar. Log in with the username \u2018ubuntu\u2019 and your cluster password. The screen will now show all the tools available to be installed. Scroll down to \u2018SMRT Analysis\u2019, and click \u2018install\u2019. The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed. When launching SMRT Portal for the first time, you will need to register yourself as a new user.","title":"PacBio SMRT Portal"},{"location":"tutorials/gvl_launch/gvl_launch/#section-6-shutting-your-machine-down","text":"There are two ways to terminate your instance. Terminating your instance is permanent and all data will be deleted (unless you have persistent volume storage which you will need to apply for). Via Cloudman Log into CloudMan by using the username \u2018ubuntu\u2019 and your cluster password. Click the Shut down\u2026 button under Cluster Controls. Via the NeCTAR dashboard Navigate to the Instances page by navigating to Project > Compute > Instances on the left panel. Find the instance you want to terminate, and on the right-most column (Actions), click on the arrow button, and select Terminate Instance .","title":"Section 6: Shutting your machine down"},{"location":"tutorials/hpc/","text":"PR reviewers and advice: Andrew Robinson, Peter Georgeson, Chol-hee Jung, Ben Moran Current slides: HTML in repository Other slides: None yet","title":"Home"},{"location":"tutorials/hpc/hpc/","text":"em {font-style: normal; font-family: courier new;} High-Performance Computing \u00b6 A hands-on-workshop covering High-Performance Computing (HPC). Overview \u00b6 Using High Performance Computing (HPC) resources such as Melbourne Bioinformatics in an effective and efficient manner is key to modern research. This workshop will introduce you to HPC environments and assist you to get on with your research. Learning Objectives \u00b6 At the end of the course, you will be able to: Define \u2018What is HPC?\u2019 Load software modules Submit jobs Select job queues Monitor your job\u2019s progress Know what resources you can request Select appropriate resources Requirements \u00b6 You will need a basic understanding of Unix, or you should have attended an Introduction to Unix workshop in the past. All participants are required to bring their own laptop computers. Introduction \u00b6 Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the main concepts of High-Performance Computing. The slides are available if you would like. Additionally the following reference material is available for later use: Reference Material ### What is an HPC? An HPC is simply a large collection of server-grade computers working together to solve large problems. * **Big**: HPCs typically have lots of CPUs and Memory and consequently large jobs. * **Shared**: There are usually lots of users making use of it at one time. * **Coordinated**: There is a coordinator program to ensure fair-use between its users. * **Compute Collection**: HPCs use a number of computers at once to solve lots of large jobs. **Figure**: The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. ### Why use HPCs? The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. * **Many CPUs**: HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. * **Large Memory**: Hundreds of GBs to multiple TBs of RAM are typical for each node. * **Efficient use**: Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. ### Software Modules There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. * **Packaged**: to avoid conflicts between software, each is packaged up into a module and only used on demand. * **Loadable**: before using a software module you need to load it. * **Versions**: given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. ### Job Submission Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. * **SLURM**: this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. * **Queues (Partition)**: when a job is submitted it is added to a work queue; in SLURM this is called a Partition. * **Batch**: HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen. #### Resources So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. 1. **CPUs**: most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. 2. **Memory**: you need to estimate (or guess) how much memory (RAM) your program needs. 3. **Nodes**: most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. 4. **Time**: like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. #### Job Types There are two types of jobs that you can submit: 1. **Shared**: a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. 2. **Exclusive**: an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node. Connecting to the HPC \u00b6 To begin this workshop you will need to connect to the HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC\u2019s tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: 1 2 3 The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Topic 1: Exploring an HPC \u00b6 An HPC (short for \u2018High-Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions. Exercises \u00b6 1.1) What is the contact email for your HPC\u2019s System Administrator? \u00b6 Hint When you login, you will be presented with a message; this is called the *Message Of The Day* and usually includes lots of useful information. On *barcoo* this includes a list of useful commands, the last login details for your account and the contact email of the system administrator Answer Depending on which computer you are working: * SNOWY & BARCOO: help@vlsci.unimelb.edu.au 1.2) Run the sinfo command. How many nodes are there in this hpc? \u00b6 Hint *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* and *barcoo[1,5]* is shorthand for *barcoo1* and *barcoo5* Additional Hint Have a look at the NODELIST column. Only count each node once. 1 2 3 4 5 6 7 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200 -00:00: 3 mix lims-hpc- [ 2 -4 ] compute* up 200 -00:00: 2 idle lims-hpc- [ 1 ,5 ] bigmem up 200 -00:00: 1 idle lims-hpc-1 8hour up 08 :00:00 3 mix lims-hpc- [ 2 -4 ] 8hour up 08 :00:00 3 idle lims-hpc- [ 1 ,5 ] ,lims-hpc-m NOTE: the above list will vary depending on the HPC setup. Answer The *sinfo* command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. * BARCOO: **70** (*barcoo001* through *barcoo070*) * SNOWY: **43** (*snowy001* through *snowy043*) Alternate Method An automatic (though more complex) way would have been running the following command: 1 $ scontrol show node | grep NodeName | wc -l Where: * *scontrol show node*: lists details of all nodes (over multiple lines) * *grep NodeName*: only shows the NodeName line * *wc -l*: counts the number of lines Topic 2: Software Modules \u00b6 Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily Exercises \u00b6 2.1) What happens if you run the module command without any options / arguments? \u00b6 Hint Literally type *module* and press *ENTER* key. Answer **Answer**: It prints an error followed by a list of available options / flags 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear 2.2) How do you find a list of available software? \u00b6 Hint Try the *module* command. Don't forget the *man* command to get help for a command Additional Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>) Answer The module command is used to show details of software modules (tools). **Answer**: 1 2 3 4 5 6 7 8 9 10 $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the */usr/local/Modules/modulefiles* line are the science software; before this are a few built-in ones that you can ignore. 2.3) How many modules are there starting with \u2018 f \u2019? \u00b6 Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the *avail* subcommand. Additional Hint > If an argument is given, then each directory in the MODULEPATH is searched for modulefiles > whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. Answer The man page told us that we could put a search term after *module avail*. 1 2 3 4 5 6 7 $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastStructure-gcc/20150320 freetype-gcc/2.5.3 fastx_toolkit-gcc/0.0.14 **Answer**: 26 modules NOTE: this was correct at time of writing this workshop and might increase over time so don't be alarmed if you got more Alternate Method To get a fully automated solution your could do the following command: 1 $ module -l avail 2 > & 1 | grep \"^f\" | wc -l Where: * *module -l avail*: lists all modules (in long format, i.e. one per line) * *2>&1*: merges output from *standard error* to the *standard output* so it can be feed into grep. For some reason the developers of the *module* command thought it was a good idea to output the module names on the *error* stream rather than the logical *output* stream. * *grep \"^f\"*: only shows lines beginning with *f* * *wc -l*: counts the number of lines 2.4) Run the pear command (without loading it), does it work? \u00b6 Hint This question is very literal Answer 1 2 $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' **Answer**: No, command not found 2.5) How would we load the pear module? \u00b6 Hint Check the man page for *module* again and look for a subcommand that might load modules; it is quite literal as well. Additional Hint Run the command *man module* Use a search to find out about the *load* subcommand (e.g. /load<enter>) Answer 1 $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software. 2.6) Now it\u2019s *load*ed, run pear again, what does it do? \u00b6 Hint The paper citation gives a clue. Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 $ module load pear-gcc/0.9.4 [ 15 :59:19 ] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____ | / \\ | _ \\ | | _ ) | _ | / _ \\ | | _ ) | | __/ | | ___ / ___ \\| _ < | _ | | _____/_/ \\_\\_ | \\_\\ PEAR v0.9.4 [ August 8 , 2014 ] - [ +bzlib ] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al ( 2014 ) Bioinformatics 30 ( 5 ) : 614 -620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... **Answer**: \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap) 2.7) List all the loaded modules. How many are there? Where did all the others come from? \u00b6 Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. Answer ** *List* all the loaded modules. How many are there?** 1 2 3 4 $ module list Currently Loaded Modulefiles: 1 ) gmp/5.1.3 3 ) mpc/1.0.2 5 ) bzip2-gcc/1.0.6 2 ) mpfr/3.1.2 4 ) gcc/4.8.2 6 ) pear-gcc/0.9.4 **Answer**: 6 **Where did all the others come from?** You may have noticed when we loaded *pear-gcc* the module called *gcc* was also loaded; this gives a hint as to where the others come from. **Answer**: They are *dependencies*; that is, they are supporting software that is used by the module we loaded. Additionally, some HPC's automatically load some modules for you when you login. 2.8) How do you undo the loading of the pear module? List the loaded modules again, did they all disappear? \u00b6 Hint Computer Scientists are not always inventive with naming commands, try something starting with *un* Answer **How do you undo the loading of the *pear* module?** 1 $ module unload pear-gcc **Answer**: the *unload* sub-command removes the named module from our current SSH session. **List the loaded modules again, did they all disapear?** **Answer**: Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question) 2.9) How do you clear ALL loaded modules? \u00b6 Hint It's easier than running *unload* for all modules This one isn't that straight forward; try a [synonym](https://www.google.com.au/search?q=rid+synonym) of *rid*. Additional Hint We will *purge* the list of loaded modules. Answer 1 $ module purge **Answer**: running the *purge* sub-command will unload all modules you loaded (and all dependencies). **Alternative**: if you close your SSH connection and re-open it the new session will be blank as well. BEFORE CONTINUING : If you are using BARCOO or SNOWY you will need to load the default commands again. Do so by running module load vlsci Topic 3: Job Submission \u00b6 Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is OK for small jobs, it\u2019s unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power Background \u00b6 On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \u201cI want XX CPUS for YY hours\u201d and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal (screen) to a file; this file is called the output file. Reservation : much like a reservation for a resturant holds a table for you, the administrator can give you an HPC reservation which holds various resources for a period of time exclusively for you. Exercises \u00b6 Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime 3.1) Which nodes could a \u2018main\u2019 job go on? \u00b6 Hint Try the *sinfo* command Additional Hint Have a look at the PARTITION and NODELIST columns. The *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* 1 2 3 4 5 6 7 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200 -00:00: 3 mix lims-hpc- [ 2 -4 ] compute* up 200 -00:00: 2 idle lims-hpc- [ 1 ,5 ] bigmem up 200 -00:00: 1 idle lims-hpc-1 8hour up 08 :00:00 3 mix lims-hpc- [ 2 -4 ] 8hour up 08 :00:00 3 idle lims-hpc- [ 1 ,5 ] ,lims-hpc-m Note: the output to the sinfo command will look different depending on which HPC you are using and it's current usage levels Answer The *sinfo* command will list the *partitions*. It summaries the nodes by their current status so there may be more than one line with *main* in the partition column. It lists the nodes in shorthand i.e. barcoo[1,3-5] means barcoo1, barcoo3, barcoo4, barcoo5. **Answer**: barcoo001, barcoo002, ..., barcoo070 Use the cat command to view the contents of task01 , task02 and task03 job script 3.2) How many cpu cores will each ask for? \u00b6 Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint **Non-exclusive (shared) jobs**: It is *--cpus-per-task x --ntasks* but if *--ntasks* is not present it defaults to 1 so it's *--cpus-per-task x 1* **Exclusive jobs**: The *--nodes* options tells us how many nodes we ask for and the *--exclusive* option says give us all it has. This one is a bit tricky as we don't really know until it runs. Answer **Answer**: * task01: **1 cpu core** * task02: **6 cpu cores** * task03: **at least 1** as this has requested all cpu cores on the node its running on (*--exclusive*). However, since we know that all nodes on *barcoo* have 16, we know it will get 16. 3.3) What about total memory? \u00b6 Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --cpus-per-task x --ntasks* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated Answer The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --ntasks x --cpus-per-task* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --share ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. **Answer**: * task01: **1024MB** (1GB) i.e. 1024 x 1 x 1 * task02: **12288MB** (12GB) i.e. 2048 x 3 x 2 * task03: **at least 1024MB** (1GB). The actual amount could be a lot more as most HPCs have 100GB+ per node 3.4) How long can each run for? \u00b6 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer The *--time* option is what tells slurm how long your job will run for. **Answer**: * task01: requests **30:00 (30mins 0secs)**, uses ~30secs * task02: requests **5:00 (5mins 0secs)**, uses ~5secs * task03: requests **1:00 (1min 0secs)**, uses ~30secs 3.5) Is this maximum, minimum or both runtime? \u00b6 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time. 3.6) Calculate the \u2013time specification for the following runtimes: \u00b6 1h30m: \u2013time= 1m20s: \u2013time= 1.5days: \u2013time= 30m: \u2013time= Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. 1h30m: --time=01:30:00 (alternatively: 0-01:30) 2. 1m20s: --time=01:20 3. 1.5days: --time=1-12 4. 30m: --time=30 3.7) What do the following \u2013time specifications mean? \u00b6 \u2013time=12-00:20 \u2013time=45 \u2013time=00:30 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. --time=12-00:20 12 days and 20 minutes 2. --time=45 45 minutes 3. --time=00:30 30 seconds Reservations \u00b6 Before we continue, a quick note on reservations. Reservations are not normally needed however sometimes we will, particularly when the HPC is busy. To make use of a reservation you need to know its name and provide it with the \u2013reservation option Today we use the training reservation so that we have resources available to run our jobs. Your jobs will need to contain the line: #SBATCH \u2013reservation=training Now use sbatch to submit the task01 job: 3.8) What job id was your job given? \u00b6 Hint Use the man page for the sbatch command. The *Synopsis* at the top will give you an idea how to run it. Answer 1 2 $ sbatch task01 Submitted batch job 9998 **Answer**: it's unique for each job; in the above example mine was *9998* 3.9) Which node did your job go on? \u00b6 Hint The *squeue* command shows you the currently running jobs. If it's been longer than 30 seconds since you submitted it you might have to resubmit it. Answer Use the *squeue* command to show all jobs. Search for your *jobid* and look in the *NODELIST* column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. 1 2 3 4 5 $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 9999 compute task01 training R 0 :05 1 lims-hpc-2 **Answer**: it's dependent on node availability at time; in the above example mine was *lims-hpc-2* Advanced \u00b6 3.10) Make a copy of task01 and call it prime_numbers . Make it load the training module and use the prime command to calculate prime numbers for 20 seconds. \u00b6 Hint You can find the *prime* command in the *training-gcc/1.0* module Answer The key points to change in the task01 script are: 1. adding the *module load training-gcc/1.0* 2. replacing the *sleep* (and *echo*) statements with a call to *prime 20*. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=PARTITION #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION module load training-gcc/1.0 echo \"Starting at: $( date ) \" prime 20 echo \"Finished at: $( date ) \" Where *RESERVATION* is replaced with *training* and *PARTITION* is replaced with *main* Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well. 3.11) Submit the job. What was the largest prime number it found in 20 seconds? \u00b6 Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the *SLURM output file*; this is called *slurm-JOBID.out* where JOBID is replaced by the actual job id. Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC 1 2 3 4 5 6 7 8 9 10 $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16 :11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16 :11:27 AEST 2015 3.12) Modify your prime_numbers script to notify you via email when it starts and ends. Submit it again. \u00b6 Did it start immediately or have some delay? How long did it actually run for? Hint There are two options that you will need to set. See sbatch manpage for details. Additional Hint Both start with *--mail* Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=TRAINING #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $( date ) \" prime 20 echo \"Finished at: $( date ) \" Where *RESERVATION* is replaced with *training*, *PARTITION* is replaced with *main* and *name@email.address* by your email address **Answers**: * **Did it start immediately or have some delay?** The *Queued time* value in the subject of start email will tell you how long it waited. * **How long did it actually run for?** The *Run time* value in the subject of the end email will tell you how long it ran for which should be ~20 seconds. Topic 4: Job Monitoring \u00b6 It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU\u2019s, processing step etc.). In this topic we will cover some of the tools that are available that enable you to watch what is happening so we can make better predictions in the future. Exercises \u00b6 4.1) What does the top command show? \u00b6 Hint When all else fails, try *man*; specifically, the description section Answer 1 2 3 4 5 $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... **Answer**: in lay-person terms *\"Continually updating CPU and Memory usage\"* Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications). 4.2) How much total memory does this HPC (head-node) have? \u00b6 Hint This would be a system-wide statistic. Answer **Answer**: If you look at the first value on the *Mem* line (line 4) it will tell you the total memory on this computer (node). * **BARCOO**: 65942760k or ~64 GigaBytes * **SNOWY**: 132035040k or ~128 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again. 4.3) What is the current total CPU usage? \u00b6 Hint This might be easier to work out what is not used and subtract it from 100% Additional Hint *Idle* is another term for not used (or *id* for short) Answer **Answer**: If you subtract the *%id* value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage 4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low ? \u00b6 Hint It's not PID but from time to time it might be ordered sequentially. Answer **Answer**: *%CPU* which gives you an indication of how much CPU time each process uses and sorted high-to-low. Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates. 4.5) Why might the numbers disagree? \u00b6 Hint It might have something to do with the total number of CPU Cores on the system. Answer **Answer**: *%CPU* column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in *top* (excluding round errors) they would add up 100% x the number of cpu cores available. On BARCOO it is 0-2400% and SNOWY it is 0-3200% for individual processes. 4.6) What command-line flag instructs top to sort results by %MEM ? \u00b6 Can you think of a reason that this might be useful? Hint Use the *top* manpage. Additional Hint *\"m is for memory!\"* Answer **Answer**: *top -m* will cause *top* to sort the processes by memory usage. **Can you think of a reason that this might be useful?** Your program might be using a lot of memory and you want to know how much; by sorting by memory will cause your program to stay at the top. 4.7) Run \u201ctop -c\u201d . What does it do? How might this be helpful? \u00b6 Hint Use the *top* manpage. Additional Hint *\"c is for complete!\"* *\"c is also for command!\"* which is another name for program Answer **What does it do?** It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. **How might this be helpful?** Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the *-c* flag to show the complete command. **NOTE**: If *top* is running you can press the *c* key to toggle show/hide complete command 4.8) How can you get top to only show your processes? Why might this be useful? \u00b6 Hint Use the *top* manpage. Additional Hint *\"u is for user[name]!\"* Answer **How can you get *top* to only show your processes?** **Answer 1**: *top -u YOURUSERNAME* **Answer 2**: while running *top* press the *u* key, type YOURUSERNAME and press key **Why might this be useful?** When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours. Topic 5: All Together \u00b6 This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job Task 1: Write a job script \u00b6 Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : main Time : 5 mins Memory : 1 GB (remember to specify it in MB) Reservation : training Task 2: Load/use software module \u00b6 Edit your job script so that it: Loads the training-gcc/1.0 module Runs the fakejob command with your name as the first parameter. FYI: fakejob is a command that was made to demonstrate what real commands might do in terms of CPU and Memory usage. It does not perform any useful task; if you must know, it just calculates prime numbers for 5 minutes and consumes some memory NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script. Task 3: Submit job \u00b6 NOTE : Task 4 is time dependent on task 3; you need to do it within 2 or 3 minutes of running step 3.1 so it might be a good idea to read ahead before hand. Don't stress if you don't complete it in time, you can simply run 3.1 again. Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task). Task 4: Monitor the job \u00b6 Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top won\u2019t be able to see the job. To be able to use top, you will first need to login to the compute node that is running your job. To login: 1 $ ssh barcooXXX Where XXX is the actual node number you were allocated (See task 3.4). You are now connected from your computer to barcoo which is connected to barcooXXX. 1 2 3 +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | BARCOO | -- SSH --> | BARCOOXXX | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt 1 2 3 4 5 [ USERNAME@barcoo USERNAME ] $ Changes to: [ USERNAME@barcooXXX USERNAME ] $ Once logged in to the relevent compute node you can run top to view your job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs. How does the CPU and Memory usage change over time? \u00b6 Hint It should vary (within the limits you set in the job script) Answer The *fakejob* program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory. The percentage that it shows is based on the total memory of the node that runs your job; check Topic 4, Question 4.2 to remember how to find the total memory. Finished \u00b6 Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don\u2019t forget to complete the training survey and return it to the workshop facilitators.","title":"Introduction to HPC"},{"location":"tutorials/hpc/hpc/#high-performance-computing","text":"A hands-on-workshop covering High-Performance Computing (HPC).","title":"High-Performance Computing"},{"location":"tutorials/hpc/hpc/#overview","text":"Using High Performance Computing (HPC) resources such as Melbourne Bioinformatics in an effective and efficient manner is key to modern research. This workshop will introduce you to HPC environments and assist you to get on with your research.","title":"Overview"},{"location":"tutorials/hpc/hpc/#learning-objectives","text":"At the end of the course, you will be able to: Define \u2018What is HPC?\u2019 Load software modules Submit jobs Select job queues Monitor your job\u2019s progress Know what resources you can request Select appropriate resources","title":"Learning Objectives"},{"location":"tutorials/hpc/hpc/#requirements","text":"You will need a basic understanding of Unix, or you should have attended an Introduction to Unix workshop in the past. All participants are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/hpc/hpc/#introduction","text":"Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the main concepts of High-Performance Computing. The slides are available if you would like. Additionally the following reference material is available for later use: Reference Material ### What is an HPC? An HPC is simply a large collection of server-grade computers working together to solve large problems. * **Big**: HPCs typically have lots of CPUs and Memory and consequently large jobs. * **Shared**: There are usually lots of users making use of it at one time. * **Coordinated**: There is a coordinator program to ensure fair-use between its users. * **Compute Collection**: HPCs use a number of computers at once to solve lots of large jobs. **Figure**: The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. ### Why use HPCs? The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. * **Many CPUs**: HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. * **Large Memory**: Hundreds of GBs to multiple TBs of RAM are typical for each node. * **Efficient use**: Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. ### Software Modules There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. * **Packaged**: to avoid conflicts between software, each is packaged up into a module and only used on demand. * **Loadable**: before using a software module you need to load it. * **Versions**: given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. ### Job Submission Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. * **SLURM**: this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. * **Queues (Partition)**: when a job is submitted it is added to a work queue; in SLURM this is called a Partition. * **Batch**: HPC jobs are not 'interactive'. By this we mean, you can't type input into your job's programs and you won't immediately see the output that your program prints on the screen. #### Resources So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. 1. **CPUs**: most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. 2. **Memory**: you need to estimate (or guess) how much memory (RAM) your program needs. 3. **Nodes**: most software will only use one of the HPC's Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. 4. **Time**: like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. #### Job Types There are two types of jobs that you can submit: 1. **Shared**: a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. 2. **Exclusive**: an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Introduction"},{"location":"tutorials/hpc/hpc/#connecting-to-the-hpc","text":"To begin this workshop you will need to connect to the HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC\u2019s tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Connection instructions : Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: 1 2 3 The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer.","title":"Connecting to the HPC"},{"location":"tutorials/hpc/hpc/#topic-1-exploring-an-hpc","text":"An HPC (short for \u2018High-Performance Computer\u2019) is simply a collection of Server Grade computers that work together to solve large problems. Figure : Overview of the computers involved when using an HPC. Computer systems are shown in rectangles and arrows represent interactions.","title":"Topic 1: Exploring an HPC"},{"location":"tutorials/hpc/hpc/#exercises","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#11-what-is-the-contact-email-for-your-hpcs-system-administrator","text":"Hint When you login, you will be presented with a message; this is called the *Message Of The Day* and usually includes lots of useful information. On *barcoo* this includes a list of useful commands, the last login details for your account and the contact email of the system administrator Answer Depending on which computer you are working: * SNOWY & BARCOO: help@vlsci.unimelb.edu.au","title":"1.1) What is the contact email for your HPC's System Administrator?"},{"location":"tutorials/hpc/hpc/#12-run-the-sinfo-command-how-many-nodes-are-there-in-this-hpc","text":"Hint *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* and *barcoo[1,5]* is shorthand for *barcoo1* and *barcoo5* Additional Hint Have a look at the NODELIST column. Only count each node once. 1 2 3 4 5 6 7 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200 -00:00: 3 mix lims-hpc- [ 2 -4 ] compute* up 200 -00:00: 2 idle lims-hpc- [ 1 ,5 ] bigmem up 200 -00:00: 1 idle lims-hpc-1 8hour up 08 :00:00 3 mix lims-hpc- [ 2 -4 ] 8hour up 08 :00:00 3 idle lims-hpc- [ 1 ,5 ] ,lims-hpc-m NOTE: the above list will vary depending on the HPC setup. Answer The *sinfo* command lists all available partitions and the status of each node within them. If you count up the names of nodes (uniquely) you will get the total nodes in this cluster. * BARCOO: **70** (*barcoo001* through *barcoo070*) * SNOWY: **43** (*snowy001* through *snowy043*) Alternate Method An automatic (though more complex) way would have been running the following command: 1 $ scontrol show node | grep NodeName | wc -l Where: * *scontrol show node*: lists details of all nodes (over multiple lines) * *grep NodeName*: only shows the NodeName line * *wc -l*: counts the number of lines","title":"1.2) Run the sinfo command.  How many nodes are there in this hpc?"},{"location":"tutorials/hpc/hpc/#topic-2-software-modules","text":"Up to this point we have been using only standard Unix software packages which are included with Linux/Unix computers. Large computing systems such as HPCs often use a system of modules to load specific software packages (and versions) when needed for the user. In this topic we will discover what science software modules (tools) are available and load them ready for analysis. This topic uses the man and module commands heavily","title":"Topic 2: Software Modules"},{"location":"tutorials/hpc/hpc/#exercises_1","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#21-what-happens-if-you-run-the-module-command-without-any-options-arguments","text":"Hint Literally type *module* and press *ENTER* key. Answer **Answer**: It prints an error followed by a list of available options / flags 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $ module cmdModule.c(166):ERROR:11: Usage is 'module command [arguments ...] ' Modules Release 3.2.10 2012-12-21 (Copyright GNU GPL v2 1991): Usage: module [ switches ] [ subcommand ] [subcommand-args ] Switches: -H|--help this usage info -V|--version modules version & configuration options -f|--force force active dependency resolution -t|--terse terse format avail and list format -l|--long long format avail and list format -h|--human readable format avail and list format -v|--verbose enable verbose messages -s|--silent disable verbose messages -c|--create create caches for avail and apropos -i|--icase case insensitive -u|--userlvl <lvl> set user level to (nov[ice],exp[ert],adv[anced]) Available SubCommands and Args: + add|load modulefile [modulefile ...] + rm|unload modulefile [modulefile ...] + switch|swap [modulefile1] modulefile2 + display|show modulefile [modulefile ...] + avail [modulefile [modulefile ...]] + use [-a|--append] dir [dir ...] + unuse dir [dir ...] + update + refresh + purge + list + clear + help [modulefile [modulefile ...]] + whatis [modulefile [modulefile ...]] + apropos|keyword string + initadd modulefile [modulefile ...] + initprepend modulefile [modulefile ...] + initrm modulefile [modulefile ...] + initswitch modulefile1 modulefile2 + initlist + initclear","title":"2.1) What happens if you run the module command without any options / arguments?"},{"location":"tutorials/hpc/hpc/#22-how-do-you-find-a-list-of-available-software","text":"Hint Try the *module* command. Don't forget the *man* command to get help for a command Additional Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>) Answer The module command is used to show details of software modules (tools). **Answer**: 1 2 3 4 5 6 7 8 9 10 $ module avail ------------------- /usr/share/Modules/modulefiles -------------------- dot module-git module-info modules null use.own ------------------- /usr/local/Modules/modulefiles -------------------- acana/1.60 mafft-gcc/7.215 aftrrad/4.1.20150201 malt/0.1.0 arlequin/3.5.1.3 matplotlib-gcc/1.3.1 ... The modules list has been shortened because it is very long. The modules after the */usr/local/Modules/modulefiles* line are the science software; before this are a few built-in ones that you can ignore.","title":"2.2) How do you find a list of available software?"},{"location":"tutorials/hpc/hpc/#23-how-many-modules-are-there-starting-with-f","text":"Hint Run the command *man module* Use a search to find out about the *avail* subcommand (e.g. /avail<enter>). You may have to press 'n' a few times to reach the section where the it describes the *avail* subcommand. Additional Hint > If an argument is given, then each directory in the MODULEPATH is searched for modulefiles > whose pathname match the argument This is a quote from the manual page for the module command explaining the avail subcommand. It uses rather technical language but basically it's saying you can put search terms after the avail subcommand when entering the command. Answer The man page told us that we could put a search term after *module avail*. 1 2 3 4 5 6 7 $ module avail f ------------------- /usr/local/Modules/modulefiles ------------------- fasta-gcc/35.4.12 flex-gcc/2.5.39 fastqc/0.10.1 fontconfig-gcc/2.11.93 fastStructure-gcc/2013.11.07 freebayes-gcc/20140603 fastStructure-gcc/20150320 freetype-gcc/2.5.3 fastx_toolkit-gcc/0.0.14 **Answer**: 26 modules NOTE: this was correct at time of writing this workshop and might increase over time so don't be alarmed if you got more Alternate Method To get a fully automated solution your could do the following command: 1 $ module -l avail 2 > & 1 | grep \"^f\" | wc -l Where: * *module -l avail*: lists all modules (in long format, i.e. one per line) * *2>&1*: merges output from *standard error* to the *standard output* so it can be feed into grep. For some reason the developers of the *module* command thought it was a good idea to output the module names on the *error* stream rather than the logical *output* stream. * *grep \"^f\"*: only shows lines beginning with *f* * *wc -l*: counts the number of lines","title":"2.3) How many modules are there starting with 'f'?"},{"location":"tutorials/hpc/hpc/#24-run-the-pear-command-without-loading-it-does-it-work","text":"Hint This question is very literal Answer 1 2 $ pear -bash: pear: command not found The error you see is from BASH, it is complaining that it doesn't know anything about a command called 'pear' **Answer**: No, command not found","title":"2.4) Run the pear command (without loading it), does it work?"},{"location":"tutorials/hpc/hpc/#25-how-would-we-load-the-pear-module","text":"Hint Check the man page for *module* again and look for a subcommand that might load modules; it is quite literal as well. Additional Hint Run the command *man module* Use a search to find out about the *load* subcommand (e.g. /load<enter>) Answer 1 $ module load pear-gcc/0.9.4 -gcc | -intel : Lots of modules will have either -gcc or -intel after the software name. This refers to the compiler that was used to make the software. If you have a choice then usually the -intel one will be faster. VERSIONS : module load pear-gcc would have been sufficient to load the module however it is best-practice (in science) to specify the version number so that the answer you get today will be the answer you get in 1 year time. Some software will produce different results with different versions of the software.","title":"2.5) How would we load the pear module?"},{"location":"tutorials/hpc/hpc/#26-now-its-loaded-run-pear-again-what-does-it-do","text":"Hint The paper citation gives a clue. Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 $ module load pear-gcc/0.9.4 [ 15 :59:19 ] training21@lims-hpc-m ~ $ pear ____ _____ _ ____ | _ \\| ____ | / \\ | _ \\ | | _ ) | _ | / _ \\ | | _ ) | | __/ | | ___ / ___ \\| _ < | _ | | _____/_/ \\_\\_ | \\_\\ PEAR v0.9.4 [ August 8 , 2014 ] - [ +bzlib ] Citation - PEAR: a fast and accurate Illumina Paired-End reAd mergeR Zhang et al ( 2014 ) Bioinformatics 30 ( 5 ) : 614 -620 | doi:10.1093/bioinformatics/btt593 ... REST REMOVED ... **Answer**: \"PEAR: a fast and accurate Illumina Paired-End reAd mergeR\" (i.e. merges paired dna reads into a single read when they overlap)","title":"2.6) Now it's *load*ed, run pear again, what does it do?"},{"location":"tutorials/hpc/hpc/#27-list-all-the-loaded-modules-how-many-are-there-where-did-all-the-others-come-from","text":"Hint Use man to find a subcommand that will list currently loaded modules. We are not really expecting you to be able to answer the 2nd question however if you do get it correct then well-done, that was very tough. Answer ** *List* all the loaded modules. How many are there?** 1 2 3 4 $ module list Currently Loaded Modulefiles: 1 ) gmp/5.1.3 3 ) mpc/1.0.2 5 ) bzip2-gcc/1.0.6 2 ) mpfr/3.1.2 4 ) gcc/4.8.2 6 ) pear-gcc/0.9.4 **Answer**: 6 **Where did all the others come from?** You may have noticed when we loaded *pear-gcc* the module called *gcc* was also loaded; this gives a hint as to where the others come from. **Answer**: They are *dependencies*; that is, they are supporting software that is used by the module we loaded. Additionally, some HPC's automatically load some modules for you when you login.","title":"2.7) List all the loaded modules. How many are there? Where did all the others come from?"},{"location":"tutorials/hpc/hpc/#28-how-do-you-undo-the-loading-of-the-pear-module-list-the-loaded-modules-again-did-they-all-disappear","text":"Hint Computer Scientists are not always inventive with naming commands, try something starting with *un* Answer **How do you undo the loading of the *pear* module?** 1 $ module unload pear-gcc **Answer**: the *unload* sub-command removes the named module from our current SSH session. **List the loaded modules again, did they all disapear?** **Answer**: Unfortunately not, the module command is not smart enough to determine if any of the other modules that were loaded are still needed or not so we will need to do it manually (or see next question)","title":"2.8) How do you undo the loading of the pear module?  List the loaded modules again, did they all disappear?"},{"location":"tutorials/hpc/hpc/#29-how-do-you-clear-all-loaded-modules","text":"Hint It's easier than running *unload* for all modules This one isn't that straight forward; try a [synonym](https://www.google.com.au/search?q=rid+synonym) of *rid*. Additional Hint We will *purge* the list of loaded modules. Answer 1 $ module purge **Answer**: running the *purge* sub-command will unload all modules you loaded (and all dependencies). **Alternative**: if you close your SSH connection and re-open it the new session will be blank as well. BEFORE CONTINUING : If you are using BARCOO or SNOWY you will need to load the default commands again. Do so by running module load vlsci","title":"2.9) How do you clear ALL loaded modules?"},{"location":"tutorials/hpc/hpc/#topic-3-job-submission","text":"Up to this point in the workshop (and the previous Unix workshop) we have only used the head-node of the HPC. While this is OK for small jobs, it\u2019s unworkable for most jobs. In this topic we will start to learn how to make use of the rest of the HPCs immense compute power","title":"Topic 3: Job Submission"},{"location":"tutorials/hpc/hpc/#background","text":"On conventional Unix computers (such as the HPC headnode) we enter the commands we want to run at the terminal and see the results directly output in front of us. On an HPC this type of computation will only make use of one node, namely, the Head Node . To make use of the remaining ( compute ) nodes we need to use the SLURM software package (called an HPC Scheduler). The purpose of SLURM is to manage all user jobs and distribute the available resources (i.e. time on the compute nodes) to each job in a fair manner. You can think of the SLURM software as like an electronic calendar and the user jobs like meetings . Users say to SLURM \u201cI want XX CPUS for YY hours\u201d and SLURM will look at its current bookings and find the next available time it can fit the job. Terminology : Node : a server grade computer which is part of an HPC Batch Job : a group of one or more related Unix commands that need to be run (executed) for a user. e.g. run fastqc on all my samples Partition (or Queue) : a list of jobs that need to be run. There is often more than one partition on an HPC which usually have specific requirements for the jobs that can be added to them. e.g. 8hour will accept jobs less than or equal to 8hours long Runtime : the amount of time a job is expected (or actually) runs Resources : computation resources that can be given to our jobs in order to run them. e.g. CPU Cores, Memory, and Time. Job Script : a special BASH script that SLURM uses to run a job on our behalf once resources become available. Job scripts contain details of the resources that our commands need to run. Output (or Results) file : When SLURM runs our batch job it will save the results that would normally be output on the terminal (screen) to a file; this file is called the output file. Reservation : much like a reservation for a resturant holds a table for you, the administrator can give you an HPC reservation which holds various resources for a period of time exclusively for you.","title":"Background"},{"location":"tutorials/hpc/hpc/#exercises_2","text":"Useful Commands : man, sinfo, cat, sbatch, squeue, cp, module, prime","title":"Exercises"},{"location":"tutorials/hpc/hpc/#31-which-nodes-could-a-main-job-go-on","text":"Hint Try the *sinfo* command Additional Hint Have a look at the PARTITION and NODELIST columns. The *barcoo[2-4]* is shorthand for *barcoo2 barcoo3 and barcoo4* 1 2 3 4 5 6 7 $ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST compute* up 200 -00:00: 3 mix lims-hpc- [ 2 -4 ] compute* up 200 -00:00: 2 idle lims-hpc- [ 1 ,5 ] bigmem up 200 -00:00: 1 idle lims-hpc-1 8hour up 08 :00:00 3 mix lims-hpc- [ 2 -4 ] 8hour up 08 :00:00 3 idle lims-hpc- [ 1 ,5 ] ,lims-hpc-m Note: the output to the sinfo command will look different depending on which HPC you are using and it's current usage levels Answer The *sinfo* command will list the *partitions*. It summaries the nodes by their current status so there may be more than one line with *main* in the partition column. It lists the nodes in shorthand i.e. barcoo[1,3-5] means barcoo1, barcoo3, barcoo4, barcoo5. **Answer**: barcoo001, barcoo002, ..., barcoo070 Use the cat command to view the contents of task01 , task02 and task03 job script","title":"3.1) Which nodes could a 'main' job go on?"},{"location":"tutorials/hpc/hpc/#32-how-many-cpu-cores-will-each-ask-for","text":"Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint **Non-exclusive (shared) jobs**: It is *--cpus-per-task x --ntasks* but if *--ntasks* is not present it defaults to 1 so it's *--cpus-per-task x 1* **Exclusive jobs**: The *--nodes* options tells us how many nodes we ask for and the *--exclusive* option says give us all it has. This one is a bit tricky as we don't really know until it runs. Answer **Answer**: * task01: **1 cpu core** * task02: **6 cpu cores** * task03: **at least 1** as this has requested all cpu cores on the node its running on (*--exclusive*). However, since we know that all nodes on *barcoo* have 16, we know it will get 16.","title":"3.2) How many cpu cores will each ask for?"},{"location":"tutorials/hpc/hpc/#33-what-about-total-memory","text":"Hint Lookup the man page for *sbatch* command. *sbatch*'s options match up with the *#SBATCH* comments at the top of each job script. Some will be affected by more than one option Additional Hint The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --cpus-per-task x --ntasks* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated Answer The *--mem-per-cpu* OR *--mem* options are holding the answer to total memory. For task01 and task02 the calculation is *--mem-per-cpu x --ntasks x --cpus-per-task* For task03, like with the cpus cores question, we get all the memory available on the node we get allocated NOTE : it might be tempting to use the --mem option on non-exclusive (i.e. --share ) jobs however this will NOT work since the meaning of --mem is \"go on a node with at least X MB of memory\" ; it does not actually allocate any of it to you so your job will get terminated once it tries to use any memory. **Answer**: * task01: **1024MB** (1GB) i.e. 1024 x 1 x 1 * task02: **12288MB** (12GB) i.e. 2048 x 3 x 2 * task03: **at least 1024MB** (1GB). The actual amount could be a lot more as most HPCs have 100GB+ per node","title":"3.3) What about total memory?"},{"location":"tutorials/hpc/hpc/#34-how-long-can-each-run-for","text":"Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer The *--time* option is what tells slurm how long your job will run for. **Answer**: * task01: requests **30:00 (30mins 0secs)**, uses ~30secs * task02: requests **5:00 (5mins 0secs)**, uses ~5secs * task03: requests **1:00 (1min 0secs)**, uses ~30secs","title":"3.4) How long can each run for?"},{"location":"tutorials/hpc/hpc/#35-is-this-maximum-minimum-or-both-runtime","text":"Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer This is a maximum time. Your job may finish early, at which point it hands back the resources for the next job. However if it tries to run longer the HPC will terminate the job. HINT : when selecting a time for your job its best to estimate your job runtime to be close to what it actually uses as it can help the HPC scheduler 'fit' your job in between other jobs though be careful to allow enough time. If you think your job may not complete in time you can ask the system administrator of your HPC to add more time.","title":"3.5) Is this maximum, minimum or both runtime?"},{"location":"tutorials/hpc/hpc/#36-calculate-the-time-specification-for-the-following-runtimes","text":"1h30m: \u2013time= 1m20s: \u2013time= 1.5days: \u2013time= 30m: \u2013time= Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. 1h30m: --time=01:30:00 (alternatively: 0-01:30) 2. 1m20s: --time=01:20 3. 1.5days: --time=1-12 4. 30m: --time=30","title":"3.6) Calculate the --time specification for the following runtimes:"},{"location":"tutorials/hpc/hpc/#37-what-do-the-following-time-specifications-mean","text":"\u2013time=12-00:20 \u2013time=45 \u2013time=00:30 Hint Use the *man sbatch* command to look up the time specification. If you search for *--time* it will describe the formats it uses (i.e. type */--time* and press enter) Answer 1. --time=12-00:20 12 days and 20 minutes 2. --time=45 45 minutes 3. --time=00:30 30 seconds","title":"3.7) What do the following --time specifications mean?"},{"location":"tutorials/hpc/hpc/#reservations","text":"Before we continue, a quick note on reservations. Reservations are not normally needed however sometimes we will, particularly when the HPC is busy. To make use of a reservation you need to know its name and provide it with the \u2013reservation option Today we use the training reservation so that we have resources available to run our jobs. Your jobs will need to contain the line: #SBATCH \u2013reservation=training Now use sbatch to submit the task01 job:","title":"Reservations"},{"location":"tutorials/hpc/hpc/#38-what-job-id-was-your-job-given","text":"Hint Use the man page for the sbatch command. The *Synopsis* at the top will give you an idea how to run it. Answer 1 2 $ sbatch task01 Submitted batch job 9998 **Answer**: it's unique for each job; in the above example mine was *9998*","title":"3.8) What job id was your job given?"},{"location":"tutorials/hpc/hpc/#39-which-node-did-your-job-go-on","text":"Hint The *squeue* command shows you the currently running jobs. If it's been longer than 30 seconds since you submitted it you might have to resubmit it. Answer Use the *squeue* command to show all jobs. Search for your *jobid* and look in the *NODELIST* column. NOTE : if there are lots of jobs you can use squeue -u YOUR_USERNAME to only show your jobs, where YOUR_USERNAME is replaced with your actual username. 1 2 3 4 5 $ sbatch task01 Submitted batch job 9999 $ squeue -u training01 JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 9999 compute task01 training R 0 :05 1 lims-hpc-2 **Answer**: it's dependent on node availability at time; in the above example mine was *lims-hpc-2*","title":"3.9) Which node did your job go on?"},{"location":"tutorials/hpc/hpc/#advanced","text":"","title":"Advanced"},{"location":"tutorials/hpc/hpc/#310-make-a-copy-of-task01-and-call-it-prime_numbers-make-it-load-the-training-module-and-use-the-prime-command-to-calculate-prime-numbers-for-20-seconds","text":"Hint You can find the *prime* command in the *training-gcc/1.0* module Answer The key points to change in the task01 script are: 1. adding the *module load training-gcc/1.0* 2. replacing the *sleep* (and *echo*) statements with a call to *prime 20*. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=PARTITION #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION module load training-gcc/1.0 echo \"Starting at: $( date ) \" prime 20 echo \"Finished at: $( date ) \" Where *RESERVATION* is replaced with *training* and *PARTITION* is replaced with *main* Repeatable Science : It's good scientific practice to include the version number of the module when loading it as this will ensure that the same version is loaded next time you run this script which will mean you get the same results. Date your work : It's also good practice to include the date command in the output so you have a permanent record of when this job was run. If you have one before and after your main program you will get a record of how long it ran for as well.","title":"3.10) Make a copy of task01 and call it prime_numbers.  Make it load the training module and use the prime command to calculate prime numbers for 20 seconds."},{"location":"tutorials/hpc/hpc/#311-submit-the-job-what-was-the-largest-prime-number-it-found-in-20-seconds","text":"Hint The output from the program will provide the results that we are after. For HPC jobs this will be placed in the *SLURM output file*; this is called *slurm-JOBID.out* where JOBID is replaced by the actual job id. Answer You should get results similar to below however the actual numbers will vary as amount of computations performed will be affected by the amount of other jobs running on the HPC 1 2 3 4 5 6 7 8 9 10 $ sbatch prime_numbers Submitted batch job 9304 $ cat slurm-9304.out Starting at: Fri May 8 16 :11:07 AEST 2015 Primes: 710119 Last trial: 10733927 Largest prime: 10733873 Runtime: 20 seconds Finished at: Fri May 8 16 :11:27 AEST 2015","title":"3.11) Submit the job.  What was the largest prime number it found in 20 seconds?"},{"location":"tutorials/hpc/hpc/#312-modify-your-prime_numbers-script-to-notify-you-via-email-when-it-starts-and-ends-submit-it-again","text":"Did it start immediately or have some delay? How long did it actually run for? Hint There are two options that you will need to set. See sbatch manpage for details. Additional Hint Both start with *--mail* Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1024 #SBATCH --partition=TRAINING #SBATCH --time=30:00 #SBATCH --reservation=RESERVATION #SBATCH --mail-user=name@email.address #SBATCH --mail-type=ALL module load training/1.0 echo \"Starting at: $( date ) \" prime 20 echo \"Finished at: $( date ) \" Where *RESERVATION* is replaced with *training*, *PARTITION* is replaced with *main* and *name@email.address* by your email address **Answers**: * **Did it start immediately or have some delay?** The *Queued time* value in the subject of start email will tell you how long it waited. * **How long did it actually run for?** The *Run time* value in the subject of the end email will tell you how long it ran for which should be ~20 seconds.","title":"3.12) Modify your prime_numbers script to notify you via email when it starts and ends.  Submit it again."},{"location":"tutorials/hpc/hpc/#topic-4-job-monitoring","text":"It is often difficult to predict how a software tool may utilise HPC System Resources (CPU/Memory) as it can vary quite widely based on a number of factors (data set, number of CPU\u2019s, processing step etc.). In this topic we will cover some of the tools that are available that enable you to watch what is happening so we can make better predictions in the future.","title":"Topic 4: Job Monitoring"},{"location":"tutorials/hpc/hpc/#exercises_3","text":"","title":"Exercises"},{"location":"tutorials/hpc/hpc/#41-what-does-the-top-command-show","text":"Hint When all else fails, try *man*; specifically, the description section Answer 1 2 3 4 5 $ man top ... DESCRIPTION The top program provides a dynamic real-time view of a running system. ... **Answer**: in lay-person terms *\"Continually updating CPU and Memory usage\"* Run the top command. Above the black line it shows some system-wide statistics and below are statistics specific to a single process (a.k.a, tasks OR software applications).","title":"4.1) What does the top command show?"},{"location":"tutorials/hpc/hpc/#42-how-much-total-memory-does-this-hpc-head-node-have","text":"Hint This would be a system-wide statistic. Answer **Answer**: If you look at the first value on the *Mem* line (line 4) it will tell you the total memory on this computer (node). * **BARCOO**: 65942760k or ~64 GigaBytes * **SNOWY**: 132035040k or ~128 GigaBytes To transfer from kB to MB you divide by 1024 and MB to GB by 1024 again.","title":"4.2) How much total memory does this HPC (head-node) have?"},{"location":"tutorials/hpc/hpc/#43-what-is-the-current-total-cpu-usage","text":"Hint This might be easier to work out what is not used and subtract it from 100% Additional Hint *Idle* is another term for not used (or *id* for short) Answer **Answer**: If you subtract the *%id* value (4th value on Cpu(s) line) from 100% you will get the total CPU Usage","title":"4.3) What is the current total CPU usage?"},{"location":"tutorials/hpc/hpc/#44-what-column-does-it-appear-to-be-sorting-the-processes-by-is-this-low-to-high-or-high-to-low","text":"Hint It's not PID but from time to time it might be ordered sequentially. Answer **Answer**: *%CPU* which gives you an indication of how much CPU time each process uses and sorted high-to-low. Add up the top few CPU usages of processes and compare this to the system-wide CPU usage at that time. NOTE: you may need to quit top (by pressing q) so you can compare before it updates.","title":"4.4) What column does it appear to be sorting the processes by? Is this low-to-high OR high-to-low?"},{"location":"tutorials/hpc/hpc/#45-why-might-the-numbers-disagree","text":"Hint It might have something to do with the total number of CPU Cores on the system. Answer **Answer**: *%CPU* column gives you an indication of how much this process uses of 1 CPU Core, where as the system-wide values at the top are exactly that, how much the entire system is utilised. i.e. if you could see all processes in *top* (excluding round errors) they would add up 100% x the number of cpu cores available. On BARCOO it is 0-2400% and SNOWY it is 0-3200% for individual processes.","title":"4.5) Why might the numbers disagree?"},{"location":"tutorials/hpc/hpc/#46-what-command-line-flag-instructs-top-to-sort-results-by-mem","text":"Can you think of a reason that this might be useful? Hint Use the *top* manpage. Additional Hint *\"m is for memory!\"* Answer **Answer**: *top -m* will cause *top* to sort the processes by memory usage. **Can you think of a reason that this might be useful?** Your program might be using a lot of memory and you want to know how much; by sorting by memory will cause your program to stay at the top.","title":"4.6) What command-line flag instructs top to sort results by %MEM?"},{"location":"tutorials/hpc/hpc/#47-run-top-c-what-does-it-do-how-might-this-be-helpful","text":"Hint Use the *top* manpage. Additional Hint *\"c is for complete!\"* *\"c is also for command!\"* which is another name for program Answer **What does it do?** It changes the COMMAND column (right most) to show the complete command (or as much that fits) including the flags and options. **How might this be helpful?** Sometimes you might be running a lot of commands with the same name that only differ by the command-line options. In this case it is hard to tell which ones are still running unless you use the *-c* flag to show the complete command. **NOTE**: If *top* is running you can press the *c* key to toggle show/hide complete command","title":"4.7) Run \"top -c\".  What does it do?  How might this be helpful?"},{"location":"tutorials/hpc/hpc/#48-how-can-you-get-top-to-only-show-your-processes-why-might-this-be-useful","text":"Hint Use the *top* manpage. Additional Hint *\"u is for user[name]!\"* Answer **How can you get *top* to only show your processes?** **Answer 1**: *top -u YOURUSERNAME* **Answer 2**: while running *top* press the *u* key, type YOURUSERNAME and press key **Why might this be useful?** When you are looking to see how much CPU or Memory you are using on a node that has other user jobs running it can be hard to quickly identify yours.","title":"4.8) How can you get top to only show your processes?  Why might this be useful?"},{"location":"tutorials/hpc/hpc/#topic-5-all-together","text":"This topic will allow you to put all the skills that you learnt in this workshop to the test. You might need to refer back to the earlier topics if you have forgotten how to do these tasks. Overview : Write jobscript Load/use software module Submit job Monitor job","title":"Topic 5: All Together"},{"location":"tutorials/hpc/hpc/#task-1-write-a-job-script","text":"Write a job script that requests the following resources: Filename : monINITIALS.slurm where INITIALS is replaced with your initials. e.g. for me it would be monAR.slurm Tasks : 1 CPUs : 1 Partition : main Time : 5 mins Memory : 1 GB (remember to specify it in MB) Reservation : training","title":"Task 1: Write a job script"},{"location":"tutorials/hpc/hpc/#task-2-loaduse-software-module","text":"Edit your job script so that it: Loads the training-gcc/1.0 module Runs the fakejob command with your name as the first parameter. FYI: fakejob is a command that was made to demonstrate what real commands might do in terms of CPU and Memory usage. It does not perform any useful task; if you must know, it just calculates prime numbers for 5 minutes and consumes some memory NOTE : remember good practice here and add the date commands to print the date/time in your output. You can copy them from the *task01* script.","title":"Task 2: Load/use software module"},{"location":"tutorials/hpc/hpc/#task-3-submit-job","text":"NOTE : Task 4 is time dependent on task 3; you need to do it within 2 or 3 minutes of running step 3.1 so it might be a good idea to read ahead before hand. Don't stress if you don't complete it in time, you can simply run 3.1 again. Use sbatch to submit the job to the HPC. Note down the job id it was given (for later). Use squeue (or qs) to check that is started ok. When it starts check which compute node it is running on (for the next task).","title":"Task 3: Submit job"},{"location":"tutorials/hpc/hpc/#task-4-monitor-the-job","text":"Use the top command to check how much CPU and Memory the job is using. Given that SLURM is running the job on your behalf on one of the compute nodes, top won\u2019t be able to see the job. To be able to use top, you will first need to login to the compute node that is running your job. To login: 1 $ ssh barcooXXX Where XXX is the actual node number you were allocated (See task 3.4). You are now connected from your computer to barcoo which is connected to barcooXXX. 1 2 3 +---------------+ +------------+ +------------+ | YOUR COMPUTER | -- SSH --> | BARCOO | -- SSH --> | BARCOOXXX | +---------------+ +------------+ +------------+ You can tell which node you are on by the text in the prompt 1 2 3 4 5 [ USERNAME@barcoo USERNAME ] $ Changes to: [ USERNAME@barcooXXX USERNAME ] $ Once logged in to the relevent compute node you can run top to view your job. Remember the u and c options we learnt earlier; they will be helpful here when everyone is running the same jobs.","title":"Task 4: Monitor the job"},{"location":"tutorials/hpc/hpc/#how-does-the-cpu-and-memory-usage-change-over-time","text":"Hint It should vary (within the limits you set in the job script) Answer The *fakejob* program should vary its CPU usage between 50 and 100% CPU and 500 and 1000MB of memory. The percentage that it shows is based on the total memory of the node that runs your job; check Topic 4, Question 4.2 to remember how to find the total memory.","title":"How does the CPU and Memory usage change over time?"},{"location":"tutorials/hpc/hpc/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable to begin submitting real jobs to the HPC (in your real account, not the training one). You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don\u2019t forget to complete the training survey and return it to the workshop facilitators.","title":"Finished"},{"location":"tutorials/hpc/intro/","text":"What is an HPC? \u00b6 An HPC is simply a large collection of server-grade computers working together to solve large problems. Big : HPCs typically have lots of CPUs and Memory and consequently large jobs. Shared : There are usually lots of users making use of it at one time. Coordinated : There is a coordinator program to ensure fair-use between its users. Compute Collection : HPCs use a number of computers at once to solve lots of large jobs. Figure : The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available. Why use HPCs? \u00b6 The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. Many CPUs : HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. Large Memory : Hundreds of GBs to multiple TBs of RAM are typical for each node. Efficient use : Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later. Software Modules \u00b6 There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. Packaged : to avoid conflicts between software, each is packaged up into a module and only used on demand. Loadable : before using a software module you need to load it. Versions : given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control. Job Submission \u00b6 Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. SLURM : this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. Queues (Partition) : when a job is submitted it is added to a work queue; in SLURM this is called a Partition. Batch : HPC jobs are not \u2018interactive\u2019. By this we mean, you can\u2019t type input into your job\u2019s programs and you won\u2019t immediately see the output that your program prints on the screen. Resources \u00b6 So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. CPUs : most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. Memory : you need to estimate (or guess) how much memory (RAM) your program needs. Nodes : most software will only use one of the HPC\u2019s Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. Time : like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards. Job Types \u00b6 There are two types of jobs that you can submit: Shared : a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. Exclusive : an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Intro"},{"location":"tutorials/hpc/intro/#what-is-an-hpc","text":"An HPC is simply a large collection of server-grade computers working together to solve large problems. Big : HPCs typically have lots of CPUs and Memory and consequently large jobs. Shared : There are usually lots of users making use of it at one time. Coordinated : There is a coordinator program to ensure fair-use between its users. Compute Collection : HPCs use a number of computers at once to solve lots of large jobs. Figure : The user (face at top) interacts with their local PC/Laptop through the keyboard and screen. The PC/Laptop will connect to the Head/Login node of the HPC interactively. The Head/Login node will send the jobs off to the Compute Nodes when one is available.","title":"What is an HPC?"},{"location":"tutorials/hpc/intro/#why-use-hpcs","text":"The main reason we use HPCs is because they are quite big. Given their size, they are usually very expensive, however through sharing the resources the per user/job cost can be kept low. Many CPUs : HPCs typically have hundreds to tens of thousands of CPUs. Compare this with the 4 or 8 that your PC/Laptop might have. Large Memory : Hundreds of GBs to multiple TBs of RAM are typical for each node. Efficient use : Through sharing the resources each user can have access to a very large computer for a period and hand it back for others to use later.","title":"Why use HPCs?"},{"location":"tutorials/hpc/intro/#software-modules","text":"There are typically hundreds to thousands of software packages installed on an HPC. Given that each can have its own special requirements and multiple versions will be made, software on the HPC will most commonly be packaged and only made available to you when you request it. Packaged : to avoid conflicts between software, each is packaged up into a module and only used on demand. Loadable : before using a software module you need to load it. Versions : given not all users want to use the same version of software (and to compare new results with old you might need the same version) each version is made into its own software module so you have ultimate control.","title":"Software Modules"},{"location":"tutorials/hpc/intro/#job-submission","text":"Job Submission is the process of instructing the HPC to perform a task for you. Depending on the HPC software installed on your HPC, the process of doing so might be different. SLURM : this workshop uses an HPC that uses the SLURM HPC software. Some common alternatives (not covered) are PBS or SGE/OGE. Queues (Partition) : when a job is submitted it is added to a work queue; in SLURM this is called a Partition. Batch : HPC jobs are not \u2018interactive\u2019. By this we mean, you can\u2019t type input into your job\u2019s programs and you won\u2019t immediately see the output that your program prints on the screen.","title":"Job Submission"},{"location":"tutorials/hpc/intro/#resources","text":"So that SLURM knows how to schedule and fit jobs around each other, you need to specify what resources your job will use. That is, you need to tell it how many CPUs, RAM, Nodes (servers), and Time you need. CPUs : most software is limited using 1 CPU by default but many can use more than one (or you can run multiple copies at once). The number of CPUs you specify needs to match how many things your software can do at once. Memory : you need to estimate (or guess) how much memory (RAM) your program needs. Nodes : most software will only use one of the HPC\u2019s Nodes (i.e. one server), but some software can make use of more than one to solve the problem sooner. Time : like when you are scheduling meetings, SLURM needs to know how long each job will take (maximum) so it can organise other jobs afterwards.","title":"Resources"},{"location":"tutorials/hpc/intro/#job-types","text":"There are two types of jobs that you can submit: Shared : a shared job (as the name suggests) is one that shares a node with other jobs. This is the default and preferred method. Exclusive : an exclusive job gets a single (or multiple) nodes to itself. Given this exclusivity, this type of job must know how to use multiple CPUs as most HPCs have at least 16 CPUs per node.","title":"Job Types"},{"location":"tutorials/hpc/robinson-hpc-link/","text":"High Performance Computing \u00b6 Please see the link here .","title":"High Performance Computing"},{"location":"tutorials/hpc/robinson-hpc-link/#high-performance-computing","text":"Please see the link here .","title":"High Performance Computing"},{"location":"tutorials/hybrid_assembly/formatting_template/","text":"Formats to use \u00b6 Text formatting \u00b6 Bold Italics Bold Italics Headings (This is 2 nd level) \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Code Blocks and inline code \u00b6 They can be added like this. Many different languages are supported. Blocks \u00b6 1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf Inline code \u00b6 Code can also be shown as an inline snippet like this import tensorflow as tf . Lists \u00b6 If you need to add a list: Unordered Lists \u00b6 Some information Some more information Ordered Lists \u00b6 Some point Another point Subpoint Sub-subpoint Images \u00b6 How to add an image: Tables \u00b6 Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit Questions and Answers \u00b6 It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: Line Breaks \u00b6 To create a line break (\\<br>), end a line with two or more spaces, and then type return. Links \u00b6 Please see the link . Blockquotes \u00b6 This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote. Text including commands to type \u00b6 Type ls and press ENTER . When referring to a filename \u00b6 canu.contigs.fasta contains the assembled sequences. Showing that a button needs clicking \u00b6 Click Start Highlighting text \u00b6 This text is highlighted. Equations \u00b6 Equations can be added as a block or inline. Block equations \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline equations \u00b6 This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} To do \u00b6 Pdf printing Survey Slides?","title":"Formats to use"},{"location":"tutorials/hybrid_assembly/formatting_template/#formats-to-use","text":"","title":"Formats to use"},{"location":"tutorials/hybrid_assembly/formatting_template/#text-formatting","text":"Bold Italics Bold Italics","title":"Text formatting"},{"location":"tutorials/hybrid_assembly/formatting_template/#headings-this-is-2nd-level","text":"","title":"Headings (This is 2nd level)"},{"location":"tutorials/hybrid_assembly/formatting_template/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"tutorials/hybrid_assembly/formatting_template/#the-4th-level","text":"","title":"The 4th level"},{"location":"tutorials/hybrid_assembly/formatting_template/#the-5th-level","text":"","title":"The 5th level"},{"location":"tutorials/hybrid_assembly/formatting_template/#the-6th-level","text":"","title":"The 6th level"},{"location":"tutorials/hybrid_assembly/formatting_template/#code-blocks-and-inline-code","text":"They can be added like this. Many different languages are supported.","title":"Code Blocks and inline code"},{"location":"tutorials/hybrid_assembly/formatting_template/#blocks","text":"1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf","title":"Blocks"},{"location":"tutorials/hybrid_assembly/formatting_template/#inline-code","text":"Code can also be shown as an inline snippet like this import tensorflow as tf .","title":"Inline code"},{"location":"tutorials/hybrid_assembly/formatting_template/#lists","text":"If you need to add a list:","title":"Lists"},{"location":"tutorials/hybrid_assembly/formatting_template/#unordered-lists","text":"Some information Some more information","title":"Unordered Lists"},{"location":"tutorials/hybrid_assembly/formatting_template/#ordered-lists","text":"Some point Another point Subpoint Sub-subpoint","title":"Ordered Lists"},{"location":"tutorials/hybrid_assembly/formatting_template/#images","text":"How to add an image:","title":"Images"},{"location":"tutorials/hybrid_assembly/formatting_template/#tables","text":"Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit","title":"Tables"},{"location":"tutorials/hybrid_assembly/formatting_template/#questions-and-answers","text":"It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this:","title":"Questions and Answers"},{"location":"tutorials/hybrid_assembly/formatting_template/#line-breaks","text":"To create a line break (\\<br>), end a line with two or more spaces, and then type return.","title":"Line Breaks"},{"location":"tutorials/hybrid_assembly/formatting_template/#links","text":"Please see the link .","title":"Links"},{"location":"tutorials/hybrid_assembly/formatting_template/#blockquotes","text":"This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote.","title":"Blockquotes"},{"location":"tutorials/hybrid_assembly/formatting_template/#text-including-commands-to-type","text":"Type ls and press ENTER .","title":"Text including commands to type"},{"location":"tutorials/hybrid_assembly/formatting_template/#when-referring-to-a-filename","text":"canu.contigs.fasta contains the assembled sequences.","title":"When referring to a filename"},{"location":"tutorials/hybrid_assembly/formatting_template/#showing-that-a-button-needs-clicking","text":"Click Start","title":"Showing that a button needs clicking"},{"location":"tutorials/hybrid_assembly/formatting_template/#highlighting-text","text":"This text is highlighted.","title":"Highlighting text"},{"location":"tutorials/hybrid_assembly/formatting_template/#equations","text":"Equations can be added as a block or inline.","title":"Equations"},{"location":"tutorials/hybrid_assembly/formatting_template/#block-equations","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Block equations"},{"location":"tutorials/hybrid_assembly/formatting_template/#inline-equations","text":"This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline equations"},{"location":"tutorials/hybrid_assembly/formatting_template/#to-do","text":"Pdf printing Survey Slides?","title":"To do"},{"location":"tutorials/hybrid_assembly/layout_template/","text":"Workshop title \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with \u2026 Description \u00b6 Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026 Requirements and preparation \u00b6 Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Preparing your laptop prior to starting this workshop \u00b6 Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026 Required Software \u00b6 IGV Software x or No additional software needs to be installed for this workshop. Required Data \u00b6 Data file or No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019 Background \u00b6 Little bit of history and context. Why is this important/useful\u2026 Section 1: Title of section 1 \u00b6 In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc. Section 2: Title of section 2 \u00b6 In this section we will \u2026 Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials.","title":"Layout template"},{"location":"tutorials/hybrid_assembly/layout_template/#workshop-title","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Workshop title"},{"location":"tutorials/hybrid_assembly/layout_template/#overview","text":"","title":"Overview"},{"location":"tutorials/hybrid_assembly/layout_template/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/hybrid_assembly/layout_template/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with \u2026","title":"Skill level"},{"location":"tutorials/hybrid_assembly/layout_template/#description","text":"Punchline\u2026 Get the skills to advance your own research! This tutorial uses \u201cTool\u201d to implement a pipeline for analysis of data \u2026 Data \u2026 Pipeline \u2026 Tools \u2026 Section 1 covers one part and in Section 2 you will learn something additional.","title":"Description"},{"location":"tutorials/hybrid_assembly/layout_template/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the \u2026 Be familiar with \u2026 Know how to \u2026","title":"Learning Objectives"},{"location":"tutorials/hybrid_assembly/layout_template/#requirements-and-preparation","text":"Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/hybrid_assembly/layout_template/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Go to: https://www. Download and install the required software (free) on your laptop. Download and install the required data on your laptop. Check that the software and data are correctly installed by executing this test: \u2026","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/hybrid_assembly/layout_template/#required-software","text":"IGV Software x or No additional software needs to be installed for this workshop.","title":"Required Software"},{"location":"tutorials/hybrid_assembly/layout_template/#required-data","text":"Data file or No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/hybrid_assembly/layout_template/#author-information","text":"Written by: Victoria Perreau Melbourne Bioinformatics, University of Melbourne Created/Reviewed: July 2019","title":"Author Information"},{"location":"tutorials/hybrid_assembly/layout_template/#background","text":"Little bit of history and context. Why is this important/useful\u2026","title":"Background"},{"location":"tutorials/hybrid_assembly/layout_template/#section-1-title-of-section-1","text":"In this section you will \u2026 Important Please look at the formatting template to see examples for formatting your documentation in line with Melbourne Bioinformatics training material e.g Questions and answers, code blocks, images etc.","title":"Section 1: Title of section 1"},{"location":"tutorials/hybrid_assembly/layout_template/#section-2-title-of-section-2","text":"In this section we will \u2026","title":"Section 2: Title of section 2"},{"location":"tutorials/hybrid_assembly/layout_template/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials.","title":"Additional reading"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/","text":"Hybrid genome assembly - nanopore and illumina \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. How do long- and short-read assembly methods differ? Description \u00b6 Assemble a genome! Learn how to create and assess genome assemblies using the powerful combination of nanopore and illumina reads This tutorial explores how long and short read data can be combined to produce a high-quality \u2018finished\u2019 bacterial genome sequence. Termed \u2018hybrid assembly\u2019, we will use read data produced from two different sequencing platforms, Illumina (short read) and Oxford Nanopore Technologies (long read), to reconstruct a bacterial genome sequence. In this tutorial we will perform \u2018 de novo assembly\u2019. De novo assembly is the process of assembling a genome from scratch using only the sequenced reads as input - no reference genome is used. This approach is common practise when working with microorganisms, and has seen increasing use for eukaryotes (including humans) in recent times. Using short read data (Illumina) alone for de novo assembly will produce a complete genome, but in pieces (commonly called a \u2018draft genome\u2019). For the genome to be assembled into a single chromosome (plus a sequence for each plasmid), reads would need to be longer than the longest repeated element on the genome (usually ~7,000 base pairs, Note: Illumina reads are 350 base maximum). Draft bacterial genome sequences are cheap to produce (less than AUD$60) and useful (>300,000 draft Salmonella enterica genome sequences published at NCBI https://www.ncbi.nlm.nih.gov/pathogens/organisms/ ), but sometimes you need a high-quality \u2018finished\u2019 bacterial genome sequence. There are <1,000 are \u2018finished\u2019 or \u2018closed\u2019 Salmonella enterica genome sequences. In these cases, long reads can be used together with short reads to produce a high-quality assembly. Nanopore long reads (commonly >40,000 bases) can fully span repeats, and reveal how all the genome fragments should be arranged. Long reads currently have higher error rate than short reads, so the combination of technologies is particularly powerful. Long reads provide information on the genome structure, and short reads provide high base-level accuracy. Combining read data from the long and short read sequencing platforms allows the production of a complete genome sequence with very few sequence errors, but the cost of the read data is about AUD$ 1,000 to produce the sequence. Understanably, we usually produce a draft genome sequence with very few sequence errors using the Illumina sequencing platform. Nanopore sequencing technology is rapidly improving, expect the cost difference to reduce!! Data: Nanopore reads, Illlumina reads, bacterial organism ( Bacillus subtilis ) reference genome Tools: Flye, Pilon, Unicycler, Quast, BUSCO Pipeline: Hybrid de novo genome assembly - Nanopore draft Illumina polishing Pipeline: Hybrid de novo genome assembly - Unicycler Learning Objectives \u00b6 At the end of this introductory workshop, you will: Understand how Nanopore and Illumina reads can be used together to produce a high quality assembly Be familiar with genome assembly and polishing programs Learn how to assess the quality of a genome assembly, regardless of whether a reference genome is present or absent Be able to assemble an unknown, previously undocumented genome to high-quality using Nanopore and Illumina reads! Requirements and preparation \u00b6 Attendees are required to bring their own laptop computers. All data and tools are available on usegalaxy.org.au. You will need a computer to connect to and use their platform. Before the tutorial, navigate to https://usegalaxy.org.au/ and use your email to create an account. Click \u201cLogin or register\u201d in the top navigation bar of galaxy to do this. Preparing your laptop prior to starting this workshop \u00b6 No additional software needs to be installed for this workshop. Required Data \u00b6 No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Grace Hall Melbourne Bioinformatics, The University of Melbourne Created/Reviewed: September 2020 Background \u00b6 How do we produce the genomic DNA for a bacterial isolate? \u00b6 Traditional in vitro culture techniques are important. Take a sample (e.g. a swab specimen from an infected sore) and streak a \u2018loopful\u2019 on to solid growth medium that suppoprts the growth of the bacteria. Technology from the time of Louis Pasteur! Mixtures of bacterial types can be sequenced e.g. prepare genomic DNA from environmental samples containing bacteria - water, soil, faecal samples etc. (Whole Metagenome Sequencing) One colony contains 10 7 \u2013 10 8 cells. The genomic DNA extracted from one colony is enough for Illumina sequencing. Larger amounts of genomic DNA are required for Nanopore sequencing. Shotgun sequencing - Illumina Sequencing Library \u00b6 Genomic DNA is prepared for sequencing by fragmenting/shearing: multiple copies of Chromosome + plasmid \u2192 ~500 bp fragments Note: Nanopore sequencing - there is usually no need to shear the genomic DNA specialist methods are used to minimise shearing during DNA preparation . For Nanopore sequencing the longer the DNA fragments the better! The read data \u00b6 Nanopore & Illumina: fastq format Section 1: Nanopore draft assembly, Illumina polishing \u00b6 In this section you will use Flye to create a draft genome assembly from Nanopore reads. We will perform assembly, then assess the quality of our assembly using two tools: Quast, and BUSCO. Getting the data \u00b6 Make sure you have an instance of Galaxy ready to go. Navigate to the Galaxy Australia server and sign in if you have an account. Copy an existing history The data you will need is available in an existing Galaxy history. You can create a copy of this history by clicking here and using the import history \u2018+\u2019 icon at the top right of the page. Look at the history you imported There are 4 files - Nanopore reads, a set of paired-end Illumina reads, and a reference genome for the organism we will assemble. Will we use this reference genome to assess the quality of our assemblies and judge which methods work best. Draft assembly with Flye + Nanopore reads \u00b6 We can use Flye to create an assembly from Nanopore reads. Making sure you are on the \u2018Analyse Data\u2019 tab of Galaxy, look for the tool search bar at the top of the left panel. Search for \u2018Flye\u2019 and select the tool We need to provide some information to Flye. Set the \u2018Input reads\u2019 parameter to nanopore_reads.fastq and \u2018estimated genome size\u2019 to 4m. Leave all else defualt. Run Flye by clicking \u2018execute\u2019 at the bottom of the page. Flye produces a number of outputs. We only need the \u2018consensus\u2019 fasta file. You can delete the other outputs. For clarity, the consensus draft assembly can be renamed to something which makes sense, like \u2018nanopore draft assembly\u2019 Assessing draft assembly quality \u00b6 Quast We need to check if our assembly is good quality or not. It is paramount that genome assemblies are high-quality for them to be useful. The supplied reference genome allows a direct comparison. We can use a tool call \u2018Quast\u2019 to compare our assembly to the reference genome. Search for the Quast tool in the tools panel. Parameters: Contigs/scaffolds file = the nanopore draft assembly you just created Use a reference genome? = Yes Reference genome = reference_genome.fasta All else default Execute Quast by clicking \u2018execute\u2019 at the bottom of the page. We are mainly interested in one of the outputs - the HTML report Open the report. It may look something like this: Note the Genome fraction (%), # mismatches per 100 kbp, # indels per 100 kbp and # contigs information. We seem to have good coverage and not too many contigs, but our error rate is quite high. BUSCO In this case we were able to use a reference genome to assess assembly quality, but this is not always the case. When our sample organism is unknown, we need another method to assess assembly quality. BUSCO analysis is one way to do this. BUSCO analysis uses the presence, absence, or fragmentation of key genes in an assembly to determine is quality. BUSCO genes are specifically selected for each taxonomic clade, and represent a group of genes which each organism in the clade is expected to possess. At higher clades, \u2018housekeeping genes\u2019 are the only members, while at more refined taxa such as order or family, lineage-specific genes can also be used. Find and select the Busco tool in the tools panel using the search bar. We will assess our Nanopore draft assembly created by Flye. In this tutorial, we will suspect that our organism is within the \u2018Bacillales\u2019 order. Parameters: Sequences to analyse = our Nanopore draft assembly Lineage = Bacillales Leave all else default and execute the program. After the program has run, look at the \u2018short summary\u2019 output. It may look something like this: The \u2018full table\u2019 is also useful. It gives a detailed list of the genes we are searching for, and information about whether they would missing, fragmented, or complete in our assembly. It seems that most expected genes are missing or fragmented in our assembly. It is likely that the frequent errors in our draft assembly are causing this result. We should be able improve our assembly with the Illumina reads available and correct some of these errors. This process involves two steps. We will first align the Illumina reads to our draft assembly, then supply the mapping information to Pilon which will use this alignment information to error-correct our assembly. Assembly Polishing with Pilon \u00b6 Illumina reads have much higher per-base accuracy than Nanopore reads. We will map the Illumina read sets to our draft assembly using a short-read aligner called BWA-MEM, then can give Pilon this alignment file to polish our draft assembly. Map Illumina reads to draft assembly Search for \u2018Map with BWA-MEM\u2019 in the tools panel and select Parameters: Will you select a reference genome from your history or use a built-in index? - Use a genome from history and build index Use the following dataset as the reference sequence - Select your Nanopore draft assembly Single or Paired-end reads - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Leave all else default and execute the program. The output will be a .BAM file (Binary Alignment Map). This is tabular data recording information about how reads were aligned to the draft assembly. We can now use this output .BAM file as an input to Pilon. Polish assembly with Pilon Search for \u2018pilon\u2019 in the tools panel and select Parameters: Select a reference genome - Your Nanopore draft assembly Input BAM file - The output .BAM file of BWA-MEM alignment Variant calling mode - No Leave all else default and execute the program. Pilon gives a single output file - the polished assembly. Compare draft and polished assemblies We are now interested to see how much pilon improved our draft assembly. Run Quast as before with the new, polished assembly - Make note of # mismatches per 100 kbp and # indels per 100 kbp. How much has our base accuracy improved? Run BUSCO as before with the new, polished assembly - Have we identified more expected genes? All going well, the polished assembly should be much higher quality than our draft. The per-base accuracy of our assembly contigs should have markedly improved. This is reflected in the lower mismatches and indels per 100kbp reported by Quast, and the higher number of complete BUSCO genes. Our contiguity and coverage (as measured by the genome fraction (%) statistic reported by Quast) may not show the same level of improvement, as the polishing step is mainly aimed at improving per-base contig accuracy. Our next step is to use a purpose-built hybrid de novo assembly tool, and compare its performance with our sequential draft + polishing approach. Section Questions \u00b6 Which read set - short or long - was used to create our draft? Answer (click to reveal) Long reads (Nanopore) were used to create the draft. Nanopore reads allow excellent recreation of the proper structure of the genome, and adequately handle repeat regions. How was the draft polished? Answer (click to reveal) Illumina reads have higher per-base accuracy than Nanopore. Illumina reads were aligned to the draft assembly, then Pilon used this alignment information to improve locations with errors in the assembly. How does Quast inform on assembly quality? Answer (click to reveal) Quast shows summary information about the assembly contigs. If a reference genome is given, it informs the genome fraction (how much of the reference is covered by the assembly), if any genomic regions appear duplicated, and error information including the rate of mismatches and indels. How does BUSCO inform on assembly quality? Answer (click to reveal) BUSCO does not use a reference genome to compare. It attempts to locate key genes which should be present in the assembly, and reports whether it could/could not find those genes. If a key gene is found, it reports whether the gene was fragmented (errors) or complete. \u00b6 Section 2: Purpose-built hybrid assembly tool - Unicycler \u00b6 In this section we will use a purpose-built tool called Unicycler to perform hybrid assembly. Unicycler uses our Nanopore and Illumina read sets together as input, and returns an assembly. Once we have created the assembly, we will assess its quality using Quast and BUSCO and compare with our previous polished assembly. We will also perform BUSCO analysis on the supplied reference genome itself, to record a baseline for our theoretical best BUSCO report. Hybrid de novo assembly with Unicycler \u00b6 Unicycler performs assembly in the opposite manner to our approach. Illumina reads are used to create an assembly graph, then Nanopore reads are used to disentangle problems in the graph. The Nanopore reads serve to bridge Illumina contigs, and to reveal how the contigs are arranged sequentially in the genome. Run Unicycler Find Unicycler in the tools panel. It is listed as \u2018Create assemblies with Unicycler\u2019 Run Unicycler using the Nanopore and Illumina read sets. Parameters: Paired or Single end data? - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Select long reads - nanopore_reads.fastq (if nanopore_reads.fastq does not appear in the dropdown, its datatype needs to be changed - click then pencil icon next to nanopore_reads.fastq in the history panel -> \u2018Datatypes\u2019 tab -> \u2018New Type\u2019 - fastqsanger) Leave all else default and execute the program. Unicycler will output two files - a Final Assembly, and a Final Assembly Graph. We are interested in the Final Assembly. Comparing Unicycler assembly to Nanopore + Illumina polished assembly BUSCO and Quast can be used again to assess this assembly. As a purpose-built tool, it generally produces much better assemblies than our sequential approach. This is reflected as (Quast) a lower number of contigs, lower mismatches and indels per 100kb, and (BUSCO) greater number of BUSCO genes complete. It is important to put perspective on the BUSCO analysis results. By running BUSCO on our supplied high-quality reference genome for this organism, we will gather the BUSCO analysis results for a 'theoretically' perfect assembly of the organism. This can provide more confidence in our quality esimates when using BUSCO. Run BUSCO on the supplied, high-quality reference genome. At time of writing, these were the BUSCO results: It seems that one BUSCO gene has two copies in the reference genome, and one other gene is fragmented. Copy number variation is not uncommon, and so the duplicated BUSCO may not represent an assembly error. Similarly, the fragmented BUSCO may be due to the appearence of multiple SNPs rather than sequencing error. Our organism may have experienced some mutation relative to the reference sequence for the BUSCO in question, causing it to appear 'fragmented'. Section Questions \u00b6 Why did we select \u2018Paired\u2019 for our Illumina reads in the Unicycler tool? Answer (click to reveal) Our short read set was 'paired-end'. Short read technology can only sequence a few hundred base-pairs in a single read. To provide better structural information, paired-end sequencing was created, where longer fragments (fixed length) are used. A few hundred bp is sequenced at both ends of the fragment, leaving the middle section unsequenced. The reads produced (the mate-pair) from a single fragment are separated by a fixed length, so we know they are nearby in the genome. Does Unicycler begin by using the Long or Short reads? Answer (click to reveal) Unicycler uses short reads first. It creates an assembly graph from short reads, then uses the long reads to provide better structural information of the genome. How does Unicycler use long reads to improve its assembly graph? Answer (click to reveal) The assembly graph produced by short reads has tangled regions. When we don't know how sections of the genome are arranged, tangled regions appear in the graph. Unicycler uses Nanopore reads which overlap these tangled regions to resolve the proper structure of the genome. Conclusion \u00b6 We have learned two methods for hybrid de novo assembly. The combination of long- and short-read technology is clearly powerful, represented by our ability to create a good assembly with only 25x coverage (100Mb) of Nanopore, and 50x coverage of Illumina reads (200Mb). To further improve our assembly, extra Nanopore read data may provide most benefit. At 50x coverage (200Mb), we may achieve a single, or few contig assembly with high per-base accuracy. The development of new purpose-built tools for hybrid de novo assembly like Unicycler have improved the quality of assemblies we can produce. These tools are of great importance and while they already produce great results, they will continue to improve over time. Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials. Flye: https://github.com/fenderglass/Flye/blob/flye/docs/USAGE.md#algorithm Pilon: https://github.com/broadinstitute/pilon/wiki/Methods-of-Operation Unicycler: https://github.com/rrwick/Unicycler Quast: https://academic.oup.com/bioinformatics/article/29/8/1072/228832 BUSCO analysis: https://academic.oup.com/bioinformatics/article/31/19/3210/211866","title":"Hybrid genome assembly - Nanopore and Illumina"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#hybrid-genome-assembly-nanopore-and-illumina","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Hybrid genome assembly - nanopore and illumina"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#overview","text":"","title":"Overview"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. How do long- and short-read assembly methods differ?","title":"Skill level"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#description","text":"Assemble a genome! Learn how to create and assess genome assemblies using the powerful combination of nanopore and illumina reads This tutorial explores how long and short read data can be combined to produce a high-quality \u2018finished\u2019 bacterial genome sequence. Termed \u2018hybrid assembly\u2019, we will use read data produced from two different sequencing platforms, Illumina (short read) and Oxford Nanopore Technologies (long read), to reconstruct a bacterial genome sequence. In this tutorial we will perform \u2018 de novo assembly\u2019. De novo assembly is the process of assembling a genome from scratch using only the sequenced reads as input - no reference genome is used. This approach is common practise when working with microorganisms, and has seen increasing use for eukaryotes (including humans) in recent times. Using short read data (Illumina) alone for de novo assembly will produce a complete genome, but in pieces (commonly called a \u2018draft genome\u2019). For the genome to be assembled into a single chromosome (plus a sequence for each plasmid), reads would need to be longer than the longest repeated element on the genome (usually ~7,000 base pairs, Note: Illumina reads are 350 base maximum). Draft bacterial genome sequences are cheap to produce (less than AUD$60) and useful (>300,000 draft Salmonella enterica genome sequences published at NCBI https://www.ncbi.nlm.nih.gov/pathogens/organisms/ ), but sometimes you need a high-quality \u2018finished\u2019 bacterial genome sequence. There are <1,000 are \u2018finished\u2019 or \u2018closed\u2019 Salmonella enterica genome sequences. In these cases, long reads can be used together with short reads to produce a high-quality assembly. Nanopore long reads (commonly >40,000 bases) can fully span repeats, and reveal how all the genome fragments should be arranged. Long reads currently have higher error rate than short reads, so the combination of technologies is particularly powerful. Long reads provide information on the genome structure, and short reads provide high base-level accuracy. Combining read data from the long and short read sequencing platforms allows the production of a complete genome sequence with very few sequence errors, but the cost of the read data is about AUD$ 1,000 to produce the sequence. Understanably, we usually produce a draft genome sequence with very few sequence errors using the Illumina sequencing platform. Nanopore sequencing technology is rapidly improving, expect the cost difference to reduce!! Data: Nanopore reads, Illlumina reads, bacterial organism ( Bacillus subtilis ) reference genome Tools: Flye, Pilon, Unicycler, Quast, BUSCO Pipeline: Hybrid de novo genome assembly - Nanopore draft Illumina polishing Pipeline: Hybrid de novo genome assembly - Unicycler","title":"Description"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#learning-objectives","text":"At the end of this introductory workshop, you will: Understand how Nanopore and Illumina reads can be used together to produce a high quality assembly Be familiar with genome assembly and polishing programs Learn how to assess the quality of a genome assembly, regardless of whether a reference genome is present or absent Be able to assemble an unknown, previously undocumented genome to high-quality using Nanopore and Illumina reads!","title":"Learning Objectives"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#requirements-and-preparation","text":"Attendees are required to bring their own laptop computers. All data and tools are available on usegalaxy.org.au. You will need a computer to connect to and use their platform. Before the tutorial, navigate to https://usegalaxy.org.au/ and use your email to create an account. Click \u201cLogin or register\u201d in the top navigation bar of galaxy to do this.","title":"Requirements and preparation"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#preparing-your-laptop-prior-to-starting-this-workshop","text":"No additional software needs to be installed for this workshop.","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#required-data","text":"No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#author-information","text":"Written by: Grace Hall Melbourne Bioinformatics, The University of Melbourne Created/Reviewed: September 2020","title":"Author Information"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#background","text":"","title":"Background"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#how-do-we-produce-the-genomic-dna-for-a-bacterial-isolate","text":"Traditional in vitro culture techniques are important. Take a sample (e.g. a swab specimen from an infected sore) and streak a \u2018loopful\u2019 on to solid growth medium that suppoprts the growth of the bacteria. Technology from the time of Louis Pasteur! Mixtures of bacterial types can be sequenced e.g. prepare genomic DNA from environmental samples containing bacteria - water, soil, faecal samples etc. (Whole Metagenome Sequencing) One colony contains 10 7 \u2013 10 8 cells. The genomic DNA extracted from one colony is enough for Illumina sequencing. Larger amounts of genomic DNA are required for Nanopore sequencing.","title":"How do we produce the genomic DNA for a bacterial isolate?"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#shotgun-sequencing-illumina-sequencing-library","text":"Genomic DNA is prepared for sequencing by fragmenting/shearing: multiple copies of Chromosome + plasmid \u2192 ~500 bp fragments Note: Nanopore sequencing - there is usually no need to shear the genomic DNA specialist methods are used to minimise shearing during DNA preparation . For Nanopore sequencing the longer the DNA fragments the better!","title":"Shotgun sequencing - Illumina Sequencing Library"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#the-read-data","text":"Nanopore & Illumina: fastq format","title":"The read data"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#section-1-nanopore-draft-assembly-illumina-polishing","text":"In this section you will use Flye to create a draft genome assembly from Nanopore reads. We will perform assembly, then assess the quality of our assembly using two tools: Quast, and BUSCO.","title":"Section 1: Nanopore draft assembly, Illumina polishing"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#getting-the-data","text":"Make sure you have an instance of Galaxy ready to go. Navigate to the Galaxy Australia server and sign in if you have an account. Copy an existing history The data you will need is available in an existing Galaxy history. You can create a copy of this history by clicking here and using the import history \u2018+\u2019 icon at the top right of the page. Look at the history you imported There are 4 files - Nanopore reads, a set of paired-end Illumina reads, and a reference genome for the organism we will assemble. Will we use this reference genome to assess the quality of our assemblies and judge which methods work best.","title":"Getting the data"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#draft-assembly-with-flye-nanopore-reads","text":"We can use Flye to create an assembly from Nanopore reads. Making sure you are on the \u2018Analyse Data\u2019 tab of Galaxy, look for the tool search bar at the top of the left panel. Search for \u2018Flye\u2019 and select the tool We need to provide some information to Flye. Set the \u2018Input reads\u2019 parameter to nanopore_reads.fastq and \u2018estimated genome size\u2019 to 4m. Leave all else defualt. Run Flye by clicking \u2018execute\u2019 at the bottom of the page. Flye produces a number of outputs. We only need the \u2018consensus\u2019 fasta file. You can delete the other outputs. For clarity, the consensus draft assembly can be renamed to something which makes sense, like \u2018nanopore draft assembly\u2019","title":"Draft assembly with Flye + Nanopore reads"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#assessing-draft-assembly-quality","text":"Quast We need to check if our assembly is good quality or not. It is paramount that genome assemblies are high-quality for them to be useful. The supplied reference genome allows a direct comparison. We can use a tool call \u2018Quast\u2019 to compare our assembly to the reference genome. Search for the Quast tool in the tools panel. Parameters: Contigs/scaffolds file = the nanopore draft assembly you just created Use a reference genome? = Yes Reference genome = reference_genome.fasta All else default Execute Quast by clicking \u2018execute\u2019 at the bottom of the page. We are mainly interested in one of the outputs - the HTML report Open the report. It may look something like this: Note the Genome fraction (%), # mismatches per 100 kbp, # indels per 100 kbp and # contigs information. We seem to have good coverage and not too many contigs, but our error rate is quite high. BUSCO In this case we were able to use a reference genome to assess assembly quality, but this is not always the case. When our sample organism is unknown, we need another method to assess assembly quality. BUSCO analysis is one way to do this. BUSCO analysis uses the presence, absence, or fragmentation of key genes in an assembly to determine is quality. BUSCO genes are specifically selected for each taxonomic clade, and represent a group of genes which each organism in the clade is expected to possess. At higher clades, \u2018housekeeping genes\u2019 are the only members, while at more refined taxa such as order or family, lineage-specific genes can also be used. Find and select the Busco tool in the tools panel using the search bar. We will assess our Nanopore draft assembly created by Flye. In this tutorial, we will suspect that our organism is within the \u2018Bacillales\u2019 order. Parameters: Sequences to analyse = our Nanopore draft assembly Lineage = Bacillales Leave all else default and execute the program. After the program has run, look at the \u2018short summary\u2019 output. It may look something like this: The \u2018full table\u2019 is also useful. It gives a detailed list of the genes we are searching for, and information about whether they would missing, fragmented, or complete in our assembly. It seems that most expected genes are missing or fragmented in our assembly. It is likely that the frequent errors in our draft assembly are causing this result. We should be able improve our assembly with the Illumina reads available and correct some of these errors. This process involves two steps. We will first align the Illumina reads to our draft assembly, then supply the mapping information to Pilon which will use this alignment information to error-correct our assembly.","title":"Assessing draft assembly quality"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#assembly-polishing-with-pilon","text":"Illumina reads have much higher per-base accuracy than Nanopore reads. We will map the Illumina read sets to our draft assembly using a short-read aligner called BWA-MEM, then can give Pilon this alignment file to polish our draft assembly. Map Illumina reads to draft assembly Search for \u2018Map with BWA-MEM\u2019 in the tools panel and select Parameters: Will you select a reference genome from your history or use a built-in index? - Use a genome from history and build index Use the following dataset as the reference sequence - Select your Nanopore draft assembly Single or Paired-end reads - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Leave all else default and execute the program. The output will be a .BAM file (Binary Alignment Map). This is tabular data recording information about how reads were aligned to the draft assembly. We can now use this output .BAM file as an input to Pilon. Polish assembly with Pilon Search for \u2018pilon\u2019 in the tools panel and select Parameters: Select a reference genome - Your Nanopore draft assembly Input BAM file - The output .BAM file of BWA-MEM alignment Variant calling mode - No Leave all else default and execute the program. Pilon gives a single output file - the polished assembly. Compare draft and polished assemblies We are now interested to see how much pilon improved our draft assembly. Run Quast as before with the new, polished assembly - Make note of # mismatches per 100 kbp and # indels per 100 kbp. How much has our base accuracy improved? Run BUSCO as before with the new, polished assembly - Have we identified more expected genes? All going well, the polished assembly should be much higher quality than our draft. The per-base accuracy of our assembly contigs should have markedly improved. This is reflected in the lower mismatches and indels per 100kbp reported by Quast, and the higher number of complete BUSCO genes. Our contiguity and coverage (as measured by the genome fraction (%) statistic reported by Quast) may not show the same level of improvement, as the polishing step is mainly aimed at improving per-base contig accuracy. Our next step is to use a purpose-built hybrid de novo assembly tool, and compare its performance with our sequential draft + polishing approach.","title":"Assembly Polishing with Pilon"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#section-questions","text":"Which read set - short or long - was used to create our draft? Answer (click to reveal) Long reads (Nanopore) were used to create the draft. Nanopore reads allow excellent recreation of the proper structure of the genome, and adequately handle repeat regions. How was the draft polished? Answer (click to reveal) Illumina reads have higher per-base accuracy than Nanopore. Illumina reads were aligned to the draft assembly, then Pilon used this alignment information to improve locations with errors in the assembly. How does Quast inform on assembly quality? Answer (click to reveal) Quast shows summary information about the assembly contigs. If a reference genome is given, it informs the genome fraction (how much of the reference is covered by the assembly), if any genomic regions appear duplicated, and error information including the rate of mismatches and indels. How does BUSCO inform on assembly quality? Answer (click to reveal) BUSCO does not use a reference genome to compare. It attempts to locate key genes which should be present in the assembly, and reports whether it could/could not find those genes. If a key gene is found, it reports whether the gene was fragmented (errors) or complete.","title":"Section Questions"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#section-2-purpose-built-hybrid-assembly-tool-unicycler","text":"In this section we will use a purpose-built tool called Unicycler to perform hybrid assembly. Unicycler uses our Nanopore and Illumina read sets together as input, and returns an assembly. Once we have created the assembly, we will assess its quality using Quast and BUSCO and compare with our previous polished assembly. We will also perform BUSCO analysis on the supplied reference genome itself, to record a baseline for our theoretical best BUSCO report.","title":"Section 2: Purpose-built hybrid assembly tool - Unicycler"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#hybrid-de-novo-assembly-with-unicycler","text":"Unicycler performs assembly in the opposite manner to our approach. Illumina reads are used to create an assembly graph, then Nanopore reads are used to disentangle problems in the graph. The Nanopore reads serve to bridge Illumina contigs, and to reveal how the contigs are arranged sequentially in the genome. Run Unicycler Find Unicycler in the tools panel. It is listed as \u2018Create assemblies with Unicycler\u2019 Run Unicycler using the Nanopore and Illumina read sets. Parameters: Paired or Single end data? - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Select long reads - nanopore_reads.fastq (if nanopore_reads.fastq does not appear in the dropdown, its datatype needs to be changed - click then pencil icon next to nanopore_reads.fastq in the history panel -> \u2018Datatypes\u2019 tab -> \u2018New Type\u2019 - fastqsanger) Leave all else default and execute the program. Unicycler will output two files - a Final Assembly, and a Final Assembly Graph. We are interested in the Final Assembly. Comparing Unicycler assembly to Nanopore + Illumina polished assembly BUSCO and Quast can be used again to assess this assembly. As a purpose-built tool, it generally produces much better assemblies than our sequential approach. This is reflected as (Quast) a lower number of contigs, lower mismatches and indels per 100kb, and (BUSCO) greater number of BUSCO genes complete. It is important to put perspective on the BUSCO analysis results. By running BUSCO on our supplied high-quality reference genome for this organism, we will gather the BUSCO analysis results for a 'theoretically' perfect assembly of the organism. This can provide more confidence in our quality esimates when using BUSCO. Run BUSCO on the supplied, high-quality reference genome. At time of writing, these were the BUSCO results: It seems that one BUSCO gene has two copies in the reference genome, and one other gene is fragmented. Copy number variation is not uncommon, and so the duplicated BUSCO may not represent an assembly error. Similarly, the fragmented BUSCO may be due to the appearence of multiple SNPs rather than sequencing error. Our organism may have experienced some mutation relative to the reference sequence for the BUSCO in question, causing it to appear 'fragmented'.","title":"Hybrid de novo assembly with Unicycler"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#section-questions_1","text":"Why did we select \u2018Paired\u2019 for our Illumina reads in the Unicycler tool? Answer (click to reveal) Our short read set was 'paired-end'. Short read technology can only sequence a few hundred base-pairs in a single read. To provide better structural information, paired-end sequencing was created, where longer fragments (fixed length) are used. A few hundred bp is sequenced at both ends of the fragment, leaving the middle section unsequenced. The reads produced (the mate-pair) from a single fragment are separated by a fixed length, so we know they are nearby in the genome. Does Unicycler begin by using the Long or Short reads? Answer (click to reveal) Unicycler uses short reads first. It creates an assembly graph from short reads, then uses the long reads to provide better structural information of the genome. How does Unicycler use long reads to improve its assembly graph? Answer (click to reveal) The assembly graph produced by short reads has tangled regions. When we don't know how sections of the genome are arranged, tangled regions appear in the graph. Unicycler uses Nanopore reads which overlap these tangled regions to resolve the proper structure of the genome.","title":"Section Questions"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#conclusion","text":"We have learned two methods for hybrid de novo assembly. The combination of long- and short-read technology is clearly powerful, represented by our ability to create a good assembly with only 25x coverage (100Mb) of Nanopore, and 50x coverage of Illumina reads (200Mb). To further improve our assembly, extra Nanopore read data may provide most benefit. At 50x coverage (200Mb), we may achieve a single, or few contig assembly with high per-base accuracy. The development of new purpose-built tools for hybrid de novo assembly like Unicycler have improved the quality of assemblies we can produce. These tools are of great importance and while they already produce great results, they will continue to improve over time.","title":"Conclusion"},{"location":"tutorials/hybrid_assembly/nanopore_assembly/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials. Flye: https://github.com/fenderglass/Flye/blob/flye/docs/USAGE.md#algorithm Pilon: https://github.com/broadinstitute/pilon/wiki/Methods-of-Operation Unicycler: https://github.com/rrwick/Unicycler Quast: https://academic.oup.com/bioinformatics/article/29/8/1072/228832 BUSCO analysis: https://academic.oup.com/bioinformatics/article/31/19/3210/211866","title":"Additional reading"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/","text":"Hybrid genome assembly - nanopore and illumina (1hr) \u00b6 Anticipated workshop duration is 1 hour . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. How do long- and short-read assembly methods differ? Description \u00b6 Assemble a genome! Learn how to create and assess genome assemblies using the powerful combination of nanopore and illumina reads This tutorial explores how long and short read data can be combined to produce a high-quality \u2018finished\u2019 bacterial genome sequence. Termed \u2018hybrid assembly\u2019, we will use read data produced from two different sequencing platforms, Illumina (short read) and Oxford Nanopore Technologies (long read), to reconstruct a bacterial genome sequence. In this tutorial we will perform \u2018 de novo assembly\u2019. De novo assembly is the process of assembling a genome from scratch using only the sequenced reads as input - no reference genome is used. This approach is common practise when working with microorganisms, and has seen increasing use for eukaryotes (including humans) in recent times. Using short read data (Illumina) alone for de novo assembly will produce a complete genome, but often in many pieces (commonly called a \u2018draft genome\u2019). For the genome to be assembled into a single chromosome (plus a sequence for each plasmid), reads would need to be longer than the longest repeated element on the genome (usually ~7,000 base pairs, Note: Illumina reads are 350 base maximum). Draft bacterial genome sequences are cheap to produce (less than AUD$60) and useful (>300,000 draft Salmonella enterica genome sequences published at NCBI https://www.ncbi.nlm.nih.gov/pathogens/organisms/ ), but sometimes you need a high-quality \u2018finished\u2019 bacterial genome sequence. There are <1,000 are \u2018finished\u2019 or \u2018closed\u2019 Salmonella enterica genome sequences. In these cases, long reads can be used together with short reads to produce a high-quality assembly. Nanopore long reads (commonly >40,000 bases) can fully span repeats, and reveal how all the genome fragments should be arranged. Long reads currently have higher error rate than short reads, so the combination of technologies is particularly powerful. Long reads provide information on the genome structure, and short reads provide high base-level accuracy. Combining read data from the long and short read sequencing platforms allows the production of a complete genome sequence with very few sequence errors, but the cost of the read data is about AUD$ 1,000 to produce the sequence. Understanably, we usually produce a draft genome sequence with very few sequence errors using the Illumina sequencing platform. Nanopore sequencing technology is rapidly improving, expect the cost difference to reduce!! Data: Nanopore reads, Illlumina reads, bacterial organism ( Bacillus subtilis ) reference genome Tools: Unicycler, Quast, BUSCO Pipeline: FastQC, MultiQC, NanoPlot, Unicycler, Quast, BUSCO Learning Objectives \u00b6 At the end of this introductory workshop, you will: Understand how Nanopore and Illumina reads can be used together to produce a high quality assembly Be familiar with genome assembly and polishing programs Learn how to assess the quality of a genome assembly, regardless of whether a reference genome is present or absent Be able to assemble an unknown, previously undocumented genome to high-quality using Nanopore and Illumina reads. Requirements and preparation \u00b6 Attendees are required to bring their own laptop computers. All data and tools are available on usegalaxy.org.au. You will need a computer to connect to and use their platform. Before the tutorial, navigate to https://usegalaxy.org.au/ and use your email to create an account. Click \u201cLogin or register\u201d in the top navigation bar of galaxy to do this. Preparing your laptop prior to starting this workshop \u00b6 No additional software needs to be installed for this workshop. Required Data \u00b6 No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Grace Hall Melbourne Bioinformatics, The University of Melbourne Created/Reviewed: March 2020 Background \u00b6 How do we produce the genomic DNA for a bacterial isolate? \u00b6 Traditional in vitro culture techniques are important. Take a sample (e.g. a swab specimen from an infected sore) and streak a \u2018loopful\u2019 on to solid growth medium that suppoprts the growth of the bacteria. Technology from the time of Louis Pasteur! Mixtures of bacterial types can be sequenced e.g. prepare genomic DNA from environmental samples containing bacteria - water, soil, faecal samples etc. (Whole Metagenome Sequencing) One colony contains 10 7 \u2013 10 8 cells. The genomic DNA extracted from one colony is enough for Illumina sequencing. Larger amounts of genomic DNA are required for Nanopore sequencing. Shotgun sequencing - Illumina Sequencing Library \u00b6 Genomic DNA is prepared for sequencing by fragmenting/shearing: multiple copies of Chromosome + plasmid \u2192 ~500 bp fragments Note: Nanopore sequencing - there is usually no need to shear the genomic DNA specialist methods are used to minimise shearing during DNA preparation . For Nanopore sequencing the longer the DNA fragments the better! Section 1: Read set summaries and QC \u00b6 In this section we will import and perform quality control (QC) on our data. Today we will use 4 pieces of data - 2 short read sets, 1 long read set, and a reference genome to compare our assembly with. Getting the data \u00b6 Make sure you have an instance of Galaxy ready to go. Navigate to the Galaxy Australia server and sign in if you have an account. Copy an existing history The data you will need is available in an existing Galaxy history: https://usegalaxy.org.au/u/graceh1024/h/hybrid-de-novo-assembly---1hr Import the history \u2018+\u2019 icon at the top right of the page. 3. Look at the history you imported * There are 4 files - Nanopore reads, two sets of Illumina reads, and a reference genome for the organism we will assemble. * Our Illumina data are paired-end reads. Two separate files will be present, with only \u20181\u2019 and \u20182\u2019 being different in their filenames. * We will use the reference_genome.fasta to assess the quality of our assembly Read set summaries \u00b6 Often, it is prudent to first assess the quality of our read sets. For the short reads, we are concerned with base quality, sequence duplication, and presence of adapter sequences. For nanopore, we want to know about the length and quality distribution of reads, as these may both be highly variable. FastQC creates summary reports for short read data. We will use this tool twice - once for each Illumina read set. We can then use a tool called MultiQC to combine these reports for easy viewing. For Nanopore data, NanoPlot is a great option. It creates plots which aim to summarise the length and quality distribution of long read sets. Depending on these summaries, we may choose to perform a QC step to remove any poor quality reads before proceeding. Run FastQC on each short read set * Find FastQC in the tools panel. It is listed as \u2018 FastQC Read Quality reports\u2019 * Parameters: * Short read data from your current history: illumina_reads_1.fastq * Leave all else default and execute the program. * Run FastQC again as above, but change Short read data from your current history: to \u2018illumina_reads_2.fastq\u2019. * Rename the FastQC RawData output for illumina_reads_1.fastq to \u2018FastQC reads 1 - RawData\u2019 * Rename the FastQC RawData output for illumina_reads_2.fastq to \u2018FastQC reads 2 - RawData\u2019 FastQC produces two outputs - \u2018RawData\u2019, and \u2018Webpage\u2019. Typically, the webpage is for human viewing, and the RawData can be given to other programs, such as MultiQC. At this stage, we have two FastQC outputs - one for each short read set. We will now combine them into a single output using MultiQC for easier interpretation. Run MultiQC * Find MultiQC in the tools panel. It is listed as \u2018 MultiQC aggregate results from bioinformatics analyses into a single report\u2019 * Parameters: * Which tool was used generate logs? FastQC * FastQC output * Type of FastQC output? RawData * FastQC ouptut FastQC reads 1 - RawData * click \u2018+ Insert FastQC output\u2019 * FastQC output * Type of FastQC output? RawData * FastQC ouptut FastQC reads 2 - RawData * Leave all else default and execute the program. MultiQC also produces two outputs - \u2018Stats\u2019 and \u2018Webpage\u2019. Inspect the Webpage output by clicking the eye icon on this history item. MultiQC produces a number of plots, but we are only interested in two today: the \u2018Sequence Counts\u2019 and \u2018Sequence Quality Histograms\u2019 plots. The \u2018Sequence Count\u2019 displays multiple pieces of information. Firstly, we can see that both read sets have the same total number of reads (sanity check - the number should be identical otherwise some reads don\u2019t have a mate!). Additionally, \u2018Duplicate Reads\u2019 are a fraction of the \u2018Unique Reads\u2019. High levels of sequence duplication can be caused by many factors, but since this is whole genome sequencing (WGS) data, we expect most reads to be unique. Scrolling down, the \u2018Sequence Quality Histograms\u2019 plot informs us that read 1 for each mate pair is generally lower quality than read 2. This said, they are both high quality and will be fine for our purpose. Our Illumina reads seem to be reasonable quality. We will now inspect the Nanopore reads. Run NanoPlot * Find MultiQC in the tools panel. It is listed as \u2018 NanoPlot Plotting suite for Oxford Nanopore sequencing data and alignments\u2019 * Parameters: * Type of the file(s) to work on * files nanopore_reads.fastq * Leave all else default and execute the program. NanoPlot produces 5 outputs, but we are only interested in the \u2018HTML report\u2019 output. View this file by clicking the eye icon on this history item. The NanoPlot HTML report includes a table, followed by a number of plots. The table provides a summary of the read set. The main statistics we will look at are Median read length , Median read quality , and Number of reads . Our median read length (3261 bp) is short for Nanopore data, but the median quality is good (11.7), equating to a per base accuracy of approximately 93%. Below the summary table, we see plots relating to read length and quality distributions. Below is the log transformed read length histogram: The vast majority of our reads sit between 1 - 10 kbp in length. Looking at this histogram closely, we see that the histogram is abruptly cut off at 1000 bp - this read set has previously been filtered to remove reads less than 1000 base pairs, resulting in this effect! Our read sets appear to be good enough to proceed without performing further QC. Next, we will create an assembly from these reads using Unicycler. Section 2: Hybrid de-novo assembly \u00b6 In this section you will use a tool called \u2018Unicycler\u2019 to create a draft genome assembly from Illumina and Nanopore reads. Unicycler \u00b6 Unicycler performs assembly using multiple steps. Illumina reads are used first to create an assembly graph using a program called SPAdes , then Nanopore reads are used to disentangle problems in the graph. The Nanopore reads serve to bridge Illumina contigs, and to reveal how the contigs are arranged sequentially in the genome. After the assembly is created, a program called Pilon uses the Illumina reads to perform a final round of error-correction before the final assembly is output. For more information on Unicycler, see this link: https://github.com/rrwick/Unicycler Run Unicycler Find Unicycler in the tools panel. It is listed as \u2018Create assemblies with Unicycler\u2019 Run Unicycler using the Nanopore and Illumina read sets. Parameters: Paired or Single end data? - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Select long reads - nanopore_reads.fastq (if nanopore_reads.fastq does not appear in the dropdown, its datatype needs to be changed - click then pencil icon next to nanopore_reads.fastq in the history panel -> \u2018Datatypes\u2019 tab -> \u2018New Type\u2019 - fastqsanger) SPAdes options Number of k-mer steps to use in SPAdes assembly 5 Rotation options Do not rotate completed replicons to start at a standard gene. \u2018Yes\u2019 Pilon options Do not use Pilon to polish the final assembly. \u2018Yes\u2019 Leave all else default and execute the program. Rename the \u2018Final Assembly\u2019 output of Unicycler to \u2018Unicycler_assembly.fasta\u2019 Unicycler will output two files - a Final Assembly, and a Final Assembly Graph. We are interested in the Final Assembly. For the sake of time, we have disabled some quality settings in Unicycler. We reduced the number of k-mer steps from 10 to 5 in SPAdes assembly (overall worse assembly), have disabled assembly polishing (improves base-level accuracy), and have chosen not to set the start of each replicon to a standard position. Given more time, we would leave these default and not disable any of these features. We now have an assembly which we can use for all kinds of downstream analysis. That said, we currently have no idea of the overall quality of this assembly. Additionally, we disabled features which improve the assembly quality due to time constraints, so this is particularly important in our case. Many factors may impact assembly quality, including the following: * Read depth / amount of sequence data * Read quality * The repetitiveness of the sequenced genome * Its ploidy (number of copies of each chromosome - humans are diploid: 2 copies) We need to therefore assess the quality of the assembly we have created. We will use two tools - Quast , and BUSCO to do this. Section Questions \u00b6 Why did we select \u2018Paired\u2019 for our Illumina reads in the Unicycler tool? Answer (click to reveal) Our short read set was 'paired-end'. Short read technology can only sequence a few hundred base-pairs in a single read. To provide better structural information, paired-end sequencing was created, where longer fragments (fixed length) are used. A few hundred bp is sequenced at both ends of the fragment, leaving the middle section unsequenced. The reads produced (the mate-pair) from a single fragment are separated by a fixed length, so we know they are nearby in the genome. Does Unicycler begin by using the Long or Short reads? Answer (click to reveal) Unicycler uses short reads first. It creates an assembly graph from short reads, then uses the long reads to provide better structural information of the genome. How does Unicycler use long reads to improve its assembly graph? Answer (click to reveal) The assembly graph produced by short reads has tangled regions. When we don't know how sections of the genome are arranged, tangled regions appear in the graph. Unicycler uses Nanopore reads which overlap these tangled regions to resolve the proper structure of the genome. Section 3: Assessing assembly quality \u00b6 Once we have created the assembly, we need to assess its quality. In this section we will use Quast and BUSCO to perform this assessment. We will also perform BUSCO analysis on the supplied reference genome itself, to record a baseline for our theoretical best BUSCO report. Quast \u00b6 A reference genome for the organism we sequenced has been supplied - listed as reference_genome.fasta in the history. This reference genome was assembled using a much higher sequencing depth, and can be used as \u2018ground truth\u2019 to assess our assembly against. The \u2018Quast\u2019 tool will perform this comparison. Search for the Quast tool in the tools panel. Parameters: Contigs/scaffolds file Unicycler_assembly.fasta Use a reference genome? Yes Reference genome reference_genome.fasta Leave all else default and click \u2018execute\u2019 We are mainly interested in one of the outputs - the HTML report Open the report. It may look something like this: Note the Genome fraction (%), # mismatches per 100 kbp, # indels per 100 kbp and # contigs information. We seem to have good coverage and not too many contigs, but our error rate is quite high. In this case we were able to use a reference genome to assess assembly quality, but this is not always the case. When our sample organism is unknown, we need another method to assess assembly quality. BUSCO analysis is one way to do this. BUSCO \u00b6 BUSCO analysis uses the presence, absence, or fragmentation of key genes in an assembly to determine is quality. BUSCO genes are specifically selected for each taxonomic clade, and represent a group of genes which each organism in the clade is expected to possess. At higher clades, \u2018housekeeping genes\u2019 are the only members, while at more refined taxa such as order or family, lineage-specific genes can also be used. We will use BUSCO to assess our Unicycler draft assembly. While sometimes we will not have a reference genome for comparison, we may be able to find out roughly where it sits in the tree of life using certain methods. In this tutorial, we will suspect that our organism is within the \u2018Bacillales\u2019 order . Search for the Quast tool in the tools panel. Parameters: Sequences to analyse Unicycler_assembly.fasta Lineage Bacillales Leave all else default and execute the program. After the program has run, look at the \u2018short summary\u2019 output. It may look something like this: The 'full table' is also useful. It gives a detailed list of the genes we are searching for, and information about whether they would missing, fragmented, or complete in our assembly. It seems that most expected genes are missing or fragmented in our assembly. It is likely that the frequent errors in our draft assembly are causing this result. We should be able improve our assembly with the Illumina reads available and correct some of these errors. This process involves two steps. We will first align the Illumina reads to our draft assembly, then supply the mapping information to Pilon which will use this alignment information to error-correct our assembly. Section Questions \u00b6 How does Quast inform on assembly quality? Answer (click to reveal) Quast shows summary information about the assembly contigs. If a reference genome is given, it informs the genome fraction (how much of the reference is covered by the assembly), if any genomic regions appear duplicated, and error information including the rate of mismatches and indels. How does BUSCO inform on assembly quality? Answer (click to reveal) BUSCO does not use a reference genome to compare. It attempts to locate key genes which should be present in the assembly, and reports whether it could/could not find those genes. If a key gene is found, it reports whether the gene was fragmented (errors) or complete. Conclusion \u00b6 Today we have learned one workflow for performing hybrid de novo genome assembly. The combination of long- and short-read technology is clearly powerful, represented by our ability to create a good assembly with only approximately 20x coverage (87Mb) of Nanopore, and 25x coverage of Illumina reads (100Mb). To further improve our assembly, extra read data may provide most benefit. For bacterial genomes, we often prefer 100x or greater coverage, with which we may achieve a high per-base accuracy assembly with the correct number of contigs (number of replicons in the genome). The development of new purpose-built tools for hybrid de novo assembly like Unicycler have improved the quality of assemblies we can produce. These tools are of great importance and while they already produce great results, they will continue to improve over time. Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials. Pilon: https://github.com/broadinstitute/pilon/wiki/Methods-of-Operation Unicycler: https://github.com/rrwick/Unicycler Quast: https://academic.oup.com/bioinformatics/article/29/8/1072/228832 BUSCO analysis: https://academic.oup.com/bioinformatics/article/31/19/3210/211866","title":"Nanopore assembly 1hr"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#hybrid-genome-assembly-nanopore-and-illumina-1hr","text":"Anticipated workshop duration is 1 hour . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Hybrid genome assembly - nanopore and illumina (1hr)"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#overview","text":"","title":"Overview"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. How do long- and short-read assembly methods differ?","title":"Skill level"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#description","text":"Assemble a genome! Learn how to create and assess genome assemblies using the powerful combination of nanopore and illumina reads This tutorial explores how long and short read data can be combined to produce a high-quality \u2018finished\u2019 bacterial genome sequence. Termed \u2018hybrid assembly\u2019, we will use read data produced from two different sequencing platforms, Illumina (short read) and Oxford Nanopore Technologies (long read), to reconstruct a bacterial genome sequence. In this tutorial we will perform \u2018 de novo assembly\u2019. De novo assembly is the process of assembling a genome from scratch using only the sequenced reads as input - no reference genome is used. This approach is common practise when working with microorganisms, and has seen increasing use for eukaryotes (including humans) in recent times. Using short read data (Illumina) alone for de novo assembly will produce a complete genome, but often in many pieces (commonly called a \u2018draft genome\u2019). For the genome to be assembled into a single chromosome (plus a sequence for each plasmid), reads would need to be longer than the longest repeated element on the genome (usually ~7,000 base pairs, Note: Illumina reads are 350 base maximum). Draft bacterial genome sequences are cheap to produce (less than AUD$60) and useful (>300,000 draft Salmonella enterica genome sequences published at NCBI https://www.ncbi.nlm.nih.gov/pathogens/organisms/ ), but sometimes you need a high-quality \u2018finished\u2019 bacterial genome sequence. There are <1,000 are \u2018finished\u2019 or \u2018closed\u2019 Salmonella enterica genome sequences. In these cases, long reads can be used together with short reads to produce a high-quality assembly. Nanopore long reads (commonly >40,000 bases) can fully span repeats, and reveal how all the genome fragments should be arranged. Long reads currently have higher error rate than short reads, so the combination of technologies is particularly powerful. Long reads provide information on the genome structure, and short reads provide high base-level accuracy. Combining read data from the long and short read sequencing platforms allows the production of a complete genome sequence with very few sequence errors, but the cost of the read data is about AUD$ 1,000 to produce the sequence. Understanably, we usually produce a draft genome sequence with very few sequence errors using the Illumina sequencing platform. Nanopore sequencing technology is rapidly improving, expect the cost difference to reduce!! Data: Nanopore reads, Illlumina reads, bacterial organism ( Bacillus subtilis ) reference genome Tools: Unicycler, Quast, BUSCO Pipeline: FastQC, MultiQC, NanoPlot, Unicycler, Quast, BUSCO","title":"Description"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#learning-objectives","text":"At the end of this introductory workshop, you will: Understand how Nanopore and Illumina reads can be used together to produce a high quality assembly Be familiar with genome assembly and polishing programs Learn how to assess the quality of a genome assembly, regardless of whether a reference genome is present or absent Be able to assemble an unknown, previously undocumented genome to high-quality using Nanopore and Illumina reads.","title":"Learning Objectives"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#requirements-and-preparation","text":"Attendees are required to bring their own laptop computers. All data and tools are available on usegalaxy.org.au. You will need a computer to connect to and use their platform. Before the tutorial, navigate to https://usegalaxy.org.au/ and use your email to create an account. Click \u201cLogin or register\u201d in the top navigation bar of galaxy to do this.","title":"Requirements and preparation"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#preparing-your-laptop-prior-to-starting-this-workshop","text":"No additional software needs to be installed for this workshop.","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#required-data","text":"No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#author-information","text":"Written by: Grace Hall Melbourne Bioinformatics, The University of Melbourne Created/Reviewed: March 2020","title":"Author Information"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#background","text":"","title":"Background"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#how-do-we-produce-the-genomic-dna-for-a-bacterial-isolate","text":"Traditional in vitro culture techniques are important. Take a sample (e.g. a swab specimen from an infected sore) and streak a \u2018loopful\u2019 on to solid growth medium that suppoprts the growth of the bacteria. Technology from the time of Louis Pasteur! Mixtures of bacterial types can be sequenced e.g. prepare genomic DNA from environmental samples containing bacteria - water, soil, faecal samples etc. (Whole Metagenome Sequencing) One colony contains 10 7 \u2013 10 8 cells. The genomic DNA extracted from one colony is enough for Illumina sequencing. Larger amounts of genomic DNA are required for Nanopore sequencing.","title":"How do we produce the genomic DNA for a bacterial isolate?"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#shotgun-sequencing-illumina-sequencing-library","text":"Genomic DNA is prepared for sequencing by fragmenting/shearing: multiple copies of Chromosome + plasmid \u2192 ~500 bp fragments Note: Nanopore sequencing - there is usually no need to shear the genomic DNA specialist methods are used to minimise shearing during DNA preparation . For Nanopore sequencing the longer the DNA fragments the better!","title":"Shotgun sequencing - Illumina Sequencing Library"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#section-1-read-set-summaries-and-qc","text":"In this section we will import and perform quality control (QC) on our data. Today we will use 4 pieces of data - 2 short read sets, 1 long read set, and a reference genome to compare our assembly with.","title":"Section 1: Read set summaries and QC"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#getting-the-data","text":"Make sure you have an instance of Galaxy ready to go. Navigate to the Galaxy Australia server and sign in if you have an account. Copy an existing history The data you will need is available in an existing Galaxy history: https://usegalaxy.org.au/u/graceh1024/h/hybrid-de-novo-assembly---1hr Import the history \u2018+\u2019 icon at the top right of the page. 3. Look at the history you imported * There are 4 files - Nanopore reads, two sets of Illumina reads, and a reference genome for the organism we will assemble. * Our Illumina data are paired-end reads. Two separate files will be present, with only \u20181\u2019 and \u20182\u2019 being different in their filenames. * We will use the reference_genome.fasta to assess the quality of our assembly","title":"Getting the data"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#read-set-summaries","text":"Often, it is prudent to first assess the quality of our read sets. For the short reads, we are concerned with base quality, sequence duplication, and presence of adapter sequences. For nanopore, we want to know about the length and quality distribution of reads, as these may both be highly variable. FastQC creates summary reports for short read data. We will use this tool twice - once for each Illumina read set. We can then use a tool called MultiQC to combine these reports for easy viewing. For Nanopore data, NanoPlot is a great option. It creates plots which aim to summarise the length and quality distribution of long read sets. Depending on these summaries, we may choose to perform a QC step to remove any poor quality reads before proceeding. Run FastQC on each short read set * Find FastQC in the tools panel. It is listed as \u2018 FastQC Read Quality reports\u2019 * Parameters: * Short read data from your current history: illumina_reads_1.fastq * Leave all else default and execute the program. * Run FastQC again as above, but change Short read data from your current history: to \u2018illumina_reads_2.fastq\u2019. * Rename the FastQC RawData output for illumina_reads_1.fastq to \u2018FastQC reads 1 - RawData\u2019 * Rename the FastQC RawData output for illumina_reads_2.fastq to \u2018FastQC reads 2 - RawData\u2019 FastQC produces two outputs - \u2018RawData\u2019, and \u2018Webpage\u2019. Typically, the webpage is for human viewing, and the RawData can be given to other programs, such as MultiQC. At this stage, we have two FastQC outputs - one for each short read set. We will now combine them into a single output using MultiQC for easier interpretation. Run MultiQC * Find MultiQC in the tools panel. It is listed as \u2018 MultiQC aggregate results from bioinformatics analyses into a single report\u2019 * Parameters: * Which tool was used generate logs? FastQC * FastQC output * Type of FastQC output? RawData * FastQC ouptut FastQC reads 1 - RawData * click \u2018+ Insert FastQC output\u2019 * FastQC output * Type of FastQC output? RawData * FastQC ouptut FastQC reads 2 - RawData * Leave all else default and execute the program. MultiQC also produces two outputs - \u2018Stats\u2019 and \u2018Webpage\u2019. Inspect the Webpage output by clicking the eye icon on this history item. MultiQC produces a number of plots, but we are only interested in two today: the \u2018Sequence Counts\u2019 and \u2018Sequence Quality Histograms\u2019 plots. The \u2018Sequence Count\u2019 displays multiple pieces of information. Firstly, we can see that both read sets have the same total number of reads (sanity check - the number should be identical otherwise some reads don\u2019t have a mate!). Additionally, \u2018Duplicate Reads\u2019 are a fraction of the \u2018Unique Reads\u2019. High levels of sequence duplication can be caused by many factors, but since this is whole genome sequencing (WGS) data, we expect most reads to be unique. Scrolling down, the \u2018Sequence Quality Histograms\u2019 plot informs us that read 1 for each mate pair is generally lower quality than read 2. This said, they are both high quality and will be fine for our purpose. Our Illumina reads seem to be reasonable quality. We will now inspect the Nanopore reads. Run NanoPlot * Find MultiQC in the tools panel. It is listed as \u2018 NanoPlot Plotting suite for Oxford Nanopore sequencing data and alignments\u2019 * Parameters: * Type of the file(s) to work on * files nanopore_reads.fastq * Leave all else default and execute the program. NanoPlot produces 5 outputs, but we are only interested in the \u2018HTML report\u2019 output. View this file by clicking the eye icon on this history item. The NanoPlot HTML report includes a table, followed by a number of plots. The table provides a summary of the read set. The main statistics we will look at are Median read length , Median read quality , and Number of reads . Our median read length (3261 bp) is short for Nanopore data, but the median quality is good (11.7), equating to a per base accuracy of approximately 93%. Below the summary table, we see plots relating to read length and quality distributions. Below is the log transformed read length histogram: The vast majority of our reads sit between 1 - 10 kbp in length. Looking at this histogram closely, we see that the histogram is abruptly cut off at 1000 bp - this read set has previously been filtered to remove reads less than 1000 base pairs, resulting in this effect! Our read sets appear to be good enough to proceed without performing further QC. Next, we will create an assembly from these reads using Unicycler.","title":"Read set summaries"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#section-2-hybrid-de-novo-assembly","text":"In this section you will use a tool called \u2018Unicycler\u2019 to create a draft genome assembly from Illumina and Nanopore reads.","title":"Section 2: Hybrid de-novo assembly"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#unicycler","text":"Unicycler performs assembly using multiple steps. Illumina reads are used first to create an assembly graph using a program called SPAdes , then Nanopore reads are used to disentangle problems in the graph. The Nanopore reads serve to bridge Illumina contigs, and to reveal how the contigs are arranged sequentially in the genome. After the assembly is created, a program called Pilon uses the Illumina reads to perform a final round of error-correction before the final assembly is output. For more information on Unicycler, see this link: https://github.com/rrwick/Unicycler Run Unicycler Find Unicycler in the tools panel. It is listed as \u2018Create assemblies with Unicycler\u2019 Run Unicycler using the Nanopore and Illumina read sets. Parameters: Paired or Single end data? - Paired Select first set of reads - illumina_reads_1.fastq Select second set of reads - illumina_reads_2.fastq Select long reads - nanopore_reads.fastq (if nanopore_reads.fastq does not appear in the dropdown, its datatype needs to be changed - click then pencil icon next to nanopore_reads.fastq in the history panel -> \u2018Datatypes\u2019 tab -> \u2018New Type\u2019 - fastqsanger) SPAdes options Number of k-mer steps to use in SPAdes assembly 5 Rotation options Do not rotate completed replicons to start at a standard gene. \u2018Yes\u2019 Pilon options Do not use Pilon to polish the final assembly. \u2018Yes\u2019 Leave all else default and execute the program. Rename the \u2018Final Assembly\u2019 output of Unicycler to \u2018Unicycler_assembly.fasta\u2019 Unicycler will output two files - a Final Assembly, and a Final Assembly Graph. We are interested in the Final Assembly. For the sake of time, we have disabled some quality settings in Unicycler. We reduced the number of k-mer steps from 10 to 5 in SPAdes assembly (overall worse assembly), have disabled assembly polishing (improves base-level accuracy), and have chosen not to set the start of each replicon to a standard position. Given more time, we would leave these default and not disable any of these features. We now have an assembly which we can use for all kinds of downstream analysis. That said, we currently have no idea of the overall quality of this assembly. Additionally, we disabled features which improve the assembly quality due to time constraints, so this is particularly important in our case. Many factors may impact assembly quality, including the following: * Read depth / amount of sequence data * Read quality * The repetitiveness of the sequenced genome * Its ploidy (number of copies of each chromosome - humans are diploid: 2 copies) We need to therefore assess the quality of the assembly we have created. We will use two tools - Quast , and BUSCO to do this.","title":"Unicycler"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#section-questions","text":"Why did we select \u2018Paired\u2019 for our Illumina reads in the Unicycler tool? Answer (click to reveal) Our short read set was 'paired-end'. Short read technology can only sequence a few hundred base-pairs in a single read. To provide better structural information, paired-end sequencing was created, where longer fragments (fixed length) are used. A few hundred bp is sequenced at both ends of the fragment, leaving the middle section unsequenced. The reads produced (the mate-pair) from a single fragment are separated by a fixed length, so we know they are nearby in the genome. Does Unicycler begin by using the Long or Short reads? Answer (click to reveal) Unicycler uses short reads first. It creates an assembly graph from short reads, then uses the long reads to provide better structural information of the genome. How does Unicycler use long reads to improve its assembly graph? Answer (click to reveal) The assembly graph produced by short reads has tangled regions. When we don't know how sections of the genome are arranged, tangled regions appear in the graph. Unicycler uses Nanopore reads which overlap these tangled regions to resolve the proper structure of the genome.","title":"Section Questions"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#section-3-assessing-assembly-quality","text":"Once we have created the assembly, we need to assess its quality. In this section we will use Quast and BUSCO to perform this assessment. We will also perform BUSCO analysis on the supplied reference genome itself, to record a baseline for our theoretical best BUSCO report.","title":"Section 3: Assessing assembly quality"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#quast","text":"A reference genome for the organism we sequenced has been supplied - listed as reference_genome.fasta in the history. This reference genome was assembled using a much higher sequencing depth, and can be used as \u2018ground truth\u2019 to assess our assembly against. The \u2018Quast\u2019 tool will perform this comparison. Search for the Quast tool in the tools panel. Parameters: Contigs/scaffolds file Unicycler_assembly.fasta Use a reference genome? Yes Reference genome reference_genome.fasta Leave all else default and click \u2018execute\u2019 We are mainly interested in one of the outputs - the HTML report Open the report. It may look something like this: Note the Genome fraction (%), # mismatches per 100 kbp, # indels per 100 kbp and # contigs information. We seem to have good coverage and not too many contigs, but our error rate is quite high. In this case we were able to use a reference genome to assess assembly quality, but this is not always the case. When our sample organism is unknown, we need another method to assess assembly quality. BUSCO analysis is one way to do this.","title":"Quast"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#busco","text":"BUSCO analysis uses the presence, absence, or fragmentation of key genes in an assembly to determine is quality. BUSCO genes are specifically selected for each taxonomic clade, and represent a group of genes which each organism in the clade is expected to possess. At higher clades, \u2018housekeeping genes\u2019 are the only members, while at more refined taxa such as order or family, lineage-specific genes can also be used. We will use BUSCO to assess our Unicycler draft assembly. While sometimes we will not have a reference genome for comparison, we may be able to find out roughly where it sits in the tree of life using certain methods. In this tutorial, we will suspect that our organism is within the \u2018Bacillales\u2019 order . Search for the Quast tool in the tools panel. Parameters: Sequences to analyse Unicycler_assembly.fasta Lineage Bacillales Leave all else default and execute the program. After the program has run, look at the \u2018short summary\u2019 output. It may look something like this: The 'full table' is also useful. It gives a detailed list of the genes we are searching for, and information about whether they would missing, fragmented, or complete in our assembly. It seems that most expected genes are missing or fragmented in our assembly. It is likely that the frequent errors in our draft assembly are causing this result. We should be able improve our assembly with the Illumina reads available and correct some of these errors. This process involves two steps. We will first align the Illumina reads to our draft assembly, then supply the mapping information to Pilon which will use this alignment information to error-correct our assembly.","title":"BUSCO"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#section-questions_1","text":"How does Quast inform on assembly quality? Answer (click to reveal) Quast shows summary information about the assembly contigs. If a reference genome is given, it informs the genome fraction (how much of the reference is covered by the assembly), if any genomic regions appear duplicated, and error information including the rate of mismatches and indels. How does BUSCO inform on assembly quality? Answer (click to reveal) BUSCO does not use a reference genome to compare. It attempts to locate key genes which should be present in the assembly, and reports whether it could/could not find those genes. If a key gene is found, it reports whether the gene was fragmented (errors) or complete.","title":"Section Questions"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#conclusion","text":"Today we have learned one workflow for performing hybrid de novo genome assembly. The combination of long- and short-read technology is clearly powerful, represented by our ability to create a good assembly with only approximately 20x coverage (87Mb) of Nanopore, and 25x coverage of Illumina reads (100Mb). To further improve our assembly, extra read data may provide most benefit. For bacterial genomes, we often prefer 100x or greater coverage, with which we may achieve a high per-base accuracy assembly with the correct number of contigs (number of replicons in the genome). The development of new purpose-built tools for hybrid de novo assembly like Unicycler have improved the quality of assemblies we can produce. These tools are of great importance and while they already produce great results, they will continue to improve over time.","title":"Conclusion"},{"location":"tutorials/hybrid_assembly/nanopore_assembly_1hr/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials. Pilon: https://github.com/broadinstitute/pilon/wiki/Methods-of-Operation Unicycler: https://github.com/rrwick/Unicycler Quast: https://academic.oup.com/bioinformatics/article/29/8/1072/228832 BUSCO analysis: https://academic.oup.com/bioinformatics/article/31/19/3210/211866","title":"Additional reading"},{"location":"tutorials/intro_R_biologists/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/intro_R_biologists/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/intro_R_biologists/formatting_template/","text":"Formats to use \u00b6 Text formatting \u00b6 Bold Italics Bold Italics Headings (This is 2 nd level) \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Code Blocks and inline code \u00b6 They can be added like this. Many different languages are supported. Blocks \u00b6 1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf Inline code \u00b6 Code can also be shown as an inline snippet like this import tensorflow as tf . Lists \u00b6 If you need to add a list: Unordered Lists \u00b6 Some information Some more information Ordered Lists \u00b6 Some point Another point Subpoint Sub-subpoint Images \u00b6 How to add an image: Tables \u00b6 Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit Questions and Answers \u00b6 It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: Line Breaks \u00b6 To create a line break (\\<br>), end a line with two or more spaces, and then type return. Links \u00b6 Please see the link . Blockquotes \u00b6 This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote. Text including commands to type \u00b6 Type ls and press ENTER . When referring to a filename \u00b6 canu.contigs.fasta contains the assembled sequences. Showing that a button needs clicking \u00b6 Click Start Highlighting text \u00b6 This text is highlighted. Equations \u00b6 Equations can be added as a block or inline. Block equations \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline equations \u00b6 This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} To do \u00b6 Pdf printing Survey Slides?","title":"Formats to use"},{"location":"tutorials/intro_R_biologists/formatting_template/#formats-to-use","text":"","title":"Formats to use"},{"location":"tutorials/intro_R_biologists/formatting_template/#text-formatting","text":"Bold Italics Bold Italics","title":"Text formatting"},{"location":"tutorials/intro_R_biologists/formatting_template/#headings-this-is-2nd-level","text":"","title":"Headings (This is 2nd level)"},{"location":"tutorials/intro_R_biologists/formatting_template/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"tutorials/intro_R_biologists/formatting_template/#the-4th-level","text":"","title":"The 4th level"},{"location":"tutorials/intro_R_biologists/formatting_template/#the-5th-level","text":"","title":"The 5th level"},{"location":"tutorials/intro_R_biologists/formatting_template/#the-6th-level","text":"","title":"The 6th level"},{"location":"tutorials/intro_R_biologists/formatting_template/#code-blocks-and-inline-code","text":"They can be added like this. Many different languages are supported.","title":"Code Blocks and inline code"},{"location":"tutorials/intro_R_biologists/formatting_template/#blocks","text":"1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf","title":"Blocks"},{"location":"tutorials/intro_R_biologists/formatting_template/#inline-code","text":"Code can also be shown as an inline snippet like this import tensorflow as tf .","title":"Inline code"},{"location":"tutorials/intro_R_biologists/formatting_template/#lists","text":"If you need to add a list:","title":"Lists"},{"location":"tutorials/intro_R_biologists/formatting_template/#unordered-lists","text":"Some information Some more information","title":"Unordered Lists"},{"location":"tutorials/intro_R_biologists/formatting_template/#ordered-lists","text":"Some point Another point Subpoint Sub-subpoint","title":"Ordered Lists"},{"location":"tutorials/intro_R_biologists/formatting_template/#images","text":"How to add an image:","title":"Images"},{"location":"tutorials/intro_R_biologists/formatting_template/#tables","text":"Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit","title":"Tables"},{"location":"tutorials/intro_R_biologists/formatting_template/#questions-and-answers","text":"It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this:","title":"Questions and Answers"},{"location":"tutorials/intro_R_biologists/formatting_template/#line-breaks","text":"To create a line break (\\<br>), end a line with two or more spaces, and then type return.","title":"Line Breaks"},{"location":"tutorials/intro_R_biologists/formatting_template/#links","text":"Please see the link .","title":"Links"},{"location":"tutorials/intro_R_biologists/formatting_template/#blockquotes","text":"This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote.","title":"Blockquotes"},{"location":"tutorials/intro_R_biologists/formatting_template/#text-including-commands-to-type","text":"Type ls and press ENTER .","title":"Text including commands to type"},{"location":"tutorials/intro_R_biologists/formatting_template/#when-referring-to-a-filename","text":"canu.contigs.fasta contains the assembled sequences.","title":"When referring to a filename"},{"location":"tutorials/intro_R_biologists/formatting_template/#showing-that-a-button-needs-clicking","text":"Click Start","title":"Showing that a button needs clicking"},{"location":"tutorials/intro_R_biologists/formatting_template/#highlighting-text","text":"This text is highlighted.","title":"Highlighting text"},{"location":"tutorials/intro_R_biologists/formatting_template/#equations","text":"Equations can be added as a block or inline.","title":"Equations"},{"location":"tutorials/intro_R_biologists/formatting_template/#block-equations","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Block equations"},{"location":"tutorials/intro_R_biologists/formatting_template/#inline-equations","text":"This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline equations"},{"location":"tutorials/intro_R_biologists/formatting_template/#to-do","text":"Pdf printing Survey Slides?","title":"To do"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/","text":"Introduction to R for Biologists \u00b6 Anticipated workshop duration when delivered to a group of participants is 3 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is is aimed at bench biologists with no coding/programming skills. Description \u00b6 Ever wanted to visualise your own data? You can with R! This introduction to R and RStudio will provide beginners with experience with loading, manipulating and visualising biological data using the tidyverse collection of R packages. The example data used is publicly available RNA-seq data, therefore attendees will gain experience in the structure and appearance of RNA-seq data. This workshop has been developed collaboratively by training specialists at Peter MacCallum Cancer Centre and Melbourne Bioinformatics. Tools: R , RStudio , tidyverse , ggplot2 Learning Objectives \u00b6 At the end of this introductory workshop, you will be able to: Load tabular data into R. Apply tidyverse functions to manipulate data in R. Produce simple plots such boxplots using ggplot. Understand and apply data faceting in ggplot. Modify the aesthetics of a ggplot plot. Requirements and preparation \u00b6 Attendees are required to bring their own laptop computers to face to face workshops. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. We recommend you watch this video to familiarise yourself with R, Rstudio and Tidyverse. Preparing your laptop prior to starting this workshop \u00b6 Download and install R (free) on your computer. Download and install RStudio (free) on your computer. Detailed instructions for installing R and RStudio are available here . Required Software \u00b6 R Rstudio Required Data \u00b6 Data file can be downloaded from here Hyper links for downloading the required data are also included in the workshop PDF, see \u2018Workshop material\u2019 below. Author Information \u00b6 Written by: Maria Doyle, Jessica Chung and Victoria Perreau This workshop has been developed collaboratively by training specialists at Peter MacCallum Cancer Centre and Melbourne Bioinformatics Created: October 2019 Last Reviewed: October 2020 Workshop Material \u00b6 Detailed workshop material, including links for data download, are available as both: HTML version PDF version Additional reading \u00b6 A short intro to R and tidyverse Topp 50 ggplot visualisations R for Data Science","title":"Introduction to R for Biologists"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#introduction-to-r-for-biologists","text":"Anticipated workshop duration when delivered to a group of participants is 3 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Introduction to R for Biologists"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#overview","text":"","title":"Overview"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#skill-level","text":"Beginner Intermediate Advanced This workshop is is aimed at bench biologists with no coding/programming skills.","title":"Skill level"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#description","text":"Ever wanted to visualise your own data? You can with R! This introduction to R and RStudio will provide beginners with experience with loading, manipulating and visualising biological data using the tidyverse collection of R packages. The example data used is publicly available RNA-seq data, therefore attendees will gain experience in the structure and appearance of RNA-seq data. This workshop has been developed collaboratively by training specialists at Peter MacCallum Cancer Centre and Melbourne Bioinformatics. Tools: R , RStudio , tidyverse , ggplot2","title":"Description"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#learning-objectives","text":"At the end of this introductory workshop, you will be able to: Load tabular data into R. Apply tidyverse functions to manipulate data in R. Produce simple plots such boxplots using ggplot. Understand and apply data faceting in ggplot. Modify the aesthetics of a ggplot plot.","title":"Learning Objectives"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#requirements-and-preparation","text":"Attendees are required to bring their own laptop computers to face to face workshops. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. We recommend you watch this video to familiarise yourself with R, Rstudio and Tidyverse.","title":"Requirements and preparation"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Download and install R (free) on your computer. Download and install RStudio (free) on your computer. Detailed instructions for installing R and RStudio are available here .","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#required-software","text":"R Rstudio","title":"Required Software"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#required-data","text":"Data file can be downloaded from here Hyper links for downloading the required data are also included in the workshop PDF, see \u2018Workshop material\u2019 below.","title":"Required Data"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#author-information","text":"Written by: Maria Doyle, Jessica Chung and Victoria Perreau This workshop has been developed collaboratively by training specialists at Peter MacCallum Cancer Centre and Melbourne Bioinformatics Created: October 2019 Last Reviewed: October 2020","title":"Author Information"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#workshop-material","text":"Detailed workshop material, including links for data download, are available as both: HTML version PDF version","title":"Workshop Material"},{"location":"tutorials/intro_R_biologists/intro_R_biologists/#additional-reading","text":"A short intro to R and tidyverse Topp 50 ggplot visualisations R for Data Science","title":"Additional reading"},{"location":"tutorials/longread_sv_calling/formatting_template/","text":"Formats to use \u00b6 Text formatting \u00b6 Bold Italics Bold Italics Headings (This is 2 nd level) \u00b6 The 3 rd level \u00b6 The 4 th level \u00b6 The 5 th level \u00b6 The 6 th level \u00b6 Code Blocks and inline code \u00b6 They can be added like this. Many different languages are supported. Blocks \u00b6 1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf Inline code \u00b6 Code can also be shown as an inline snippet like this import tensorflow as tf . Lists \u00b6 If you need to add a list: Unordered Lists \u00b6 Some information Some more information Ordered Lists \u00b6 Some point Another point Subpoint Sub-subpoint Images \u00b6 How to add an image: Tables \u00b6 Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit Questions and Answers \u00b6 It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: Line Breaks \u00b6 To create a line break (\\<br>), end a line with two or more spaces, and then type return. Links \u00b6 Please see the link . Blockquotes \u00b6 This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote. Text including commands to type \u00b6 Type ls and press ENTER . When referring to a filename \u00b6 canu.contigs.fasta contains the assembled sequences. Showing that a button needs clicking \u00b6 Click Start Highlighting text \u00b6 This text is highlighted. Equations \u00b6 Equations can be added as a block or inline. Block equations \u00b6 \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline equations \u00b6 This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} To do \u00b6 Pdf printing Survey Slides?","title":"Formats to use"},{"location":"tutorials/longread_sv_calling/formatting_template/#formats-to-use","text":"","title":"Formats to use"},{"location":"tutorials/longread_sv_calling/formatting_template/#text-formatting","text":"Bold Italics Bold Italics","title":"Text formatting"},{"location":"tutorials/longread_sv_calling/formatting_template/#headings-this-is-2nd-level","text":"","title":"Headings (This is 2nd level)"},{"location":"tutorials/longread_sv_calling/formatting_template/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"tutorials/longread_sv_calling/formatting_template/#the-4th-level","text":"","title":"The 4th level"},{"location":"tutorials/longread_sv_calling/formatting_template/#the-5th-level","text":"","title":"The 5th level"},{"location":"tutorials/longread_sv_calling/formatting_template/#the-6th-level","text":"","title":"The 6th level"},{"location":"tutorials/longread_sv_calling/formatting_template/#code-blocks-and-inline-code","text":"They can be added like this. Many different languages are supported.","title":"Code Blocks and inline code"},{"location":"tutorials/longread_sv_calling/formatting_template/#blocks","text":"1 2 $ ls exp01 file01 muscle.fq or 1 import tensorflow as tf","title":"Blocks"},{"location":"tutorials/longread_sv_calling/formatting_template/#inline-code","text":"Code can also be shown as an inline snippet like this import tensorflow as tf .","title":"Inline code"},{"location":"tutorials/longread_sv_calling/formatting_template/#lists","text":"If you need to add a list:","title":"Lists"},{"location":"tutorials/longread_sv_calling/formatting_template/#unordered-lists","text":"Some information Some more information","title":"Unordered Lists"},{"location":"tutorials/longread_sv_calling/formatting_template/#ordered-lists","text":"Some point Another point Subpoint Sub-subpoint","title":"Ordered Lists"},{"location":"tutorials/longread_sv_calling/formatting_template/#images","text":"How to add an image:","title":"Images"},{"location":"tutorials/longread_sv_calling/formatting_template/#tables","text":"Tables can have text that is left, centred or right aligned. Left Center Right Lorem dolor amet ipsum sit","title":"Tables"},{"location":"tutorials/longread_sv_calling/formatting_template/#questions-and-answers","text":"It is useful to number questions with the section number they appear in so participants can easily refer to the number when asking a question. Indentation is important to display correctly. Question 1.1 What is the full path name of your home directory? Hint Remember your Current Working Directory starts in your home directory. Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX Question 1.1 What is the full path name of your home directory? Answer You can find out the full path name of the current working directory with the pwd command. Your home directory will look something like this:","title":"Questions and Answers"},{"location":"tutorials/longread_sv_calling/formatting_template/#line-breaks","text":"To create a line break (\\<br>), end a line with two or more spaces, and then type return.","title":"Line Breaks"},{"location":"tutorials/longread_sv_calling/formatting_template/#links","text":"Please see the link .","title":"Links"},{"location":"tutorials/longread_sv_calling/formatting_template/#blockquotes","text":"This is a quote. Two spaces are needed at the end of this to make next line of quote appear on a different line. This is the second line of quote.","title":"Blockquotes"},{"location":"tutorials/longread_sv_calling/formatting_template/#text-including-commands-to-type","text":"Type ls and press ENTER .","title":"Text including commands to type"},{"location":"tutorials/longread_sv_calling/formatting_template/#when-referring-to-a-filename","text":"canu.contigs.fasta contains the assembled sequences.","title":"When referring to a filename"},{"location":"tutorials/longread_sv_calling/formatting_template/#showing-that-a-button-needs-clicking","text":"Click Start","title":"Showing that a button needs clicking"},{"location":"tutorials/longread_sv_calling/formatting_template/#highlighting-text","text":"This text is highlighted.","title":"Highlighting text"},{"location":"tutorials/longread_sv_calling/formatting_template/#equations","text":"Equations can be added as a block or inline.","title":"Equations"},{"location":"tutorials/longread_sv_calling/formatting_template/#block-equations","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Block equations"},{"location":"tutorials/longread_sv_calling/formatting_template/#inline-equations","text":"This equation will appear in line with this text: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline equations"},{"location":"tutorials/longread_sv_calling/formatting_template/#to-do","text":"Pdf printing Survey Slides?","title":"To do"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/","text":"Structural variant calling - long read data \u00b6 Anticipated workshop duration when delivered to a group of participants is 4 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. We will be using 1 line awk programs to process text output, but these will be supplied and explained. Description \u00b6 Long reads have turbo-charged structural variant detection - be part of the renaissance! Structural variation has historically been hard to detect. The advent of long reads, and improvements to the quality of reference genomes over time has recently enabled new discoveries in the field. This tutorial uses sniffles to implement a structural variant calling pipeline. Structural variant calling will be performed on a bacterial dataset to benchmark sniffles, then using a human clinical dataset to identify patient disease. We will explore one workflow for structural variant detection, then will visualise and summarise our results using multiple methods. Data: Nanopore reads: bacterial & human (FASTQ), genomic feature annotations (GFF), human reference genome hg38 Pipeline: Read summaries & QC, alignment, SV calling, text processing, visualisation Tools: NanoPlot, Filtlong, minimap2, CalMD, SortSam, sniffles, VCFsort, VCFannotate, awk, Circos, IGV Section 1 covers bacterial SV calling and benchmarking of bioinformatics tools. Section 2 will demonstrate SV calling on a human sample to diagnose a patient condition. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Be able to perform SV calling in model and non-model organisms Be familiar with the current field of SV calling Gain an understanding of why and when SV calling is an appropriate analysis to perform. Required Software \u00b6 No additional software needs to be installed for this workshop. Required Data \u00b6 No additional data needs to be downloaded for this workshop. Author Information \u00b6 Written by: Grace Hall Melbourne Bioinformatics, University of Melbourne Created/Reviewed: March 2021 Background \u00b6 What is Structural Variation? \u00b6 Genetic variation is always relative. In general, we have a reference sequence which we know lots about, and query sequences to compare against this reference. Given we know lots about the reference, the impact of any variation we find in the query sequences can be inferred. The query sequences can originate from a single individual or a group, depending on the biological question. Genetic variants are often separated into two categories: sequence variants , and structural variants . Sequence variants cover small-scale changes which affect a few nucleotides, such as single nucleotide variants (SNVs) or small insertions / deletions (Indels). They are particularly important when they impact coding sequences of genes, as can alter the amino acid sequence of proteins. Structural variants (SVs) are large-scale events (>50 bp) where entire sections of genetic material have changed. An example is a deletion, where an entire section of DNA has been removed. Structural variants have the potential to greatly alter the gene dosage of a cell by duplicating or deleting entire exons or genes at a time. Benefit of Long Reads \u00b6 Structural variation has historically been hard to detect. This is because structural variation often involves repeat elements which are notoriously hard to resolve using short-read sequencing technologies. Either the structural variant itself is a repeat, or it occurs in a repetitive region of the reference genome. In general, long reads have greater mappability. Mappability is the ability to unambiguously align reads to a reference. In the example above, a short read sampled from a tandem repeat cannot be accurately mapped, as it is equally likely it came from 3 different locations. On the other hand, a long read sampled from this region can be uniquely mapped to a single location. The extra repeat found in the isolate could also be detected given such a read. This is highly pertinent when working with the human genome, as more than 60% appears to be repetitive sequence . This repetitive sequence consists of 2 main elements: mobile elements, and repeats. Mobile elements are sections of DNA which copy or move themselves throughout our genome and include retrotransposons (LINE, SINE, LTR, and SVA) and DNA transposons. Repeats are genomic regions which contain the same sequence repeated many times, and consist of Short Tandem Repeats (STRs / microsatellites) which are 1-6 bp tandem repeats, or Variable Number of Tandem Repeats (VNTRs / minisatellites) which are tandem repeats where the repeat length is greater than 7 bp. Applications of Structural Variant Detection \u00b6 The importance of structural variation has become more apparent in recent years. This has been driven by the advent of long-read sequencing technologies, and the availability of better reference genomes (including improvements to human reference genome hg38). Today, SV detection is implicated in many areas of bioinformatics, including the following: Human Disease Cohort studies Identifying new, disease-causing structural variation Assessing an individual\u2019s susceptibility to disease mediated by SVs Cancer genomics Susceptibility of individual to certain cancers Monitoring of tumor progression Agriculture Identifying desirable traits Flowering rate, frost / drought resistance, improved crop yield Genetic modification The Microbial World Understanding the relationship between microbiome SVs and human health Studying phylogeny and evolution of microbes (including horizontal gene transfer) Researching the spread of mobile elements and plasmids which convey virulence or antibiotic resistance genes Structural Variant Types \u00b6 Structural variants are either destructive (involving a change in total genetic material), or non-destructive (total genetic material stays the same). Destructive variants are often particularly interesting, as they alter the gene dosage of a call. This can lead to overexpression of genes, or knock-out variants with zero expression if a gene was removed. Destructive SVs are highly implicated in cancer genomics, as undesirable genes such as tumor-suppressors can be removed, while desirable genes involving growth and proliferation can be expressed in much higher quantity. There are 5 main types of structural variation. Insertions, Deletions, Inversions, Duplications, and Translocations. Specifically, they involve the following: Insertions A foreign section of genetic material is inserted Deletions A section of genetic material is removed Inversions A section of genetic material swaps its orientation Duplications (interspersed or tandem) A section of genetic material is copied, then inserted. Can be separated into: Interspersed duplications, where the insertion site is away from the copied section Tandem duplications, where the insertion site is directly beside the copied section Translocations (intra- or inter-chromosomal) A section of genetic material is cut out, then inserted somewhere else. Can be separated into: Intra-chromosomal translocations, where the insertion site is within the same chromosome Inter-chromosomal translocations, where the insertion site is on another chromosome Variants detected by common SV calling programs \u00b6 Many SV calling programs do not attempt to detect interspersed duplications and intra-chromosomal translocations. This is because most SV callers have been designed for use with human data, and these variants are less relevant to the human genome than other organisms. Regarding interspersed duplications , most RNA transposons (the mechanism of variation) are inactive in the human genome. A few are still active, but an impact will only be seen if the insertion site (insert location of the copied segment) is within an important genomic feature, such as a gene body or enhancer region. Intra-chromosomal translocations are ignored for similar reasons. These are mainly caused by DNA transposons, which are considered inactive in humans. DNA transposons are still active in other eukaryotes (including most plants) and bacteria, so must be considered when working outside human data. TODO if time: Mechanisms \u00b6 SV calling pipeline \u00b6 Today, we will explore one workflow for SV calling. We will first call variants using a benchmarking dataset, where the true SVs are known. We will then swap to a human clinical scenario, and will identify a structural variant causing patient disease. We will use the workflow below to explore structural variation. This workflow has 3 main sections - data exploration, calling variants, then interpreting our findings. Our process will be slightly different depending on whether we are working with the bacterial read set, or the human sample. Our SV calling workflow will consist of 5 key steps: Read QC The length and quality distribution of long-read data is highly variable. We will summarise our read set using NanoPlot , to understand what quality of results to expect during analysis. Filtlong will then be used to remove short reads, or those with patches of low quality. This will reduce erroneous read mapping and will improve our results. Alignment Structural variation is always relative to a reference. Our isolate reads will act as the query, and an appropriate reference genome will be selected to measure structural variation against. We will align our isolate reads using minimap2 to the chosen reference genome, then will pass the output alignment BAM file to our SV caller. SV calling Our SV caller, sniffles , uses the read alignment information to detect structural variation. It will output variant calls in VCF format, which we will then process with awk to create a summary which is easy to read. Feature annotation Variants which span or otherwise intersect with key genomic regions are most likely to have functional impact. Rather than doing this manually for each variant, we can use awk and VCFannotate to automatically label variants with the genomic features they intersect with. Visualisation Visualisation will be performed to understand variation on a genome-wide scale, and to probe single variants for their potential functional impact. Circos plots will be generated for the bacterial read set to view the total variation across the genome, and IGV will be used on the human clinical dataset to infer the functional consequences of variants. Section 1: Bacterial dataset \u00b6 Introduction \u00b6 The first dataset we will use is a synthetically mutated bacterial genome. Structural variants and single base mutations have been added to an E. coli sakai assembly, then Nanopore reads of this mutated genome were simulated using NanoSim . We will use these reads to identify the SVs we added using our SV calling pipeline. The benefit to this approach is that we know the ground truth. SVs were manually added to our E. coli sakai reference genome, and their details were recorded. As we have a list of SVs which were added, we can assess sniffles performance at SV calling by the number of SVs it correctly identifies. In this section we will: Perform read Quality Control (QC) Align reads Perform SV calling Create SV call summaries using awk (text processing) Visualise SV calls with Circos We will employ a number of tools in our pipeline, so let\u2019s get to it! Getting the data \u00b6 To start, we need a set of reads from our synthetic isolate, a reference genome to call SVs against, and a \u2018ground truth\u2019 list of variants which have been added to our isolate. Import the following Galaxy history to get started: https://usegalaxy.org.au/u/graceh1024/h/sniffles-benchmarking-ecoli-sakai Importing a galaxy history How to import Galaxy histories can be accessed via a link, or in the \u2018Shared Data\u2019 tab of the top navigation bar. Once you find a history you want to copy, press the \u2018+\u2019 icon at the top right of the page to import the history The reference genome is \u201cecoli_sakai.fasta\u201d Our reads are \u201cisolate_reads.fastq\u201d The list of added SVs is \u201cisolate_sv_record.tsv\u201d Read QC \u00b6 Before proceeding, we want to understand our read set. Nanopore reads can have highly variable read length and quality distributions from sample to sample - by creating a summary, we can know what to expect from our reads, and whether we should perform QC filtering. Create a summary of the reads First, we want to get a summary of our read set. NanoPlot creates plots to summarise the length distribution and quality of our read set. Tool: NanoPlot Type of the file(s) to work on: fastq files: isolate_reads.fastq NanoPlot creates 5 outputs. Today, we are only interested in the HTML report. This report contains a summary table which will summarise most of the important information, followed by plots displaying the read length and quality distribution of reads. Your report may be similar to the following image: Answer the questions below regarding isolate_reads.fastq: What is the median read length? answer ____ bp What is the median read quality? answer Q__ (phred ___) What is the length and mean quality of the longest single read? answer ____ bp, Q ___ As a significant proportion of the reads are short or low quality, we will perform a filtering step to reduce incorrect SV calls downstream in our analysis. Filter reads Long reads can have a wide range of lengths and qualities. Filtering of long reads is often conducted slightly differently to short reads. Often, reads less than 1000 bp are removed, and reads containing patches of poor quality are discarded. This is an important distinction to short read filtering, as a given long read can have a good overall base quality, while having areas which are very low quality. As our SV caller, sniffles, looks at alignment quality when identifying structural variation, it is important we remove reads with low-quality patches. Tool: filtlong Input FASTQ isolate_reads.fastq Output thresholds Min. Length 1000 Min. window quality 9 rename the output to \u2018isolate_reads_filtered.fastq\u2019 The above will remove any reads which are less than 1000 bp, or those where a section (250bp) of the read has mean quality below Q9. Our readset will now be ready for structural variant calling. Alignment \u00b6 Sniffles requires an alignment file when calling variants. It searches the alignments for split reads, alignments containing patches of high error rate, and soft clipping of reads to identify structural variation. Below we can see how different isolate reads capturing an inversion might be aligned to the reference genome. In the first example, the inversion appears near the middle of the read and is large. In this case, the aligner may split the read and report the alignment in 3 sections. In scenario 2, the inversion is quite small, and the read may not be split. In this case there would be a patch of low sequence identity between the read and reference genome at the inversion location. In the final example, only the very edge of the read has spanned the SV, and as a result the aligner may just report a single alignment with part of the read clipped off. All this information is available to our SV caller after the reads are aligned to our reference genome. To provide this file, we need to map our read set to our reference genome of choice. The reference genome should be as close as possible to the isolate from which our reads originate for best results. In this situation, the original E. coli sakai assembly which we computationally mutated will be our reference genome. Align reads Tool: Map with minimap2 Will you select a reference genome from your history or use a built-in index? Use a genome from history and build index Use the following dataset as the reference sequence ecoli_sakai.fna Single or Paired-end reads single Select fastq dataset isolate_reads_filtered.fastq Select a profile of preset options Oxford Nanopore read to reference mapping\u2026 Set advanced output options Generate CIGAR Yes Rename output to \u201cisolate mapped reads\u201d Calculate MD Tag Sniffles requires alignments to contain the \u2018MD tag\u2019 in our BAM file. This is a condensed representation of the alignment of a read to the reference, and is similar to a CIGAR string. We can use CalMD to add this to each alignment in our BAM file. Tool: CalMD BAM file to recalculate isolate mapped reads Choose the source for the reference genome Use a genome from the history Using reference file ecoli_sakai.fna Rename output to \u201cisolate mapped reads MD\u201d SV calling with sniffles \u00b6 Now we have our alignments (BAM) in the correct format, we can call variants with sniffles. Sniffles generally has good defaults, so we will leave everything as-is for now. Later, we will tweak the settings based on our read set for better results. Call variants Tool: sniffles Input BAM file isolate mapped reads MD Rename the output to \u2018sniffles variant calls\u2019 Have a look at the VCF output of sniffles. It contains header lines providing metadata and definitions, followed by variant call lines. VCF stands for \u2018Variant Call Format\u2019 and is the standard format for recording variant information - both sequence variants , and structural variants. Click the eye icon to view. Sort VCF output Before continuing, we well sort the variant calls so they are in coordinate order. This will help us compare against the truth SV record for our simulated isolates (provided SV records are sorted by coordinate), and in future will allow us to view the variants using a genome browser. Tool: VCFsort Select VCF dataset sniffles output VCF Rename the output to \u2018sniffles variant calls sorted\u2019 Creating a summary (awk) \u00b6 Unfortunately, the VCF file format was not created to store structural variant information, and generally does a poor job. In its current state, it is not very easy to quickly summarise our variant calls, as a lot of the important information is shoved in the \u2018INFO\u2019 field. We would prefer a format which identifies the contig, location, type, and size of each variant call in an easy to read manner. We can create such a summary using text processing - specifically by using the \u2018awk\u2019 tool. For each variant call we will extract the value for the #CHROM, #POS, and #ID fields, and from the #INFO field we will extract the remaining information we need. Awk processes the input file line-by-line. For each line, it follows the following approach: 1 pattern { action } For a given line, if the pattern is matched, the action will be performed. For example, given the following input file: 1 2 3 chr12 gene NTRK2 chr18 CDS BRCA1 chr22 gene P53 And the command: 1 Awk \u2018 / gene / { print $ 0 }\u2019 We will obtain the output: 1 2 chr12 gene NTRK2 chr22 gene P53 How it works: / gene / will match any line which contains \u201cgene\u201d, and the statement { print $ 0 } will print the line. Awk always sets some variables for the line being processed - in particular, the complete line is stored as $0, and $1, $2, $3 etc store the value for field 1, 2, 3 etc respectively. If desired, we can also perform multiple actions when the pattern is matched: 1 pattern { action 1 ; action 2 ; action 3 ; } Awk is a highly versatile tool for text processing, and can perform all the common functionality including conditional if / else statements and loops. From here the awk programs will be supplied, but if you wish to learn more, here is a good place to start: https://zetcode.com/lang/awk/ Process VCF with awk Tool: Text reformatting with awk File to process: sniffles variant calls sorted AWK Program: 1 / SVTYPE =/ { split ( $ 8 , infoArr , \";\" ); print substr ( infoArr [ 9 ], 8 ), $ 3 , $ 1 , substr ( infoArr [ 3 ], 6 ), $ 2 , substr ( infoArr [ 4 ], 5 ), substr ( infoArr [ 11 ], 7 ) } Rename output to \u201csniffles VCF summary\u201d Your output may look something like this: Calculating sniffles Performance Metrics \u00b6 We now have both files we need to measure the performance of sniffles - the variant calls provided by sniffles, and the ground SV truth. Open both files in new tabs and compare them by right-clicking the eye icon then selecting \u2018open link in new tab\u2019. Specifically, note the following: How many real SVs did sniffles identify (true positives) How many did it miss? (false negatives) How many SVs were called by sniffles which were not actually added to the reference genome? (false positives) From this information we can calculate performance metrics for sniffles. The following formulas for accuracy, precision and recall are commonly used when benchmarking bioinformatics software. What was sniffles recall? answer ____ bp While the accuracy and precision of sniffles was good, the recall is low. This is due to a key setting in sniffles which relates to our read set - read support. Tuning sniffles Settings \u00b6 Sniffles has a default setting called \u2018read support\u2019 which requires 10 reads to support a possible SV for it to be accepted as genuine. Reducing this number allows more SVs to be discovered, but may also cause some false positives (SV calls for variants which do not actually exist). Conversely, we can be more strict by increasing this number. The best choice for this setting depends on the biological question you wish to answer, and the amount of read depth your dataset has. Our filtered read set fastq file was 139 mb, so we have approximately 70 mbp worth of long read data. As the genome size of our isolate is approximately 5 mbp, this equates to only around 12x mean read depth for a given location in the reference genome. Read depth is not uniform, so we expect some regions to have less than 12x depth, resulting in some structural variants being missed by sniffles if not enough reads supported the call. As our mean depth is 12x, but the quality of our reads is good, we will reduce the \u2018read support\u2019 setting to 5. Re-run sniffles Run sniffles again by clicking the re-run button on the sniffles VCF output. By clicking the re-run button, all the settings previously used will already be filled. Change the following: Set general options Minimum Support: 5 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles variant calls RS5\u2019 Re-run VCFsort Run VCFsort again by clicking the re-run button. Change the following: Select VCF dataset: \u2018sniffles variant calls RS5\u2019 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles variant calls RS5 sorted\u2019 Re-run awk Run awk again to create a summary by clicking the re-run button. Change the following: File to process: \u2018sniffles variant calls RS5 sorted\u2019 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles VCF summary RS5\u2019 Our new variant calls should be an improvement on the original settings. You may see something similar to the following: We have increased our recall by lowering our read support threshold to 5, rather than 10. While being less conservative in this manner will increase recall, it also may result in greater false positives (sniffles calling variants that don\u2019t exist). Whether we maximise precision or recall depends on the task at hand - in this case, we want to discover all the variants and prefer high recall, and should therefore treat our variant calls with more scepticism. Visualising SV calls \u00b6 Visualisation and interpretation is an important part of any analysis. Now we have our SV calls, we can view them on a genome-wide scale using a type of visualisation called a circos plot. Circos plots are often featured on the cover of academic journals as they can communicate a large amount of genomic information at a glance. We will now make a circos plot which displays our variant calls. Similar to genome browsers, circos plots are built from data tracks. When dealing with genomic data, the outer coordinate system (called the ideogram ) is usually the chromosomes of a genome, and the tracks pin data to their genomic positions. For this workshop, we will use a galaxy workflow to create our circos plot, but if you would like to learn how to create these plots yourself, see the following tutorial: https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html Create Circos plot We will be using a workflow to create circos plots for us. This will process our sniffles VCF summary and SV truth report, and produce a plot. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=6588e175004aba38 Optional: importing a galaxy workflow importing rather than running Galaxy workflows can be directly run, or can be imported as a workflow. The benefit to importing a workflow is that you can see all the tools that are being run, and can customise the workflow to suit your needs. Like shared histories, workflows can also be found in the \u2018Shared Data\u2019 tab of the top navigation bar. Once you find a workflow you want to import, press the \u2018+\u2019 icon at the top right of the page to import the workflow. The circos plot workflow can be imported using the following link: https://usegalaxy.org.au/u/graceh1024/w/long-read-sv-calling---circos-plots Set the following: sniffles VCF summary: sniffles VCF summary RS4 Reference Genome: ecoli_sakai.fna SV truth record: isolate_sv_record.tsv Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. Your output might be similar to the following figure: The circos plot has been formatted so both the variant calls (sniffles) and the true SVs are displayed. The ideogram (outer coordinate system) is displaying the bacterial chromosome and two plasmids. One plasmid is reasonable size, while the other is tiny. The outer tile track displays sniffles calls, and the inner track is the true added variants. Link tracks (lines connecting regions of the plot) have been added which show the translocation breakends called by sniffles in yellow, and any undetected breakends in red. From this plot, we can quickly see that sniffles detected most of the variants, but a few were still missed. Section 2: Human clinical dataset \u00b6 Introduction \u00b6 Pathogenic structural variation has become more thoroughly understood in recent years, partly driven by the advent of long-read sequencing technologies. For the remainder of this workshop, we will use a dataset which emulates a patient case. In this example, long-read sequence data was able to identify a causal SV, where short-read sequencing had previously reported a negative result. The dataset was simulated according to the findings of Merker et al (2018) and can be found at https://dx.doi.org/10.1038%2Fgim.2017.86 . Patient Case \u00b6 The patient is a male who had numerous recurring tumorous growths over their development. At age 7, an atrial myxoma of the heart was discovered and removed, followed by a Sertoli-Leydig cell tumor at age 10, a pituitary tumor at 13, more growths on the heart at 16 and 18 which were surgically removed. After the heart surgery at 18 years old, he suffered from a cardiac arrest which he eventually recovered from. At 18, the possibility of a Carney complex was suggested, but short read sequencing and analysis of the PRKAR1A gene returned negative for pathogenic variation. The patient continued to develop tumors over the following years, prompting another round of sequencing - this time, whole genome sequencing (WGS) using long reads. 26.7 Gb of reads were produced using the PacBio Sequel system, equating to an average read depth of 8.6x. The following dataset is simulated reads from a section of chr17 which emulate the patient case. Getting the data \u00b6 To start, we need reads from our section of chr17 (pos 66,000,000 - 69,000,000) for variant calling against hg38, and a file listing genomic features (GFF) for automated annotation later on. Import the following Galaxy history to get started: https://usegalaxy.org.au/u/graceh1024/h/carney-complex---chr17-reads SV Calling using Workflow \u00b6 Rather than manually running each tool again using our chr17 reads and human reference genome hg38 as reference, we will use a workflow to do the analysis for us. hg38 is a built-in genome in galaxy, so we do not need to provide it ourselves. The workflow will perform the following: Read QC (Filtlong) Produce a summary report of filtered reads (NanoPlot) Align reads to hg38 (minimap2) Calculate the MD tag (CalMD) and sort the BAM file by coordinate (SortSam) Call variants (sniffles) Sort the variant calls (VCFsort) Create a summary of the variant calls (awk) As we are aligning reads to hg38, the alignment step will take longer than for the bacterial dataset. The human genome is approximately 640x larger than E. coli sakai , so this is understandable. Reads may take 15 minutes to map during the workflow. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=d69a765cfc82a399 Set the following: Long reads: chr17_reads.fastq CalMD Using reference genome: \u2018hg38\u2019 Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. This workflow produces the key outputs we need. The NanoPlot HTML report summarises our reads after filtering, which we can view to determine the quality of our read set. The sorted alignments contain the alignment information, which we can download to view using a genome browser. variant calls sorted is the sorted VCF file produced by sniffles, and variant calls summary is our simplified awk summary of the variant calls. Use the eye icon to view these outputs for your own interest. All going well, your final variant calls summary will look similar to the following: As our reads are from a 3 mbp segment of chr17, the variants are all located on chr17, between position 6,600,000 and 6,900,000. 19 structural variants have been detected, which is a reasonable number for this segment given that multiple thousand variants are generally detected between any individual and hg38. One of these variants is causing patient disease, and we will identify the culprit using IGV. Visualising Alignments with IGV \u00b6 We will use IGV to inspect the variant calls, and determine which may be causing disease. In particular, we are looking for structural variation which spans or interrupts gene coding sequences, as these are most likely to cause a disease phenotype. We wish to view the alignments and variant calls, so will download the sorted alignment files (BAM and BAI) and the variant calls (VCF). We can then upload this data to the IGV webapp to visualise. Download Alignments and Variant Calls Data Click the save icon in the \u2018sorted alignments\u2019 output, and download both the dataset (BAM) and the bam_index (BAI) files. The BAM contains read alignment information, and the BAI contains an index which allows IGV to load this data. Do the same for the \u2018variant calls sorted\u2019 VCF output. This time, simply clicking the save icon will download the VCF as there is only 1 piece of data. Open IGV and set to hg38 Navigate to https://igv.org/app/ to open IGV. Genome browsers use a reference genome as a coordinate system, and anchor data to these coordinates. Datasets are loaded as \u2018tracks\u2019, and use the selected reference genome as coordinates. The first thing we need to do is ensure the correct genome is loaded. By default, this is human reference genome hg19. We used hg38 as reference genome when aligning our chr17 reads. In the top toolbar, click \u2018Genome\u2019 then select Human (GRCh38/hg38) Now we have the current version of the human genome loaded and ready to use. The genome is divided by chromosome markers which you will see as sections marked at the top of the screen. Below that, we have a single \u2018track\u2019 - RefSeq gene annotations. We now need to add our two tracks - the variant calls, and alignments. To add the variant calls, click \u2018Tracks\u2019 then \u2018Local File\u2019 in the top navigation bar, and select the sorted variant_calls_sorted VCF file. To add the read alignments, click \u2018Tracks\u2019 then \u2018Local File\u2019 in the top navigation bar, then upload the sorted_alignments BAM and BAI files we downloaded from the SortSam output. Both the .BAM and .BAI files must be selected together. Currently, there is too much data to load the alignments and variant calls. We will need to zoom in to see this information. In the grey bar at the top of the genome browser, next to hg38, use the dropdown to select chr17. Now that chr17 is selected, we will investigate our variant calls. One of the variants was located approximately between position 66270000 and 66276000. We can view this region by typing coordinates in the box next to the chromosome selector. Set it to the following: This is what we can see in the region: A deletion is evident. No reads are aligned in this region, and the coverage is high enough to support this variant call. While this is clearly a variant, it is not spanning any known RefSeq genes. We are looking for a variant which is causing tumors, so genes involved in cell signalling such as PRKAR1A, tumor suppression, or growth factors may be implicated. Once you have looked at some variant calls in detail, expand the below to reveal the variant causing disease: Disease causing variant location Reveal chr17:68,509,063-68,520,941 Enter the coordinates above to view the disease variant. Your IGV may look similar to the following: There is a clear deletion spanning the first coding exon of the PRKAR1A gene. This deletion would likely have a large impact on protein function, as the start codon and an entire exon has been deleted. After obtaining this result, managing doctors diagnosed the patient with Carney complex due to the pathogenic deletion in PRKAR1A. This shows an early example of how SV detection can be used in a clinical setting to diagnose patient disease, and that long reads have an advantage over short reads for structural variation detection. Genomic Feature Annotation of Variant Calls \u00b6 As a final summary, we will annotate our variant calls with the genomic features they intersect with. Rather than manually inspect each variant call with a genome browser, we can automate this process. Genome annotations provide locations and descriptions for important genomic features which have been discovered. Genome annotations are available for most assemblies on RefSeq, and the hg38 annotations are naturally very good. Today we will use genome annotations in the general feature format (GFF) format. We will invoke a workflow which uses a GFF file and our variant calls VCF file as input, then annotates the variant calls with any features they intersect with. A GFF has been provided for chr17, and includes a vast amount of information. We will just look at coding sequences (CDS) intersecting our variants, as distruptions in these regions are likely to have functional impact. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=142e5f7c1f340838 Set the following: Genome Annotations (GFF): chr17_annotations.gff Sniffles Variant Calls (VCF): variant calls sorted Extract features Extract features: select \u2018CDS\u2019 from the list Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. View the \u2018variant calls summary (annotated)\u2019 output. Yours may be similar to the following: Considering we may have thousands of structural variants between an individual and hg38, this process can drastically cut down on time. In the associated paper, authors reduced the initial > 13,500 variant calls down to only 3, by filtering for variants which overlap a disease gene coding exon, and those which are not present in a healthy control sample. Conclusion \u00b6 Today, we have covered a large amount of theory and analysis. Structural variant detection is a growing area, and will no doubt yield many discoveries in the near future. As with any bioinformatic analysis, we first explored our input data, performed our analysis, then finished by visualising and interpreting our findings. For those wishing to learn more about human clinical SV calling, dbVar ( https://www.ncbi.nlm.nih.gov/dbvar/ ) and other repositories which contain information linking structural variants and disease is a good place to start. Additional reading \u00b6 Links to additional recommended reading and suggestions for related tutorials.","title":"Long-read Structural Variant Calling"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#structural-variant-calling-long-read-data","text":"Anticipated workshop duration when delivered to a group of participants is 4 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Structural variant calling - long read data"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#overview","text":"","title":"Overview"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with no command line knowledge. A web-based platform called Galaxy will be used to run our analysis. We will be using 1 line awk programs to process text output, but these will be supplied and explained.","title":"Skill level"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#description","text":"Long reads have turbo-charged structural variant detection - be part of the renaissance! Structural variation has historically been hard to detect. The advent of long reads, and improvements to the quality of reference genomes over time has recently enabled new discoveries in the field. This tutorial uses sniffles to implement a structural variant calling pipeline. Structural variant calling will be performed on a bacterial dataset to benchmark sniffles, then using a human clinical dataset to identify patient disease. We will explore one workflow for structural variant detection, then will visualise and summarise our results using multiple methods. Data: Nanopore reads: bacterial & human (FASTQ), genomic feature annotations (GFF), human reference genome hg38 Pipeline: Read summaries & QC, alignment, SV calling, text processing, visualisation Tools: NanoPlot, Filtlong, minimap2, CalMD, SortSam, sniffles, VCFsort, VCFannotate, awk, Circos, IGV Section 1 covers bacterial SV calling and benchmarking of bioinformatics tools. Section 2 will demonstrate SV calling on a human sample to diagnose a patient condition.","title":"Description"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#learning-objectives","text":"At the end of this introductory workshop, you will : Be able to perform SV calling in model and non-model organisms Be familiar with the current field of SV calling Gain an understanding of why and when SV calling is an appropriate analysis to perform.","title":"Learning Objectives"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#required-software","text":"No additional software needs to be installed for this workshop.","title":"Required Software"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#required-data","text":"No additional data needs to be downloaded for this workshop.","title":"Required Data"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#author-information","text":"Written by: Grace Hall Melbourne Bioinformatics, University of Melbourne Created/Reviewed: March 2021","title":"Author Information"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#background","text":"","title":"Background"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#what-is-structural-variation","text":"Genetic variation is always relative. In general, we have a reference sequence which we know lots about, and query sequences to compare against this reference. Given we know lots about the reference, the impact of any variation we find in the query sequences can be inferred. The query sequences can originate from a single individual or a group, depending on the biological question. Genetic variants are often separated into two categories: sequence variants , and structural variants . Sequence variants cover small-scale changes which affect a few nucleotides, such as single nucleotide variants (SNVs) or small insertions / deletions (Indels). They are particularly important when they impact coding sequences of genes, as can alter the amino acid sequence of proteins. Structural variants (SVs) are large-scale events (>50 bp) where entire sections of genetic material have changed. An example is a deletion, where an entire section of DNA has been removed. Structural variants have the potential to greatly alter the gene dosage of a cell by duplicating or deleting entire exons or genes at a time.","title":"What is Structural Variation?"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#benefit-of-long-reads","text":"Structural variation has historically been hard to detect. This is because structural variation often involves repeat elements which are notoriously hard to resolve using short-read sequencing technologies. Either the structural variant itself is a repeat, or it occurs in a repetitive region of the reference genome. In general, long reads have greater mappability. Mappability is the ability to unambiguously align reads to a reference. In the example above, a short read sampled from a tandem repeat cannot be accurately mapped, as it is equally likely it came from 3 different locations. On the other hand, a long read sampled from this region can be uniquely mapped to a single location. The extra repeat found in the isolate could also be detected given such a read. This is highly pertinent when working with the human genome, as more than 60% appears to be repetitive sequence . This repetitive sequence consists of 2 main elements: mobile elements, and repeats. Mobile elements are sections of DNA which copy or move themselves throughout our genome and include retrotransposons (LINE, SINE, LTR, and SVA) and DNA transposons. Repeats are genomic regions which contain the same sequence repeated many times, and consist of Short Tandem Repeats (STRs / microsatellites) which are 1-6 bp tandem repeats, or Variable Number of Tandem Repeats (VNTRs / minisatellites) which are tandem repeats where the repeat length is greater than 7 bp.","title":"Benefit of Long Reads"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#applications-of-structural-variant-detection","text":"The importance of structural variation has become more apparent in recent years. This has been driven by the advent of long-read sequencing technologies, and the availability of better reference genomes (including improvements to human reference genome hg38). Today, SV detection is implicated in many areas of bioinformatics, including the following: Human Disease Cohort studies Identifying new, disease-causing structural variation Assessing an individual\u2019s susceptibility to disease mediated by SVs Cancer genomics Susceptibility of individual to certain cancers Monitoring of tumor progression Agriculture Identifying desirable traits Flowering rate, frost / drought resistance, improved crop yield Genetic modification The Microbial World Understanding the relationship between microbiome SVs and human health Studying phylogeny and evolution of microbes (including horizontal gene transfer) Researching the spread of mobile elements and plasmids which convey virulence or antibiotic resistance genes","title":"Applications of Structural Variant Detection"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#structural-variant-types","text":"Structural variants are either destructive (involving a change in total genetic material), or non-destructive (total genetic material stays the same). Destructive variants are often particularly interesting, as they alter the gene dosage of a call. This can lead to overexpression of genes, or knock-out variants with zero expression if a gene was removed. Destructive SVs are highly implicated in cancer genomics, as undesirable genes such as tumor-suppressors can be removed, while desirable genes involving growth and proliferation can be expressed in much higher quantity. There are 5 main types of structural variation. Insertions, Deletions, Inversions, Duplications, and Translocations. Specifically, they involve the following: Insertions A foreign section of genetic material is inserted Deletions A section of genetic material is removed Inversions A section of genetic material swaps its orientation Duplications (interspersed or tandem) A section of genetic material is copied, then inserted. Can be separated into: Interspersed duplications, where the insertion site is away from the copied section Tandem duplications, where the insertion site is directly beside the copied section Translocations (intra- or inter-chromosomal) A section of genetic material is cut out, then inserted somewhere else. Can be separated into: Intra-chromosomal translocations, where the insertion site is within the same chromosome Inter-chromosomal translocations, where the insertion site is on another chromosome","title":"Structural Variant Types"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#variants-detected-by-common-sv-calling-programs","text":"Many SV calling programs do not attempt to detect interspersed duplications and intra-chromosomal translocations. This is because most SV callers have been designed for use with human data, and these variants are less relevant to the human genome than other organisms. Regarding interspersed duplications , most RNA transposons (the mechanism of variation) are inactive in the human genome. A few are still active, but an impact will only be seen if the insertion site (insert location of the copied segment) is within an important genomic feature, such as a gene body or enhancer region. Intra-chromosomal translocations are ignored for similar reasons. These are mainly caused by DNA transposons, which are considered inactive in humans. DNA transposons are still active in other eukaryotes (including most plants) and bacteria, so must be considered when working outside human data.","title":"Variants detected by common SV calling programs"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#todo-if-time-mechanisms","text":"","title":"TODO if time: Mechanisms"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#sv-calling-pipeline","text":"Today, we will explore one workflow for SV calling. We will first call variants using a benchmarking dataset, where the true SVs are known. We will then swap to a human clinical scenario, and will identify a structural variant causing patient disease. We will use the workflow below to explore structural variation. This workflow has 3 main sections - data exploration, calling variants, then interpreting our findings. Our process will be slightly different depending on whether we are working with the bacterial read set, or the human sample. Our SV calling workflow will consist of 5 key steps: Read QC The length and quality distribution of long-read data is highly variable. We will summarise our read set using NanoPlot , to understand what quality of results to expect during analysis. Filtlong will then be used to remove short reads, or those with patches of low quality. This will reduce erroneous read mapping and will improve our results. Alignment Structural variation is always relative to a reference. Our isolate reads will act as the query, and an appropriate reference genome will be selected to measure structural variation against. We will align our isolate reads using minimap2 to the chosen reference genome, then will pass the output alignment BAM file to our SV caller. SV calling Our SV caller, sniffles , uses the read alignment information to detect structural variation. It will output variant calls in VCF format, which we will then process with awk to create a summary which is easy to read. Feature annotation Variants which span or otherwise intersect with key genomic regions are most likely to have functional impact. Rather than doing this manually for each variant, we can use awk and VCFannotate to automatically label variants with the genomic features they intersect with. Visualisation Visualisation will be performed to understand variation on a genome-wide scale, and to probe single variants for their potential functional impact. Circos plots will be generated for the bacterial read set to view the total variation across the genome, and IGV will be used on the human clinical dataset to infer the functional consequences of variants.","title":"SV calling pipeline"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#section-1-bacterial-dataset","text":"","title":"Section 1: Bacterial dataset"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#introduction","text":"The first dataset we will use is a synthetically mutated bacterial genome. Structural variants and single base mutations have been added to an E. coli sakai assembly, then Nanopore reads of this mutated genome were simulated using NanoSim . We will use these reads to identify the SVs we added using our SV calling pipeline. The benefit to this approach is that we know the ground truth. SVs were manually added to our E. coli sakai reference genome, and their details were recorded. As we have a list of SVs which were added, we can assess sniffles performance at SV calling by the number of SVs it correctly identifies. In this section we will: Perform read Quality Control (QC) Align reads Perform SV calling Create SV call summaries using awk (text processing) Visualise SV calls with Circos We will employ a number of tools in our pipeline, so let\u2019s get to it!","title":"Introduction"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#getting-the-data","text":"To start, we need a set of reads from our synthetic isolate, a reference genome to call SVs against, and a \u2018ground truth\u2019 list of variants which have been added to our isolate. Import the following Galaxy history to get started: https://usegalaxy.org.au/u/graceh1024/h/sniffles-benchmarking-ecoli-sakai Importing a galaxy history How to import Galaxy histories can be accessed via a link, or in the \u2018Shared Data\u2019 tab of the top navigation bar. Once you find a history you want to copy, press the \u2018+\u2019 icon at the top right of the page to import the history The reference genome is \u201cecoli_sakai.fasta\u201d Our reads are \u201cisolate_reads.fastq\u201d The list of added SVs is \u201cisolate_sv_record.tsv\u201d","title":"Getting the data"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#read-qc","text":"Before proceeding, we want to understand our read set. Nanopore reads can have highly variable read length and quality distributions from sample to sample - by creating a summary, we can know what to expect from our reads, and whether we should perform QC filtering. Create a summary of the reads First, we want to get a summary of our read set. NanoPlot creates plots to summarise the length distribution and quality of our read set. Tool: NanoPlot Type of the file(s) to work on: fastq files: isolate_reads.fastq NanoPlot creates 5 outputs. Today, we are only interested in the HTML report. This report contains a summary table which will summarise most of the important information, followed by plots displaying the read length and quality distribution of reads. Your report may be similar to the following image: Answer the questions below regarding isolate_reads.fastq: What is the median read length? answer ____ bp What is the median read quality? answer Q__ (phred ___) What is the length and mean quality of the longest single read? answer ____ bp, Q ___ As a significant proportion of the reads are short or low quality, we will perform a filtering step to reduce incorrect SV calls downstream in our analysis. Filter reads Long reads can have a wide range of lengths and qualities. Filtering of long reads is often conducted slightly differently to short reads. Often, reads less than 1000 bp are removed, and reads containing patches of poor quality are discarded. This is an important distinction to short read filtering, as a given long read can have a good overall base quality, while having areas which are very low quality. As our SV caller, sniffles, looks at alignment quality when identifying structural variation, it is important we remove reads with low-quality patches. Tool: filtlong Input FASTQ isolate_reads.fastq Output thresholds Min. Length 1000 Min. window quality 9 rename the output to \u2018isolate_reads_filtered.fastq\u2019 The above will remove any reads which are less than 1000 bp, or those where a section (250bp) of the read has mean quality below Q9. Our readset will now be ready for structural variant calling.","title":"Read QC"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#alignment","text":"Sniffles requires an alignment file when calling variants. It searches the alignments for split reads, alignments containing patches of high error rate, and soft clipping of reads to identify structural variation. Below we can see how different isolate reads capturing an inversion might be aligned to the reference genome. In the first example, the inversion appears near the middle of the read and is large. In this case, the aligner may split the read and report the alignment in 3 sections. In scenario 2, the inversion is quite small, and the read may not be split. In this case there would be a patch of low sequence identity between the read and reference genome at the inversion location. In the final example, only the very edge of the read has spanned the SV, and as a result the aligner may just report a single alignment with part of the read clipped off. All this information is available to our SV caller after the reads are aligned to our reference genome. To provide this file, we need to map our read set to our reference genome of choice. The reference genome should be as close as possible to the isolate from which our reads originate for best results. In this situation, the original E. coli sakai assembly which we computationally mutated will be our reference genome. Align reads Tool: Map with minimap2 Will you select a reference genome from your history or use a built-in index? Use a genome from history and build index Use the following dataset as the reference sequence ecoli_sakai.fna Single or Paired-end reads single Select fastq dataset isolate_reads_filtered.fastq Select a profile of preset options Oxford Nanopore read to reference mapping\u2026 Set advanced output options Generate CIGAR Yes Rename output to \u201cisolate mapped reads\u201d Calculate MD Tag Sniffles requires alignments to contain the \u2018MD tag\u2019 in our BAM file. This is a condensed representation of the alignment of a read to the reference, and is similar to a CIGAR string. We can use CalMD to add this to each alignment in our BAM file. Tool: CalMD BAM file to recalculate isolate mapped reads Choose the source for the reference genome Use a genome from the history Using reference file ecoli_sakai.fna Rename output to \u201cisolate mapped reads MD\u201d","title":"Alignment"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#sv-calling-with-sniffles","text":"Now we have our alignments (BAM) in the correct format, we can call variants with sniffles. Sniffles generally has good defaults, so we will leave everything as-is for now. Later, we will tweak the settings based on our read set for better results. Call variants Tool: sniffles Input BAM file isolate mapped reads MD Rename the output to \u2018sniffles variant calls\u2019 Have a look at the VCF output of sniffles. It contains header lines providing metadata and definitions, followed by variant call lines. VCF stands for \u2018Variant Call Format\u2019 and is the standard format for recording variant information - both sequence variants , and structural variants. Click the eye icon to view. Sort VCF output Before continuing, we well sort the variant calls so they are in coordinate order. This will help us compare against the truth SV record for our simulated isolates (provided SV records are sorted by coordinate), and in future will allow us to view the variants using a genome browser. Tool: VCFsort Select VCF dataset sniffles output VCF Rename the output to \u2018sniffles variant calls sorted\u2019","title":"SV calling with sniffles"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#creating-a-summary-awk","text":"Unfortunately, the VCF file format was not created to store structural variant information, and generally does a poor job. In its current state, it is not very easy to quickly summarise our variant calls, as a lot of the important information is shoved in the \u2018INFO\u2019 field. We would prefer a format which identifies the contig, location, type, and size of each variant call in an easy to read manner. We can create such a summary using text processing - specifically by using the \u2018awk\u2019 tool. For each variant call we will extract the value for the #CHROM, #POS, and #ID fields, and from the #INFO field we will extract the remaining information we need. Awk processes the input file line-by-line. For each line, it follows the following approach: 1 pattern { action } For a given line, if the pattern is matched, the action will be performed. For example, given the following input file: 1 2 3 chr12 gene NTRK2 chr18 CDS BRCA1 chr22 gene P53 And the command: 1 Awk \u2018 / gene / { print $ 0 }\u2019 We will obtain the output: 1 2 chr12 gene NTRK2 chr22 gene P53 How it works: / gene / will match any line which contains \u201cgene\u201d, and the statement { print $ 0 } will print the line. Awk always sets some variables for the line being processed - in particular, the complete line is stored as $0, and $1, $2, $3 etc store the value for field 1, 2, 3 etc respectively. If desired, we can also perform multiple actions when the pattern is matched: 1 pattern { action 1 ; action 2 ; action 3 ; } Awk is a highly versatile tool for text processing, and can perform all the common functionality including conditional if / else statements and loops. From here the awk programs will be supplied, but if you wish to learn more, here is a good place to start: https://zetcode.com/lang/awk/ Process VCF with awk Tool: Text reformatting with awk File to process: sniffles variant calls sorted AWK Program: 1 / SVTYPE =/ { split ( $ 8 , infoArr , \";\" ); print substr ( infoArr [ 9 ], 8 ), $ 3 , $ 1 , substr ( infoArr [ 3 ], 6 ), $ 2 , substr ( infoArr [ 4 ], 5 ), substr ( infoArr [ 11 ], 7 ) } Rename output to \u201csniffles VCF summary\u201d Your output may look something like this:","title":"Creating a summary (awk)"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#calculating-sniffles-performance-metrics","text":"We now have both files we need to measure the performance of sniffles - the variant calls provided by sniffles, and the ground SV truth. Open both files in new tabs and compare them by right-clicking the eye icon then selecting \u2018open link in new tab\u2019. Specifically, note the following: How many real SVs did sniffles identify (true positives) How many did it miss? (false negatives) How many SVs were called by sniffles which were not actually added to the reference genome? (false positives) From this information we can calculate performance metrics for sniffles. The following formulas for accuracy, precision and recall are commonly used when benchmarking bioinformatics software. What was sniffles recall? answer ____ bp While the accuracy and precision of sniffles was good, the recall is low. This is due to a key setting in sniffles which relates to our read set - read support.","title":"Calculating sniffles Performance Metrics"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#tuning-sniffles-settings","text":"Sniffles has a default setting called \u2018read support\u2019 which requires 10 reads to support a possible SV for it to be accepted as genuine. Reducing this number allows more SVs to be discovered, but may also cause some false positives (SV calls for variants which do not actually exist). Conversely, we can be more strict by increasing this number. The best choice for this setting depends on the biological question you wish to answer, and the amount of read depth your dataset has. Our filtered read set fastq file was 139 mb, so we have approximately 70 mbp worth of long read data. As the genome size of our isolate is approximately 5 mbp, this equates to only around 12x mean read depth for a given location in the reference genome. Read depth is not uniform, so we expect some regions to have less than 12x depth, resulting in some structural variants being missed by sniffles if not enough reads supported the call. As our mean depth is 12x, but the quality of our reads is good, we will reduce the \u2018read support\u2019 setting to 5. Re-run sniffles Run sniffles again by clicking the re-run button on the sniffles VCF output. By clicking the re-run button, all the settings previously used will already be filled. Change the following: Set general options Minimum Support: 5 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles variant calls RS5\u2019 Re-run VCFsort Run VCFsort again by clicking the re-run button. Change the following: Select VCF dataset: \u2018sniffles variant calls RS5\u2019 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles variant calls RS5 sorted\u2019 Re-run awk Run awk again to create a summary by clicking the re-run button. Change the following: File to process: \u2018sniffles variant calls RS5 sorted\u2019 Leave all else default and click \u2018execute\u2019 Rename the output to \u2018sniffles VCF summary RS5\u2019 Our new variant calls should be an improvement on the original settings. You may see something similar to the following: We have increased our recall by lowering our read support threshold to 5, rather than 10. While being less conservative in this manner will increase recall, it also may result in greater false positives (sniffles calling variants that don\u2019t exist). Whether we maximise precision or recall depends on the task at hand - in this case, we want to discover all the variants and prefer high recall, and should therefore treat our variant calls with more scepticism.","title":"Tuning sniffles Settings"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#visualising-sv-calls","text":"Visualisation and interpretation is an important part of any analysis. Now we have our SV calls, we can view them on a genome-wide scale using a type of visualisation called a circos plot. Circos plots are often featured on the cover of academic journals as they can communicate a large amount of genomic information at a glance. We will now make a circos plot which displays our variant calls. Similar to genome browsers, circos plots are built from data tracks. When dealing with genomic data, the outer coordinate system (called the ideogram ) is usually the chromosomes of a genome, and the tracks pin data to their genomic positions. For this workshop, we will use a galaxy workflow to create our circos plot, but if you would like to learn how to create these plots yourself, see the following tutorial: https://training.galaxyproject.org/training-material/topics/visualisation/tutorials/circos/tutorial.html Create Circos plot We will be using a workflow to create circos plots for us. This will process our sniffles VCF summary and SV truth report, and produce a plot. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=6588e175004aba38 Optional: importing a galaxy workflow importing rather than running Galaxy workflows can be directly run, or can be imported as a workflow. The benefit to importing a workflow is that you can see all the tools that are being run, and can customise the workflow to suit your needs. Like shared histories, workflows can also be found in the \u2018Shared Data\u2019 tab of the top navigation bar. Once you find a workflow you want to import, press the \u2018+\u2019 icon at the top right of the page to import the workflow. The circos plot workflow can be imported using the following link: https://usegalaxy.org.au/u/graceh1024/w/long-read-sv-calling---circos-plots Set the following: sniffles VCF summary: sniffles VCF summary RS4 Reference Genome: ecoli_sakai.fna SV truth record: isolate_sv_record.tsv Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. Your output might be similar to the following figure: The circos plot has been formatted so both the variant calls (sniffles) and the true SVs are displayed. The ideogram (outer coordinate system) is displaying the bacterial chromosome and two plasmids. One plasmid is reasonable size, while the other is tiny. The outer tile track displays sniffles calls, and the inner track is the true added variants. Link tracks (lines connecting regions of the plot) have been added which show the translocation breakends called by sniffles in yellow, and any undetected breakends in red. From this plot, we can quickly see that sniffles detected most of the variants, but a few were still missed.","title":"Visualising SV calls"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#section-2-human-clinical-dataset","text":"","title":"Section 2: Human clinical dataset"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#introduction_1","text":"Pathogenic structural variation has become more thoroughly understood in recent years, partly driven by the advent of long-read sequencing technologies. For the remainder of this workshop, we will use a dataset which emulates a patient case. In this example, long-read sequence data was able to identify a causal SV, where short-read sequencing had previously reported a negative result. The dataset was simulated according to the findings of Merker et al (2018) and can be found at https://dx.doi.org/10.1038%2Fgim.2017.86 .","title":"Introduction"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#patient-case","text":"The patient is a male who had numerous recurring tumorous growths over their development. At age 7, an atrial myxoma of the heart was discovered and removed, followed by a Sertoli-Leydig cell tumor at age 10, a pituitary tumor at 13, more growths on the heart at 16 and 18 which were surgically removed. After the heart surgery at 18 years old, he suffered from a cardiac arrest which he eventually recovered from. At 18, the possibility of a Carney complex was suggested, but short read sequencing and analysis of the PRKAR1A gene returned negative for pathogenic variation. The patient continued to develop tumors over the following years, prompting another round of sequencing - this time, whole genome sequencing (WGS) using long reads. 26.7 Gb of reads were produced using the PacBio Sequel system, equating to an average read depth of 8.6x. The following dataset is simulated reads from a section of chr17 which emulate the patient case.","title":"Patient Case"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#getting-the-data_1","text":"To start, we need reads from our section of chr17 (pos 66,000,000 - 69,000,000) for variant calling against hg38, and a file listing genomic features (GFF) for automated annotation later on. Import the following Galaxy history to get started: https://usegalaxy.org.au/u/graceh1024/h/carney-complex---chr17-reads","title":"Getting the data"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#sv-calling-using-workflow","text":"Rather than manually running each tool again using our chr17 reads and human reference genome hg38 as reference, we will use a workflow to do the analysis for us. hg38 is a built-in genome in galaxy, so we do not need to provide it ourselves. The workflow will perform the following: Read QC (Filtlong) Produce a summary report of filtered reads (NanoPlot) Align reads to hg38 (minimap2) Calculate the MD tag (CalMD) and sort the BAM file by coordinate (SortSam) Call variants (sniffles) Sort the variant calls (VCFsort) Create a summary of the variant calls (awk) As we are aligning reads to hg38, the alignment step will take longer than for the bacterial dataset. The human genome is approximately 640x larger than E. coli sakai , so this is understandable. Reads may take 15 minutes to map during the workflow. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=d69a765cfc82a399 Set the following: Long reads: chr17_reads.fastq CalMD Using reference genome: \u2018hg38\u2019 Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. This workflow produces the key outputs we need. The NanoPlot HTML report summarises our reads after filtering, which we can view to determine the quality of our read set. The sorted alignments contain the alignment information, which we can download to view using a genome browser. variant calls sorted is the sorted VCF file produced by sniffles, and variant calls summary is our simplified awk summary of the variant calls. Use the eye icon to view these outputs for your own interest. All going well, your final variant calls summary will look similar to the following: As our reads are from a 3 mbp segment of chr17, the variants are all located on chr17, between position 6,600,000 and 6,900,000. 19 structural variants have been detected, which is a reasonable number for this segment given that multiple thousand variants are generally detected between any individual and hg38. One of these variants is causing patient disease, and we will identify the culprit using IGV.","title":"SV Calling using Workflow"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#visualising-alignments-with-igv","text":"We will use IGV to inspect the variant calls, and determine which may be causing disease. In particular, we are looking for structural variation which spans or interrupts gene coding sequences, as these are most likely to cause a disease phenotype. We wish to view the alignments and variant calls, so will download the sorted alignment files (BAM and BAI) and the variant calls (VCF). We can then upload this data to the IGV webapp to visualise. Download Alignments and Variant Calls Data Click the save icon in the \u2018sorted alignments\u2019 output, and download both the dataset (BAM) and the bam_index (BAI) files. The BAM contains read alignment information, and the BAI contains an index which allows IGV to load this data. Do the same for the \u2018variant calls sorted\u2019 VCF output. This time, simply clicking the save icon will download the VCF as there is only 1 piece of data. Open IGV and set to hg38 Navigate to https://igv.org/app/ to open IGV. Genome browsers use a reference genome as a coordinate system, and anchor data to these coordinates. Datasets are loaded as \u2018tracks\u2019, and use the selected reference genome as coordinates. The first thing we need to do is ensure the correct genome is loaded. By default, this is human reference genome hg19. We used hg38 as reference genome when aligning our chr17 reads. In the top toolbar, click \u2018Genome\u2019 then select Human (GRCh38/hg38) Now we have the current version of the human genome loaded and ready to use. The genome is divided by chromosome markers which you will see as sections marked at the top of the screen. Below that, we have a single \u2018track\u2019 - RefSeq gene annotations. We now need to add our two tracks - the variant calls, and alignments. To add the variant calls, click \u2018Tracks\u2019 then \u2018Local File\u2019 in the top navigation bar, and select the sorted variant_calls_sorted VCF file. To add the read alignments, click \u2018Tracks\u2019 then \u2018Local File\u2019 in the top navigation bar, then upload the sorted_alignments BAM and BAI files we downloaded from the SortSam output. Both the .BAM and .BAI files must be selected together. Currently, there is too much data to load the alignments and variant calls. We will need to zoom in to see this information. In the grey bar at the top of the genome browser, next to hg38, use the dropdown to select chr17. Now that chr17 is selected, we will investigate our variant calls. One of the variants was located approximately between position 66270000 and 66276000. We can view this region by typing coordinates in the box next to the chromosome selector. Set it to the following: This is what we can see in the region: A deletion is evident. No reads are aligned in this region, and the coverage is high enough to support this variant call. While this is clearly a variant, it is not spanning any known RefSeq genes. We are looking for a variant which is causing tumors, so genes involved in cell signalling such as PRKAR1A, tumor suppression, or growth factors may be implicated. Once you have looked at some variant calls in detail, expand the below to reveal the variant causing disease: Disease causing variant location Reveal chr17:68,509,063-68,520,941 Enter the coordinates above to view the disease variant. Your IGV may look similar to the following: There is a clear deletion spanning the first coding exon of the PRKAR1A gene. This deletion would likely have a large impact on protein function, as the start codon and an entire exon has been deleted. After obtaining this result, managing doctors diagnosed the patient with Carney complex due to the pathogenic deletion in PRKAR1A. This shows an early example of how SV detection can be used in a clinical setting to diagnose patient disease, and that long reads have an advantage over short reads for structural variation detection.","title":"Visualising Alignments with IGV"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#genomic-feature-annotation-of-variant-calls","text":"As a final summary, we will annotate our variant calls with the genomic features they intersect with. Rather than manually inspect each variant call with a genome browser, we can automate this process. Genome annotations provide locations and descriptions for important genomic features which have been discovered. Genome annotations are available for most assemblies on RefSeq, and the hg38 annotations are naturally very good. Today we will use genome annotations in the general feature format (GFF) format. We will invoke a workflow which uses a GFF file and our variant calls VCF file as input, then annotates the variant calls with any features they intersect with. A GFF has been provided for chr17, and includes a vast amount of information. We will just look at coding sequences (CDS) intersecting our variants, as distruptions in these regions are likely to have functional impact. Invoke the workflow using this link: https://usegalaxy.org.au/workflows/run?id=142e5f7c1f340838 Set the following: Genome Annotations (GFF): chr17_annotations.gff Sniffles Variant Calls (VCF): variant calls sorted Extract features Extract features: select \u2018CDS\u2019 from the list Click the blue \u2018Run Workflow\u2019 button on the top right to execute the workflow. View the \u2018variant calls summary (annotated)\u2019 output. Yours may be similar to the following: Considering we may have thousands of structural variants between an individual and hg38, this process can drastically cut down on time. In the associated paper, authors reduced the initial > 13,500 variant calls down to only 3, by filtering for variants which overlap a disease gene coding exon, and those which are not present in a healthy control sample.","title":"Genomic Feature Annotation of Variant Calls"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#conclusion","text":"Today, we have covered a large amount of theory and analysis. Structural variant detection is a growing area, and will no doubt yield many discoveries in the near future. As with any bioinformatic analysis, we first explored our input data, performed our analysis, then finished by visualising and interpreting our findings. For those wishing to learn more about human clinical SV calling, dbVar ( https://www.ncbi.nlm.nih.gov/dbvar/ ) and other repositories which contain information linking structural variants and disease is a good place to start.","title":"Conclusion"},{"location":"tutorials/longread_sv_calling/longread_sv_calling/#additional-reading","text":"Links to additional recommended reading and suggestions for related tutorials.","title":"Additional reading"},{"location":"tutorials/miRNAseq_basic/","text":"Templates \u00b6 This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/miRNAseq_basic/#templates","text":"This contains templates for overall workshop layout, as well as specific examples of how to incorporate aspects such as code blocks, equations, questions and answers etc.","title":"Templates"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/","text":"Basic miRNA-seq data analysis \u00b6 Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 This is a tutorial for basic miRNA-seq data analysis using miRdeep2 on Galaxy. It assumes that you have your sequencing result files in FASTQ format. From there this tutorial takes you through a series of tasks, including an essential quality control procedure, alignment, identification and quantification of (known) miRNAs and a basic differential expression analysis. Additionally, some subsequent analyses that commonly follow the differential expression analysis are suggested in the end. Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with limited skills in computers and linux/unix environments. Description \u00b6 This tutorial uses a subset of small RNA-seq data from a published paper, which was for a study of biogenesis of miRNAs (ref). The raw sequencing data for this study are downloadable from Gene Expression Omnibus database of NCBI, a public repository for genomic data, in FASTQ format. The downloaded FASTQ files were quality-checked and pre-processed by FastQC and Trimmomatic, and the pre-processed FASTQ files were analysed by MirDeep2 for the sequence alignment, identification and quantification of known miRNAs. The latest MiRBase database (ref) was used as the reference for known miRNAs of human. MiRBase is a database of published miRNAs sequences and annotation ( http://www.mirbase.org/ ) After the quantification of known human miRNAs, edgeR was used for differential expression analysis. Data: 4 samples from Gene Expression Omnibus dataset GSE56862 Pipeline & Tools: FastQC, Trimmomatic, miRDeep2, edgeR Section 1 covers basic background of miRNA and miRNA-seq experiment. Section 2 illustrates the overall miRNA-seq data procedure, introducing tools utilized for each step in this tutorial. Section 3 introduces the example dataset from NCBI GEO database. Section 4 covers essential quality control and pre-processing procedures. Section 5 covers taking the pre-processed miRNA-seq data through the \u2018MiRDeep2\u2019 pipeline. Section 6 covers how to combine (known) miRNA quantification result from each sample into a single read-count matrix. Section 7 (optional) introduces an alternative way to generate a single read-count matrix, using spreadsheet softwares such as MS Excel, Google-spreadsheet, etc. Section 8 covers basic differential expression analysis on miRNA read-count matrix using edgeR. In the Additional reading, some subsequent analysis that commonly follow the differential expression analysis are introduced. Learning Objectives \u00b6 At the end of this introductory workshop, you will : Understand the overall procedure of miRNA-seq data analysis. Be familiar with data types and tools for miRNA-seq data analysis. Know how to use the Galaxy for data-processing and exploring. Requirements and preparation \u00b6 Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Preparing your laptop prior to starting this workshop \u00b6 Required softwares: Web browser is necessary ( Chrome or FireFox are preferable.) Required Data: Required data will be directly downloaded from NCBI\u2019s GEO site and MiRBase database site. Author Information \u00b6 Written by: Chol-hee JUNG Melbourne Bioinformatics, University of Melbourne Created/Reviewed: March, 2020 Updated: Aug, 2020 Background \u00b6 MicroRNA (miRNA) is a type of small non-coding RNA, which is typically ~21nt long and processed from a hairpin structured precursor (typically 60-120nt long). MiRNA is found in animals and plants, and involved in the gene-expression regulation in various mechanisms by binding to their target mRNAs (e.g., inducing degradation, inhibiting translation, etc.). Section 1: Basics of miRNAs \u00b6 In this section you will learn about some basics of miRNA-seq experiments. Section 2: Overview of miRNA-seq data analysis procedure \u00b6 In this section we will take a look at the overview of miRNA-seq analysis procedure. As shown in the image below, once library construction and sequencing are done as per the design of the experiment, the resulting sequencing data (FASTQ files) will go through two steps of quality control (QC). The first QC is for checking the sequencing quality. Popular FASTQ quality assessment tools, such as FastQC , should be able to report the overall quality of the sequencing data. FastQC also reports the over-represented sequences in the FASTQ file. This is particularly important for miRNA-seq. In miRNA-seq data the 3\u2019-adapter sequence almost always appears in every read, because the typical read-length of sequencing machines exceed the full length of mature miRNAs. Unless there is a serious contamination, the most over-represented sequence in the FASTQ file should be the 3\u2019adapter sequence. Unlike 3\u2019-adapter sequence, 5\u2019-adapter sequence should be appear in the FASTQ file. If it does, it would mean poor library quality or poor sequencing quality. Once the over-represented sequences are identified as 3\u2019-adapter sequence, sequence trimming tools, such as Trimmomatic , can trim out from each read the adapter sequence and/or prevailing or trailing poor quality regions. The second QC checks more specifically for the library quality. At this stage, 3\u2019-adapter sequence should be removed from the sequencing data. Without 3\u2019-adapter sequence, each read should contains only the mature miRNA sequence, the length of which typically ranges 20-25nt. If the trimmed sequences include a large quantity of sequence reads that are outside of this size-range, it may mean high level of contamination at the library construction stage. After two-steps of QC, when all look good, the trimmed reads will be mapped to the genome. From this alignment result, the expression level known miRNAs can be measured, and novel miRNAs can be also identified and quantified. Known/unknown miRNAs were identified and quantified across samples from multiple groups (e.g., disease vs control), differential expression analysis can follow using other popular tools, such as edgeR . Section 3: Example dataset from NCBI GEO \u00b6 In this section, we will import the example dataset to Galaxy for the analysis. Example dataset from NCBI GEO \u00b6 GSE: GSE56862 SRA: SRP041228 This GEO dataset GSE56832 consist of 12 samples, including 7 small RNA-seq samples and 5 RNA-seq samples. The GSE dataset page from the link contains the description of the study using this dataset. Briefly, this study examines the biogenesis of miRNAs by the Microprocessor complex. The actual raw sequencing data are separately deposited in Sequence Read Archive (SRA) with the ID SRP041228. Although there are a total of 12 samples in this dataset, we will use only 4 of them with individual sample accession IDs from GSM1370365 through to GSM1370368 as below. Sample Description SRX ID SRR ID GSM1370365 Hela_repA_smallRNASeq SRX518817 SRR1240812 GSM1370366 Hela_repB_smallRNASeq SRX518818 SRR1240813 GSM1370367 A549_repA_smallRNASeq SRX518819 SRR1240814 GSM1370368 A549_repB_smallRNASeq SRX518820 SRR1240815 1. Download example sequences \u00b6 Raw FASTQ files for the 4 small RNA-seq samples need to be downloaded to Galaxy. Galaxy has a functionality to directly retrieve data from SRA using the accession IDs. a. \u2018Get Data\u2019 -> \u2018Download and Extract Reads in FASTA/Q\u2019 Put a SRR ID (e.g, SRR1240812) in the \u2018Accession\u2019 box b. Repeat for three other SRR IDs. It may take some time to complete this task. When completed the downloaded FASTQ files will appear in the \u2018History\u2019 panel on the right-hand side of the screen. Clicking on the \u2018View data\u2019 icon show top lines of the imported FASTQ file as below. c. Rename the uploaded data to more appropriate names by clicking on \u2018Edit attributes\u2019 icon e.g., \u2018SRR1240812\u2019 to \u2018Hela repA FASTQ\u2019, \u2018SRR1240813\u2019 to \u2018Hela repB FASTQ\u2019, \u2018SRR1240814\u2019 to \u2018A549 repA FASTQ\u2019, \u2018SRR1240815\u2019 to \u2018A549 repB FASTQ\u2019 d. Alternatively, the raw FASTQ files can be downloaded to local computer and be uploaded using \u2018Get Data\u2019 -> \u2018Upload File\u2019 . (need a screenshot) 2. Upload reference miRNA data from miRBase \u00b6 MiRDeep2 uses sequences of known miRNAs (both precursor and mature) as reference to identify which miRNAs are present in the input data. In this tutorial, we will use the miRNA sequences deposited in miRBase database (ref). The reference precursor miRNAs and mature miRNA sequences can be downloaded to local computer from the miRBase FTP site, and be uploaded to Galaxy. But, Galaxy can also fetch files directly as using the URL of files: a. \u2018Get Data\u2019 -> \u2018Upload File\u2019 -> \u2018Paste/Fetch data\u2019 Copy and paste the URLs below into the text box, and set the \u2018Type\u2019 to \u2018fasta.gz\u2019. Leave other parameters as default. ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.gz ftp://mirbase.org/pub/mirbase/CURRENT/mature.fa.gz Section 4: Quality control and pre-processing \u00b6 In this section we will check the quality of the 4 small RNA-seq data uploaded to Galaxy. As mentioned above, we will use FastQC and Trimmomatic. Quality control and pre-processing include the checking of general sequencing quality, adapter-contamination and the removal of any un-wanted parts from the sequencing data. FastQC examines the sequencing quality, and Trimmomatic cleans up the sequencing data. 1. Check sequencing quality \u00b6 a. \u2018FASTQ Quality Control\u2019 -> \u2018FastQC\u2019 All four FASTQ files can be selected at once. b. In most cases, the sequence reads are longer than mature miRNA sequences. In consequence, 3\u2019-adapter sequence is left in the FASTQ file. In contrast, 5\u2019-adapter sequence should not be present in the FASTQ file. It this is from one\u2019s own analysis, the information of 3\u2019-adapter sequence would be probably provided. However, if the adapter sequence is unknown, it could be identified from FastQC reports. Among the over-represented sequences, the sequence in the first row takes up ~64% of all sequences of Hela_repA. Also, the whole 50bp region is identical to a known RNA PCR primer sequence with 100% identity. This is likely the adapter sequence to be removed before further analysis. 2. Trim out adapter sequence and low-quality region \u00b6 a. \u2018FASTA/FASTQ\u2019 -> \u2018Trimmomatic\u2019 b. Select all 4 FastQ files c. \u2018Yes\u2019 to \u2018Perform initial ILLUMINACLIP step?\u2019 d. \u2018Custom\u2019 to \u2018Select standard adapter sequences or provide custom?\u2019 and paste in the most over-represented RNA PCR Primer sequence in FASTA format e.g. >primer TGGAATTCTCGGGTGCCAAGGAACTCCAGTCACATCACGATCTCGTATGC e. \u2018Execute\u2019 f. Rename Trimmomatic results to more appropriate names e.g., \u2018Trimmomatic on Hela repA FASTQ\u2019 to \u2018Hela repA trimmed\u2019 3. Run FastQC again on the trimmed \u00b6 It is always a good idea to check the quality of the pre-processed data, which are the result files from Trimmomatic, in this case. Thus, FastQC run at this stage is mainly for checking on read-length distribution. a. \u2018FASTQ Quality Control\u2019 -> \u2018FastQC\u2019 b. \u2018Execute\u2019 c. After adapter-trimming, sequences of typical miRNA-sizes are enriched. Note that this sample contains miRNAs and other types of small RNAs. So, we should expect to see peaks at the sizes outside of typical miRNA lengths. Section 5: Running miRdeep2 \u00b6 In this section we will run MiRDeep2 to identify and quantify known human miRNAs present in the sequencing data. First, the pre-processed sequencing data will be mapped to the reference genome using \u2018MiRDeep2 Mapper\u2019. By default, pre-processed reads are collapsed before mapping. I.e., identical reads become a single read, but the total number of identical reads is kept in the sequence description line. This reduces the mapping time. 1. Run \u2018MiRDeep2 Mapper\u2019 on built-in genome (hg38) with trimmed sequences \u00b6 a. Select all 4 trimmed results b. Select \u2018Clip Sequence\u2019 to clip 3\u2019-Adapter Sequence and insert the adapter sequence used for Trimmomatic. c. Select \u2018Human (Homo sapiens) (b38): hg38\u2019 in \u2018Select a reference genome\u2019 d. \u2018Execute\u2019 This may take a while. e. Rename the mapping results to more appropriate names e.g., \u2018Collapsed reads of MiRDeep2 Mapper on data 16\u2019 to \u2018Hela repA mapped\u2019 2. Run \u2018MiRDeep2 Quantifier\u2019 on \u2018MiRDeep2 Mapper\u2019 results \u00b6 a. In \u2018Collapsed deep sequencing reads\u2019, Select all four \u2018MirDeep2 Mapper\u2019 results. b. Select \u2018 ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.gz \u2019 for \u2018Precursor sequences\u2019 c. Select \u2018 ftp://mirbase.org/pub/mirbase/CURRENT/mature.fa.gz \u2019 for \u2018Mature miRNA sequences\u2019 d. Select \u2018human\u2019 for \u2018Search in species\u2019 e. \u2018Execute\u2019 This may take a while. f. Rename the quantification results to more appropriate names e.g., \u2018output of MiRDeep2 Quantifier on data 7, data 6, and data 20\u2019 to \u2018MiRDeep2 Quantifier on Hela repA\u2019 \u2018MiRDeep2 Quantifier on data 7, data 6, and data 20 (html report)\u2019 to \u2018MiRDeep2 Quantifier on Hela repA (html report)\u2019 3. Inspect the quantification results. \u00b6 \u2018MirDeep2 Quantifier\u2019 generates two output files: quantification table and detailed HTML report of the miRNA quantification. a. \u2018MiRDeep2 Quantifier on A549 repB\u2019 shows the number of reads associated to each of known miRNA in the reference data. In this example, \u2018read_count\u2019, \u2018total\u2019 and \u2018seq\u2019 columns have the same raw-read counts, which could be used for the subsequent differential expression analysis. b. \u2018MiRDeep2 Quantifier on A549 repB (html report)\u2019 show the same read-count table with links to detailed information. Section 6: Generate a single read-count matrix \u00b6 In this section we will merge the individual quantification results into one read-count matrix, which can be used for differential expression analysis. \u2018MiRDeep2 Quantifier\u2019 output data for each of 4 samples will be merged into one big table, and relevant columns will be extracted from the merged table. 1. Merge read-count columns from four quantification results into one table. \u00b6 a. \u2018Text Manipulation\u2019 -> \u2018Paste\u2019. Select \u2018output of MiRDeep2 Quantifier on Hela repA\u2019 for the first drop-down box, and \u2018output of MiRDeep2 Quantifier on Hela repB\u2019 for the second drop-down box. b. Repeat the \u2018Paste\u2019 with the previous \u2018Paste\u2019 result and another quantification output: \u2018Paste on data 39 and data 37\u2019 for the first drop-box\u2019 and \u2018output of MiRDeep2 Quantifier on A549 repA\u2019 c. Repeat \u2018b\u2019 for \u2018output of MiRDeep2 Quantifier on A549 repB\u2019 The result is a table of 24 columns, because 4 tables of 6 columns were merged side-by-side. 2. Extract the read-count matrix \u00b6 The merged table from the previous step has lots of redundant columns. So, we\u2019ll have to extract only read-count columns. Also, miRNA IDs are not appropriate for unique identifier, because there could be multiple copies of precursor miRNAs originating the same miRNAs. For example, \u2018hsa-let-7a-5p\u2019 appears 3 times in the table, because there are 3 copies of \u2018hsa-let-7a\u2019 (hsa-let-7a-1, hsa-let-7a-2 and hsa-let-7a-3). So, we\u2019ll combine the precursor ID and mature miRNA ID with \u2018.\u2019 in-between. a. \u2018Text Manipulation\u2019 -> \u2018Add column\u2019 \u2018Add this value\u2019: \u2018.\u2019 \u2018to Dataset\u2019: \u201838: Paste on data 34 and data 37\u2019 The resulting table has a column on \u2018.\u2019 at the right-most end (column 25). b. \u2018Merge Columns\u2019-> \u2018+Insert Columns\u2019 button (we\u2019re merging 3 columns). \u2018Select data\u2019: result of the previous step (e.g., \u201848: Add column on data 47\u2019) \u2018Merge column\u2019: \u2018column: 3\u2019 (column for \u2018precursor\u2019) \u2018with column\u2019: \u2018column: 25\u2019 (added column in the previous step, containing only \u2018.\u2019) \u2018Add column\u2019: \u2018Column: 1\u2019 (column for \u2018mature miRNA\u2019) c. \u2018Cut\u2019 Input \u2018c26,c2,c8,c14,c20\u2019 \u2018c26\u2019 (Column 26) is the newly added column containing the new ID, which is precursor name followed by \u2018.\u2019 and mature miRNA name. c2, c8, c14 and c20 are columns for read-counts. d. The resulting table has only the new IDs and four read-count columns. e. The merged table include all known miRNAs in the reference data. But they aren\u2019t always expressed. We\u2019re keeping in miRNAs that were expressed at least in one sample. \u2018Filter and Sort\u2019 -> \u2018Filter\u2019 Insert \u2018(c2+c3+c4+c5) > 0\u2019 in \u2018With following condition\u2019 3. Add column names to the filtered read-count matrix \u00b6 The filtered table has no column names. The easiest way to add them in is downloading the table file, adding the header using a text editor and re-uploading the new file. a. Download the filtered table by clicking on \u2018Download\u2019 icon. b. Use any text editor to add the header line as below: \u2018precursor.miRNA Hela_repA Hela_repB A549_repA A549_repB\u2019 Remember to use \u2018tab\u2019 as separator c. Save and upload the edited table file \u2018Get Data\u2019 -> \u2018Upload File\u2019 d. Rename the uploaded table to \u2018GSE56862 miRNAs read-counts\u2019 Section 7: An alternative way to generate a single read-count matrix (optional) \u00b6 In this section we will use a spreadsheet software and a text editor on your local computer to generate a single read-count matrix. Depending on your familiarity to those software, you may find it much easier to achieve the same thing that we did in the Section 6. Section 8: Basic differential expression analysis \u00b6 In this section we will try a differential expression analysis on the miRNAs. Now, we have miRNA expression data from a subset of GSE56862 dataset. The miRNA expression data were now merged into a single read-count table, which can be analyzed by any count-based differential expression analysis tools. In this tutorial, we will use \u2018edgeR\u2019. a. \u2018NGA: RNA Analysis\u2019 -> \u2018edgeR\u2019 \u2018Single Count Matrix\u2019 \u2018Factor Name\u2019: \u2018cell_line\u2019 \u2018Groups\u2019: HeLa,HeLa,A549,A549 \u2018Contrast of Interest\u2019: HeLa-A549 b. Before the execution, we can choose to obtain the normalised counts, R scripts and R dataset from this edgeR analysis. c. \u2018edgeR\u2019 report d. Summary of differential expression test for each miRNA Additional reading (optional) \u00b6 Once differentially expressed miRNAs are identified, one of the common subsequent analyses is looking for the (potential) target genes, which are the genes that are regulated by miRNAs. Target gene analysis tools include miRanda and TargetScan among many. For example, \u2018miRanda\u2019 takes in a list of miRNAs of interest and retrieves the list of genes that have the target sites. Two miRNAs \u201clet-7e\u201d and \u201cmir-1\u201d were among the differentially expressed miRNAs in this tutorial. Searching for the target sites of these two miRNAs retrieved thousands of genes as below.","title":"intro miRNAseq"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#basic-mirna-seq-data-analysis","text":"Anticipated workshop duration when delivered to a group of participants is 2 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"Basic miRNA-seq data analysis"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#overview","text":"This is a tutorial for basic miRNA-seq data analysis using miRdeep2 on Galaxy. It assumes that you have your sequencing result files in FASTQ format. From there this tutorial takes you through a series of tasks, including an essential quality control procedure, alignment, identification and quantification of (known) miRNAs and a basic differential expression analysis. Additionally, some subsequent analyses that commonly follow the differential expression analysis are suggested in the end.","title":"Overview"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with limited skills in computers and linux/unix environments.","title":"Skill level"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#description","text":"This tutorial uses a subset of small RNA-seq data from a published paper, which was for a study of biogenesis of miRNAs (ref). The raw sequencing data for this study are downloadable from Gene Expression Omnibus database of NCBI, a public repository for genomic data, in FASTQ format. The downloaded FASTQ files were quality-checked and pre-processed by FastQC and Trimmomatic, and the pre-processed FASTQ files were analysed by MirDeep2 for the sequence alignment, identification and quantification of known miRNAs. The latest MiRBase database (ref) was used as the reference for known miRNAs of human. MiRBase is a database of published miRNAs sequences and annotation ( http://www.mirbase.org/ ) After the quantification of known human miRNAs, edgeR was used for differential expression analysis. Data: 4 samples from Gene Expression Omnibus dataset GSE56862 Pipeline & Tools: FastQC, Trimmomatic, miRDeep2, edgeR Section 1 covers basic background of miRNA and miRNA-seq experiment. Section 2 illustrates the overall miRNA-seq data procedure, introducing tools utilized for each step in this tutorial. Section 3 introduces the example dataset from NCBI GEO database. Section 4 covers essential quality control and pre-processing procedures. Section 5 covers taking the pre-processed miRNA-seq data through the \u2018MiRDeep2\u2019 pipeline. Section 6 covers how to combine (known) miRNA quantification result from each sample into a single read-count matrix. Section 7 (optional) introduces an alternative way to generate a single read-count matrix, using spreadsheet softwares such as MS Excel, Google-spreadsheet, etc. Section 8 covers basic differential expression analysis on miRNA read-count matrix using edgeR. In the Additional reading, some subsequent analysis that commonly follow the differential expression analysis are introduced.","title":"Description"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#learning-objectives","text":"At the end of this introductory workshop, you will : Understand the overall procedure of miRNA-seq data analysis. Be familiar with data types and tools for miRNA-seq data analysis. Know how to use the Galaxy for data-processing and exploring.","title":"Learning Objectives"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#requirements-and-preparation","text":"Important Attendees are required to bring their own laptop computers. At least one week before the workshop, participants should install the software and data files below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#preparing-your-laptop-prior-to-starting-this-workshop","text":"Required softwares: Web browser is necessary ( Chrome or FireFox are preferable.) Required Data: Required data will be directly downloaded from NCBI\u2019s GEO site and MiRBase database site.","title":"Preparing your laptop prior to starting this workshop"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#author-information","text":"Written by: Chol-hee JUNG Melbourne Bioinformatics, University of Melbourne Created/Reviewed: March, 2020 Updated: Aug, 2020","title":"Author Information"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#background","text":"MicroRNA (miRNA) is a type of small non-coding RNA, which is typically ~21nt long and processed from a hairpin structured precursor (typically 60-120nt long). MiRNA is found in animals and plants, and involved in the gene-expression regulation in various mechanisms by binding to their target mRNAs (e.g., inducing degradation, inhibiting translation, etc.).","title":"Background"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-1-basics-of-mirnas","text":"In this section you will learn about some basics of miRNA-seq experiments.","title":"Section 1: Basics of miRNAs"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-2-overview-of-mirna-seq-data-analysis-procedure","text":"In this section we will take a look at the overview of miRNA-seq analysis procedure. As shown in the image below, once library construction and sequencing are done as per the design of the experiment, the resulting sequencing data (FASTQ files) will go through two steps of quality control (QC). The first QC is for checking the sequencing quality. Popular FASTQ quality assessment tools, such as FastQC , should be able to report the overall quality of the sequencing data. FastQC also reports the over-represented sequences in the FASTQ file. This is particularly important for miRNA-seq. In miRNA-seq data the 3\u2019-adapter sequence almost always appears in every read, because the typical read-length of sequencing machines exceed the full length of mature miRNAs. Unless there is a serious contamination, the most over-represented sequence in the FASTQ file should be the 3\u2019adapter sequence. Unlike 3\u2019-adapter sequence, 5\u2019-adapter sequence should be appear in the FASTQ file. If it does, it would mean poor library quality or poor sequencing quality. Once the over-represented sequences are identified as 3\u2019-adapter sequence, sequence trimming tools, such as Trimmomatic , can trim out from each read the adapter sequence and/or prevailing or trailing poor quality regions. The second QC checks more specifically for the library quality. At this stage, 3\u2019-adapter sequence should be removed from the sequencing data. Without 3\u2019-adapter sequence, each read should contains only the mature miRNA sequence, the length of which typically ranges 20-25nt. If the trimmed sequences include a large quantity of sequence reads that are outside of this size-range, it may mean high level of contamination at the library construction stage. After two-steps of QC, when all look good, the trimmed reads will be mapped to the genome. From this alignment result, the expression level known miRNAs can be measured, and novel miRNAs can be also identified and quantified. Known/unknown miRNAs were identified and quantified across samples from multiple groups (e.g., disease vs control), differential expression analysis can follow using other popular tools, such as edgeR .","title":"Section 2: Overview of miRNA-seq data analysis procedure"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-3-example-dataset-from-ncbi-geo","text":"In this section, we will import the example dataset to Galaxy for the analysis.","title":"Section 3: Example dataset from NCBI GEO"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#example-dataset-from-ncbi-geo","text":"GSE: GSE56862 SRA: SRP041228 This GEO dataset GSE56832 consist of 12 samples, including 7 small RNA-seq samples and 5 RNA-seq samples. The GSE dataset page from the link contains the description of the study using this dataset. Briefly, this study examines the biogenesis of miRNAs by the Microprocessor complex. The actual raw sequencing data are separately deposited in Sequence Read Archive (SRA) with the ID SRP041228. Although there are a total of 12 samples in this dataset, we will use only 4 of them with individual sample accession IDs from GSM1370365 through to GSM1370368 as below. Sample Description SRX ID SRR ID GSM1370365 Hela_repA_smallRNASeq SRX518817 SRR1240812 GSM1370366 Hela_repB_smallRNASeq SRX518818 SRR1240813 GSM1370367 A549_repA_smallRNASeq SRX518819 SRR1240814 GSM1370368 A549_repB_smallRNASeq SRX518820 SRR1240815","title":"Example dataset from NCBI GEO"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#1-download-example-sequences","text":"Raw FASTQ files for the 4 small RNA-seq samples need to be downloaded to Galaxy. Galaxy has a functionality to directly retrieve data from SRA using the accession IDs. a. \u2018Get Data\u2019 -> \u2018Download and Extract Reads in FASTA/Q\u2019 Put a SRR ID (e.g, SRR1240812) in the \u2018Accession\u2019 box b. Repeat for three other SRR IDs. It may take some time to complete this task. When completed the downloaded FASTQ files will appear in the \u2018History\u2019 panel on the right-hand side of the screen. Clicking on the \u2018View data\u2019 icon show top lines of the imported FASTQ file as below. c. Rename the uploaded data to more appropriate names by clicking on \u2018Edit attributes\u2019 icon e.g., \u2018SRR1240812\u2019 to \u2018Hela repA FASTQ\u2019, \u2018SRR1240813\u2019 to \u2018Hela repB FASTQ\u2019, \u2018SRR1240814\u2019 to \u2018A549 repA FASTQ\u2019, \u2018SRR1240815\u2019 to \u2018A549 repB FASTQ\u2019 d. Alternatively, the raw FASTQ files can be downloaded to local computer and be uploaded using \u2018Get Data\u2019 -> \u2018Upload File\u2019 . (need a screenshot)","title":"1. Download example sequences"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#2-upload-reference-mirna-data-from-mirbase","text":"MiRDeep2 uses sequences of known miRNAs (both precursor and mature) as reference to identify which miRNAs are present in the input data. In this tutorial, we will use the miRNA sequences deposited in miRBase database (ref). The reference precursor miRNAs and mature miRNA sequences can be downloaded to local computer from the miRBase FTP site, and be uploaded to Galaxy. But, Galaxy can also fetch files directly as using the URL of files: a. \u2018Get Data\u2019 -> \u2018Upload File\u2019 -> \u2018Paste/Fetch data\u2019 Copy and paste the URLs below into the text box, and set the \u2018Type\u2019 to \u2018fasta.gz\u2019. Leave other parameters as default. ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.gz ftp://mirbase.org/pub/mirbase/CURRENT/mature.fa.gz","title":"2. Upload reference miRNA data from miRBase"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-4-quality-control-and-pre-processing","text":"In this section we will check the quality of the 4 small RNA-seq data uploaded to Galaxy. As mentioned above, we will use FastQC and Trimmomatic. Quality control and pre-processing include the checking of general sequencing quality, adapter-contamination and the removal of any un-wanted parts from the sequencing data. FastQC examines the sequencing quality, and Trimmomatic cleans up the sequencing data.","title":"Section 4: Quality control and pre-processing"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#1-check-sequencing-quality","text":"a. \u2018FASTQ Quality Control\u2019 -> \u2018FastQC\u2019 All four FASTQ files can be selected at once. b. In most cases, the sequence reads are longer than mature miRNA sequences. In consequence, 3\u2019-adapter sequence is left in the FASTQ file. In contrast, 5\u2019-adapter sequence should not be present in the FASTQ file. It this is from one\u2019s own analysis, the information of 3\u2019-adapter sequence would be probably provided. However, if the adapter sequence is unknown, it could be identified from FastQC reports. Among the over-represented sequences, the sequence in the first row takes up ~64% of all sequences of Hela_repA. Also, the whole 50bp region is identical to a known RNA PCR primer sequence with 100% identity. This is likely the adapter sequence to be removed before further analysis.","title":"1. Check sequencing quality"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#2-trim-out-adapter-sequence-and-low-quality-region","text":"a. \u2018FASTA/FASTQ\u2019 -> \u2018Trimmomatic\u2019 b. Select all 4 FastQ files c. \u2018Yes\u2019 to \u2018Perform initial ILLUMINACLIP step?\u2019 d. \u2018Custom\u2019 to \u2018Select standard adapter sequences or provide custom?\u2019 and paste in the most over-represented RNA PCR Primer sequence in FASTA format e.g. >primer TGGAATTCTCGGGTGCCAAGGAACTCCAGTCACATCACGATCTCGTATGC e. \u2018Execute\u2019 f. Rename Trimmomatic results to more appropriate names e.g., \u2018Trimmomatic on Hela repA FASTQ\u2019 to \u2018Hela repA trimmed\u2019","title":"2. Trim out adapter sequence and low-quality region"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#3-run-fastqc-again-on-the-trimmed","text":"It is always a good idea to check the quality of the pre-processed data, which are the result files from Trimmomatic, in this case. Thus, FastQC run at this stage is mainly for checking on read-length distribution. a. \u2018FASTQ Quality Control\u2019 -> \u2018FastQC\u2019 b. \u2018Execute\u2019 c. After adapter-trimming, sequences of typical miRNA-sizes are enriched. Note that this sample contains miRNAs and other types of small RNAs. So, we should expect to see peaks at the sizes outside of typical miRNA lengths.","title":"3. Run FastQC again on the trimmed"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-5-running-mirdeep2","text":"In this section we will run MiRDeep2 to identify and quantify known human miRNAs present in the sequencing data. First, the pre-processed sequencing data will be mapped to the reference genome using \u2018MiRDeep2 Mapper\u2019. By default, pre-processed reads are collapsed before mapping. I.e., identical reads become a single read, but the total number of identical reads is kept in the sequence description line. This reduces the mapping time.","title":"Section 5: Running miRdeep2"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#1-run-mirdeep2-mapper-on-built-in-genome-hg38-with-trimmed-sequences","text":"a. Select all 4 trimmed results b. Select \u2018Clip Sequence\u2019 to clip 3\u2019-Adapter Sequence and insert the adapter sequence used for Trimmomatic. c. Select \u2018Human (Homo sapiens) (b38): hg38\u2019 in \u2018Select a reference genome\u2019 d. \u2018Execute\u2019 This may take a while. e. Rename the mapping results to more appropriate names e.g., \u2018Collapsed reads of MiRDeep2 Mapper on data 16\u2019 to \u2018Hela repA mapped\u2019","title":"1. Run \u2018MiRDeep2 Mapper\u2019 on built-in genome (hg38) with trimmed sequences"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#2-run-mirdeep2-quantifier-on-mirdeep2-mapper-results","text":"a. In \u2018Collapsed deep sequencing reads\u2019, Select all four \u2018MirDeep2 Mapper\u2019 results. b. Select \u2018 ftp://mirbase.org/pub/mirbase/CURRENT/hairpin.fa.gz \u2019 for \u2018Precursor sequences\u2019 c. Select \u2018 ftp://mirbase.org/pub/mirbase/CURRENT/mature.fa.gz \u2019 for \u2018Mature miRNA sequences\u2019 d. Select \u2018human\u2019 for \u2018Search in species\u2019 e. \u2018Execute\u2019 This may take a while. f. Rename the quantification results to more appropriate names e.g., \u2018output of MiRDeep2 Quantifier on data 7, data 6, and data 20\u2019 to \u2018MiRDeep2 Quantifier on Hela repA\u2019 \u2018MiRDeep2 Quantifier on data 7, data 6, and data 20 (html report)\u2019 to \u2018MiRDeep2 Quantifier on Hela repA (html report)\u2019","title":"2. Run \u2018MiRDeep2 Quantifier\u2019 on \u2018MiRDeep2 Mapper\u2019 results"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#3-inspect-the-quantification-results","text":"\u2018MirDeep2 Quantifier\u2019 generates two output files: quantification table and detailed HTML report of the miRNA quantification. a. \u2018MiRDeep2 Quantifier on A549 repB\u2019 shows the number of reads associated to each of known miRNA in the reference data. In this example, \u2018read_count\u2019, \u2018total\u2019 and \u2018seq\u2019 columns have the same raw-read counts, which could be used for the subsequent differential expression analysis. b. \u2018MiRDeep2 Quantifier on A549 repB (html report)\u2019 show the same read-count table with links to detailed information.","title":"3. Inspect the quantification results."},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-6-generate-a-single-read-count-matrix","text":"In this section we will merge the individual quantification results into one read-count matrix, which can be used for differential expression analysis. \u2018MiRDeep2 Quantifier\u2019 output data for each of 4 samples will be merged into one big table, and relevant columns will be extracted from the merged table.","title":"Section 6: Generate a single read-count matrix"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#1-merge-read-count-columns-from-four-quantification-results-into-one-table","text":"a. \u2018Text Manipulation\u2019 -> \u2018Paste\u2019. Select \u2018output of MiRDeep2 Quantifier on Hela repA\u2019 for the first drop-down box, and \u2018output of MiRDeep2 Quantifier on Hela repB\u2019 for the second drop-down box. b. Repeat the \u2018Paste\u2019 with the previous \u2018Paste\u2019 result and another quantification output: \u2018Paste on data 39 and data 37\u2019 for the first drop-box\u2019 and \u2018output of MiRDeep2 Quantifier on A549 repA\u2019 c. Repeat \u2018b\u2019 for \u2018output of MiRDeep2 Quantifier on A549 repB\u2019 The result is a table of 24 columns, because 4 tables of 6 columns were merged side-by-side.","title":"1. Merge read-count columns from four quantification results into one table."},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#2-extract-the-read-count-matrix","text":"The merged table from the previous step has lots of redundant columns. So, we\u2019ll have to extract only read-count columns. Also, miRNA IDs are not appropriate for unique identifier, because there could be multiple copies of precursor miRNAs originating the same miRNAs. For example, \u2018hsa-let-7a-5p\u2019 appears 3 times in the table, because there are 3 copies of \u2018hsa-let-7a\u2019 (hsa-let-7a-1, hsa-let-7a-2 and hsa-let-7a-3). So, we\u2019ll combine the precursor ID and mature miRNA ID with \u2018.\u2019 in-between. a. \u2018Text Manipulation\u2019 -> \u2018Add column\u2019 \u2018Add this value\u2019: \u2018.\u2019 \u2018to Dataset\u2019: \u201838: Paste on data 34 and data 37\u2019 The resulting table has a column on \u2018.\u2019 at the right-most end (column 25). b. \u2018Merge Columns\u2019-> \u2018+Insert Columns\u2019 button (we\u2019re merging 3 columns). \u2018Select data\u2019: result of the previous step (e.g., \u201848: Add column on data 47\u2019) \u2018Merge column\u2019: \u2018column: 3\u2019 (column for \u2018precursor\u2019) \u2018with column\u2019: \u2018column: 25\u2019 (added column in the previous step, containing only \u2018.\u2019) \u2018Add column\u2019: \u2018Column: 1\u2019 (column for \u2018mature miRNA\u2019) c. \u2018Cut\u2019 Input \u2018c26,c2,c8,c14,c20\u2019 \u2018c26\u2019 (Column 26) is the newly added column containing the new ID, which is precursor name followed by \u2018.\u2019 and mature miRNA name. c2, c8, c14 and c20 are columns for read-counts. d. The resulting table has only the new IDs and four read-count columns. e. The merged table include all known miRNAs in the reference data. But they aren\u2019t always expressed. We\u2019re keeping in miRNAs that were expressed at least in one sample. \u2018Filter and Sort\u2019 -> \u2018Filter\u2019 Insert \u2018(c2+c3+c4+c5) > 0\u2019 in \u2018With following condition\u2019","title":"2. Extract the read-count matrix"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#3-add-column-names-to-the-filtered-read-count-matrix","text":"The filtered table has no column names. The easiest way to add them in is downloading the table file, adding the header using a text editor and re-uploading the new file. a. Download the filtered table by clicking on \u2018Download\u2019 icon. b. Use any text editor to add the header line as below: \u2018precursor.miRNA Hela_repA Hela_repB A549_repA A549_repB\u2019 Remember to use \u2018tab\u2019 as separator c. Save and upload the edited table file \u2018Get Data\u2019 -> \u2018Upload File\u2019 d. Rename the uploaded table to \u2018GSE56862 miRNAs read-counts\u2019","title":"3. Add column names to the filtered read-count matrix"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-7-an-alternative-way-to-generate-a-single-read-count-matrix-optional","text":"In this section we will use a spreadsheet software and a text editor on your local computer to generate a single read-count matrix. Depending on your familiarity to those software, you may find it much easier to achieve the same thing that we did in the Section 6.","title":"Section 7: An alternative way to generate a single read-count matrix (optional)"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#section-8-basic-differential-expression-analysis","text":"In this section we will try a differential expression analysis on the miRNAs. Now, we have miRNA expression data from a subset of GSE56862 dataset. The miRNA expression data were now merged into a single read-count table, which can be analyzed by any count-based differential expression analysis tools. In this tutorial, we will use \u2018edgeR\u2019. a. \u2018NGA: RNA Analysis\u2019 -> \u2018edgeR\u2019 \u2018Single Count Matrix\u2019 \u2018Factor Name\u2019: \u2018cell_line\u2019 \u2018Groups\u2019: HeLa,HeLa,A549,A549 \u2018Contrast of Interest\u2019: HeLa-A549 b. Before the execution, we can choose to obtain the normalised counts, R scripts and R dataset from this edgeR analysis. c. \u2018edgeR\u2019 report d. Summary of differential expression test for each miRNA","title":"Section 8: Basic differential expression analysis"},{"location":"tutorials/miRNAseq_basic/intro_miRNAseq/#additional-reading-optional","text":"Once differentially expressed miRNAs are identified, one of the common subsequent analyses is looking for the (potential) target genes, which are the genes that are regulated by miRNAs. Target gene analysis tools include miRanda and TargetScan among many. For example, \u2018miRanda\u2019 takes in a list of miRNAs of interest and retrieves the list of genes that have the target sites. Two miRNAs \u201clet-7e\u201d and \u201cmir-1\u201d were among the differentially expressed miRNAs in this tutorial. Searching for the target sites of these two miRNAs retrieved thousands of genes as below.","title":"Additional reading (optional)"},{"location":"tutorials/molecular_dynamics_101/","text":"PR reviewers and advice: Thomas Coudrat Current slides: https://drive.google.com/open?id=1tm2UjKIBikFb9daYBI0z_53Agh4ktCt9lyNr0SH6GrY Other slides: None yet","title":"Home"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/","text":"Molecular Dynamics Tutorial - Introduction to cluster computing \u00b6 Overview \u00b6 In the following tutorials we will be logging on to a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing (this tutorial) : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD. Learning Objectives \u00b6 At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results Requirements \u00b6 This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select \u2018Software agreement\u2019, \u2018Add software\u2019, \u2018NAMD\u2019. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat. 1 - Overview \u00b6 The aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs. The program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the Theoretical and Computational Biophysics Group at Illinois University at Urbana Champaign . It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output. 2 - Basic introduction to cluster computing \u00b6 a) Logging in to the cluster \u00b6 Using a terminal on your local computer and your username and password, login to the HPC cluster. 1 ssh < username > @snowy . vlsci . unimelb . edu . au You should see a welcome screen and a command line prompt. If you type ls at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet! Note : be careful to use the right terminal when you are typing in commands! Sometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example: [ <username>@snowy ~ ] $ <- tells you your terminal is on Snowy b) Copy across files, starting the job \u00b6 We\u2019ll need to copy across the basic example directory to our working directory on Snowy . Do this with: 1 cp - r / vlsci / examples / namd / Namd_simple_example_01 . Note that the dot is important! Change into this directory and launch the job with the command sbatch and the sbatch script. 1 cd Namd_simple_example_01 Then type: 1 sbatch sbatch_namd_start_job Your job has now been submitted to the cluster. Easy hey? Check the job is running with the showq command. (type it at the command line). Too much information? Try: 1 showq - u < username > This particular job is very short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the .dcd suffix. c) Understanding the input files \u00b6 While we wait, let\u2019s take a look at the sbatch example script to understand what is going on. Type: 1 less sbatch_namd_start_job less is a Unix file viewer. Press \u201cq\u201d to quit the viewing of the file You should see the lines: 1 # SBATCH --nodes=4 This line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC. Now let us have a look at the NAMD configuration script: 1 less namd_1ubq_example . conf Woah! There is quite a bit of information here, don\u2019t worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top: 1 2 3 structure 1 ubq_example . psf coordinates 1 ubq_example . pdb outputName 1 ubq_output_01 These are simply defining the input files, (the protein structure file .psf , and the coordinate file, .pdb ) and also the name of the output files. Further down you will see: 1 2 set Temp 310 temperature $ Temp Which is setting the temperature to 310 K (37 C) while below that we have: 1 2 3 4 ## Parameter file paraTypeCharmm on parameters par_all27_prot_na . prm which tells NAMD which parameter file to use. (you\u2019ll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file. Somewhere in the middle you will see these lines: 1 2 3 4 5 6 7 8 ## Periodic Boundary Conditions cellBasisVector1 48 . 0 . 0 . cellBasisVector2 0 . 48 . 0 . cellBasisVector3 0 . 0 . 48 . cellOrigin 0 0 0 wrapAll on wrapWater on This defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other. Near the bottom we have the lines: 1 2 3 4 5 6 7 8 ## Output files restartfreq 5000 dcdfreq 100 xstFreq 100 outputEnergies 100 outputPressure 100 outputTiming 100 These lines tell us how often to write out to the output files. The most important is the dcdfreq , (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The .dcd output file can become ridiculously HUGE if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs! The last few line in the configuration file: 1 2 3 4 ## Minimize , reinitialize velocities , run dynamics minimize 500 run 10000 tell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. ( This is a very short example! ). Typically you might set \u201crun\u201d to 10,000,000 or more. Press \u201cq\u201d to quit viewing the configuration file. Check again on the status of your job: 1 showq - u < username > If you don\u2019t see anything it probably means the job has finished. List your directory using the command ls - lrt and you should see something like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [ mike@snowy Namd_simple_example_01 ] $ ls - rlt total 17088 - rw - r --r-- 1 mike VR0021 243622 Dec 8 11:40 par_all27_prot_na.prm - rw - r --r-- 1 mike VR0021 655 Dec 8 11:40 sbatch_namd_start_bluegene - rw - r --r-- 1 mike VR0021 3932 Dec 8 11:40 namd_1ubq_example.conf - rw - r --r-- 1 mike VR0021 814960 Dec 8 11:40 1ubq_example.pdb drwxr - xr - x 2 mike VR0021 512 Dec 8 11 : 40 BUILD_DIR - rw - r --r-- 1 mike VR0021 700 Dec 8 11:40 sbatch_namd_restartjob_bluegene - rw - r --r-- 1 mike VR0021 1182412 Dec 8 11:40 1ubq_example.psf - rw - r --r-- 1 mike VR0021 4508 Dec 8 11:40 namd_1ubq_restart_example.conf - rw - r --r-- 1 mike VR0021 159 Dec 8 11:40 slurm-2746442.out - rw - r --r-- 1 mike VR0021 1371 Dec 8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.coor.old - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.vel.old - rw - r --r-- 1 mike VR0021 216 Dec 8 11:41 1ubq_output.restart.xsc.old - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.coor - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.vel - rw - r --r-- 1 mike VR0021 218 Dec 8 11:42 1ubq_output.restart.xsc - rw - r --r-- 1 mike VR0021 8417 Dec 8 11:42 1ubq_output.xst - rw - r --r-- 1 mike VR0021 13005576 Dec 8 11:42 1ubq_output.dcd - rw - r --r-- 1 mike VR0021 215 Dec 8 11:42 1ubq_output.xsc - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.coor - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.vel - rw - r --r-- 1 mike VR0021 425681 Dec 8 11:43 Namd_1ubq_example_output.txt The highlighted .dcd file is the main output file while the .xsc , .coor , .vel files all have to do with being able to restart the simulation at a later date, while the Namd_1ubq_example_output . txt file contains the text output from the simulation. Congratulations! You have just run a short molecular dynamics simulation on the cluster. Next, we\u2019ll copy that information back to your local computer and use VMD to visualize the results. Now on to part 2, visualizing the results with VMD. 3 - Visualizing NAMD results with VMD \u00b6 In this section you will be using the molecular visualization program VMD to look at the trajectory data of the ubiquitin protein you generated in the first part of the tutorial. If you haven\u2019t already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux). Tip : the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the Snowy cluster using the program ssh . You can usually tell which computer you are logged into by the terminal command line: for example the terminal command line: [ mike@snowy Namd_simple_example_01 ] $ tells me I am logged into Snowy , in the Namd_simple_example_01 directory. Compared to my local terminal command line: mike @axion : ~ $ tells me I am on my local machine (called \u201caxion\u201d in this case). Download the entire NAMD example directory back to your local computer . For example, in Linux you can type in your local computer terminal: (if you see snowy in the command line prompt you are typing in the wrong terminal!) 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : Namd_simple_example_01 . What to do if your simulations didn\u2019t run. If for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal: 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_simple_example_01_finished . You can now start VMD locally and load up the trajectory data. In a new local terminal type: 1 vmd Note : On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer. Two new windows should pop up. The main panel : The display : And the terminal should turn into the console : a) Reading structure data files into VMD \u00b6 The first file you need to read into VMD is the protein structure file, (1ubq_example.psf in this case). The .psf file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information. From the main panel : File \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load You should see nothing in the display, but an entry in the Main panel. Next load the coordinates from the .pdb file. First, select the 1ubq_example.psf entry in the VMD main panel, then: File \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load Now you should have the model in the display that can be moved around with the mouse. This is the initial starting position of the simulation. Next load in the trajectory data into VMD (again, select the entry in the VMD main panel): File \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load This data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the main panel . (Use the speed scroll bar to the left of that button to slow it down). What you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases. From the main panel you can bring up the graphical representations window to play with more rendering types: - try them out! Graphics \u2192 Representations And this conclude the basic tutorial to running a simple job on a cluster. Wasn\u2019t so scary now was it? Please play around with VMD. Once you feel comfortable, try start the next tutorial: Molecular dynamics - Building input files","title":"Molecular Dynamics - Introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#molecular-dynamics-tutorial-introduction-to-cluster-computing","text":"","title":"Molecular Dynamics Tutorial - Introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#overview","text":"In the following tutorials we will be logging on to a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing (this tutorial) : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.","title":"Overview"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#learning-objectives","text":"At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results","title":"Learning Objectives"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#requirements","text":"This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select \u2018Software agreement\u2019, \u2018Add software\u2019, \u2018NAMD\u2019. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat.","title":"Requirements"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#1-overview","text":"The aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs. The program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the Theoretical and Computational Biophysics Group at Illinois University at Urbana Champaign . It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output.","title":"1 - Overview"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#2-basic-introduction-to-cluster-computing","text":"","title":"2 - Basic introduction to cluster computing"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-logging-in-to-the-cluster","text":"Using a terminal on your local computer and your username and password, login to the HPC cluster. 1 ssh < username > @snowy . vlsci . unimelb . edu . au You should see a welcome screen and a command line prompt. If you type ls at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet! Note : be careful to use the right terminal when you are typing in commands! Sometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example: [ <username>@snowy ~ ] $ <- tells you your terminal is on Snowy","title":"a) Logging in to the cluster"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#b-copy-across-files-starting-the-job","text":"We\u2019ll need to copy across the basic example directory to our working directory on Snowy . Do this with: 1 cp - r / vlsci / examples / namd / Namd_simple_example_01 . Note that the dot is important! Change into this directory and launch the job with the command sbatch and the sbatch script. 1 cd Namd_simple_example_01 Then type: 1 sbatch sbatch_namd_start_job Your job has now been submitted to the cluster. Easy hey? Check the job is running with the showq command. (type it at the command line). Too much information? Try: 1 showq - u < username > This particular job is very short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the .dcd suffix.","title":"b) Copy across files, starting the job"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#c-understanding-the-input-files","text":"While we wait, let\u2019s take a look at the sbatch example script to understand what is going on. Type: 1 less sbatch_namd_start_job less is a Unix file viewer. Press \u201cq\u201d to quit the viewing of the file You should see the lines: 1 # SBATCH --nodes=4 This line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC. Now let us have a look at the NAMD configuration script: 1 less namd_1ubq_example . conf Woah! There is quite a bit of information here, don\u2019t worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top: 1 2 3 structure 1 ubq_example . psf coordinates 1 ubq_example . pdb outputName 1 ubq_output_01 These are simply defining the input files, (the protein structure file .psf , and the coordinate file, .pdb ) and also the name of the output files. Further down you will see: 1 2 set Temp 310 temperature $ Temp Which is setting the temperature to 310 K (37 C) while below that we have: 1 2 3 4 ## Parameter file paraTypeCharmm on parameters par_all27_prot_na . prm which tells NAMD which parameter file to use. (you\u2019ll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file. Somewhere in the middle you will see these lines: 1 2 3 4 5 6 7 8 ## Periodic Boundary Conditions cellBasisVector1 48 . 0 . 0 . cellBasisVector2 0 . 48 . 0 . cellBasisVector3 0 . 0 . 48 . cellOrigin 0 0 0 wrapAll on wrapWater on This defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other. Near the bottom we have the lines: 1 2 3 4 5 6 7 8 ## Output files restartfreq 5000 dcdfreq 100 xstFreq 100 outputEnergies 100 outputPressure 100 outputTiming 100 These lines tell us how often to write out to the output files. The most important is the dcdfreq , (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The .dcd output file can become ridiculously HUGE if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs! The last few line in the configuration file: 1 2 3 4 ## Minimize , reinitialize velocities , run dynamics minimize 500 run 10000 tell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. ( This is a very short example! ). Typically you might set \u201crun\u201d to 10,000,000 or more. Press \u201cq\u201d to quit viewing the configuration file. Check again on the status of your job: 1 showq - u < username > If you don\u2019t see anything it probably means the job has finished. List your directory using the command ls - lrt and you should see something like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [ mike@snowy Namd_simple_example_01 ] $ ls - rlt total 17088 - rw - r --r-- 1 mike VR0021 243622 Dec 8 11:40 par_all27_prot_na.prm - rw - r --r-- 1 mike VR0021 655 Dec 8 11:40 sbatch_namd_start_bluegene - rw - r --r-- 1 mike VR0021 3932 Dec 8 11:40 namd_1ubq_example.conf - rw - r --r-- 1 mike VR0021 814960 Dec 8 11:40 1ubq_example.pdb drwxr - xr - x 2 mike VR0021 512 Dec 8 11 : 40 BUILD_DIR - rw - r --r-- 1 mike VR0021 700 Dec 8 11:40 sbatch_namd_restartjob_bluegene - rw - r --r-- 1 mike VR0021 1182412 Dec 8 11:40 1ubq_example.psf - rw - r --r-- 1 mike VR0021 4508 Dec 8 11:40 namd_1ubq_restart_example.conf - rw - r --r-- 1 mike VR0021 159 Dec 8 11:40 slurm-2746442.out - rw - r --r-- 1 mike VR0021 1371 Dec 8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.coor.old - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:41 1ubq_output.restart.vel.old - rw - r --r-- 1 mike VR0021 216 Dec 8 11:41 1ubq_output.restart.xsc.old - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.coor - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.restart.vel - rw - r --r-- 1 mike VR0021 218 Dec 8 11:42 1ubq_output.restart.xsc - rw - r --r-- 1 mike VR0021 8417 Dec 8 11:42 1ubq_output.xst - rw - r --r-- 1 mike VR0021 13005576 Dec 8 11:42 1ubq_output.dcd - rw - r --r-- 1 mike VR0021 215 Dec 8 11:42 1ubq_output.xsc - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.coor - rw - r --r-- 1 mike VR0021 247564 Dec 8 11:42 1ubq_output.vel - rw - r --r-- 1 mike VR0021 425681 Dec 8 11:43 Namd_1ubq_example_output.txt The highlighted .dcd file is the main output file while the .xsc , .coor , .vel files all have to do with being able to restart the simulation at a later date, while the Namd_1ubq_example_output . txt file contains the text output from the simulation. Congratulations! You have just run a short molecular dynamics simulation on the cluster. Next, we\u2019ll copy that information back to your local computer and use VMD to visualize the results. Now on to part 2, visualizing the results with VMD.","title":"c) Understanding the input files"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#3-visualizing-namd-results-with-vmd","text":"In this section you will be using the molecular visualization program VMD to look at the trajectory data of the ubiquitin protein you generated in the first part of the tutorial. If you haven\u2019t already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux). Tip : the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the Snowy cluster using the program ssh . You can usually tell which computer you are logged into by the terminal command line: for example the terminal command line: [ mike@snowy Namd_simple_example_01 ] $ tells me I am logged into Snowy , in the Namd_simple_example_01 directory. Compared to my local terminal command line: mike @axion : ~ $ tells me I am on my local machine (called \u201caxion\u201d in this case). Download the entire NAMD example directory back to your local computer . For example, in Linux you can type in your local computer terminal: (if you see snowy in the command line prompt you are typing in the wrong terminal!) 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : Namd_simple_example_01 . What to do if your simulations didn\u2019t run. If for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal: 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_simple_example_01_finished . You can now start VMD locally and load up the trajectory data. In a new local terminal type: 1 vmd Note : On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer. Two new windows should pop up. The main panel : The display : And the terminal should turn into the console :","title":"3 - Visualizing NAMD results with VMD"},{"location":"tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-reading-structure-data-files-into-vmd","text":"The first file you need to read into VMD is the protein structure file, (1ubq_example.psf in this case). The .psf file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information. From the main panel : File \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load You should see nothing in the display, but an entry in the Main panel. Next load the coordinates from the .pdb file. First, select the 1ubq_example.psf entry in the VMD main panel, then: File \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load Now you should have the model in the display that can be moved around with the mouse. This is the initial starting position of the simulation. Next load in the trajectory data into VMD (again, select the entry in the VMD main panel): File \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load This data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the main panel . (Use the speed scroll bar to the left of that button to slow it down). What you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases. From the main panel you can bring up the graphical representations window to play with more rendering types: - try them out! Graphics \u2192 Representations And this conclude the basic tutorial to running a simple job on a cluster. Wasn\u2019t so scary now was it? Please play around with VMD. Once you feel comfortable, try start the next tutorial: Molecular dynamics - Building input files","title":"a) Reading structure data files into VMD"},{"location":"tutorials/molecular_dynamics_201/","text":"PR reviewers and advice: Thomas Coudrat Current slides: https://drive.google.com/open?id=1cJoL7WI-GHIr2iMFm_R9lXrYXiM4kdTBujzpUJH8x0c Other slides: None yet","title":"Home"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/","text":"Molecular Dynamics Tutorial - Building input files, visualising the trajectory \u00b6 Overview \u00b6 In the following tutorials we will be logging on a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files (this tutorial) : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD. Learning Objectives \u00b6 At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results Requirements \u00b6 This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select \u2018Software agreement\u2019, \u2018Add software\u2019, \u2018NAMD\u2019. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat. 1 - Overview \u00b6 The aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in our introductory molecular dynamics tutorial . Tip : in conjunction with this tutorial there are some excellent NAMD tutorials available that are worth working through. 2 - NAMD overview \u00b6 a) Recap: what we\u2019ve done so far \u00b6 In the previous introductory tutorial we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution. To run a molecular dynamics simulation on a cluster, the minimum files we need are: < filename > . psf - protein structure file . A list of the atoms, masses, charges and connections between atoms. < filename > . pdb - protein database file . The actual starting coordinates of the models. This has to be the same order as the psf file. < filename > . conf - NAMD configuration file . Tells NAMD how to run the job. par_all27_prot_na . par - a parameter file . (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). It is used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms. sbatch_batchfile - a script file to launch the job on the cluster depending on the scheduler used (i.e. PBS or Slurm). Tells the cluster how to run the NAMD job and how many cores to use. In our introductory tutorial all we had to do was launch the job. We will now go through the process of building a NAMD input files from scratch. b) Building NAMD input files overview \u00b6 In order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated! To generate NAMD input files, we will use the psfgen module within VMD, together with pdb and topology files , to generate a new pdb file and psf file. In a flowchart, the process looks something like this: c) The Namd_intermediate_template directory structure \u00b6 Note : one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the introductory tutorial you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect outputs. In the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories. This directory is found on Snowy, under /vlsci/examples/namd/Namd_intermediate_template . It has a particular directory structure as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 / Namd_intermediate_template sbatch_start \u2190 script to start equilibration phase sbatch_production \u2190 script to start production phase sim_opt . conf \u2190 Namd configuration file for optimization sim_production . conf \u2190 Namd configuration file for production run project_plan . txt \u2190 A guide to thinking about your simulation counter . txt \u2190 File for keeping track of job number max_jobnumber . txt \u2190 Defines maximum number of jobs / BUILD_DIR \u2190 this is where we will build our models / Errors \u2190 errors go here / Examples \u2190 find some example files here / InputFiles \u2190 our input files go here / Parameters / JobLog \u2190 details of our running jobs end up here / LastRestart \u2190 If we need to restart a job / OutputFiles \u2190 Our NAMD output goes here / OutputText \u2190 Text output from the job / RestartFiles \u2190 Generic restart files saved here Rather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching: sbatch sbatch_start (don\u2019t do this just yet!) This will launch a job using the sim_opt.conf configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run: sbatch sbatch_production Production runs are designed to run less than 24 hours at a time, at the end of which restart files are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a counter.txt file by 1. Once that reaches the number in the MaxJobNumber.txt file, the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we\u2019d set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure. The script also date stamps and directs the output to the appropriate folders. A complete dcd trajectory can be reconstructed from the ordered files in the /OutputFile directory. More of running a job later. First we need to build input files! 3 - Building a HIV protease model \u00b6 Download a copy of the Namd_intermediate_template to your local computer . We will prepare our files here and then upload them to the cluster once ready. 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_intermediate_template . Note : don\u2019t forget the dot at the end Change into the build directory: 1 cd Namd_intermediate_template / BUILD_DIR We are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the RCSB entry 7hvp . Download the pdb file by clicking on \u201cDownload Files\u201d (right hand corner), PDB Format. On your local computer start VMD and load the newly downloaded 7hvp.pdb file by doing the following in the VMD main panel : File \u2192 New Molecule \u2192 Browse\u2026 \u2192 Load In this structure there are 3 \u201cchains\u201d. Chain A and B are the monomers and chain C is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective. Highlight the protein selection in the VMD main panel and then click: File \u2192 save coordinates In the \u201cselected atoms\u201d box of the save trajectory window type: 1 chain A and protein Save the chain as a pdb file with a sensible name and a pdb extension into BUILD_DIR . (e.g. 7hvp_chainA.pdb) Repeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise. We should now have two pdb files in the /BUILD_DIR which will be the basis of our model building, i.e.: 1 2 7 hvp_chainA . pdb 7 hvp_chainB . pdb We now have to use a text editor to change the build_example script to point to these files. Open the build_example file with a text editor. First thing to look for is that we are calling the right topology files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids). 1 2 package require psfgen topology .. / InputFiles / Parameters / top_all27_prot_na . rtf If we were to be building a model with a lipid bilayer we would need to also include the topology file referring to lipids, i.e.: 1 topology .. / InputFiles / Parameter / top_all27_prot_lipid . rtf We now need to change and add \u201csegment\u201d lines to point to our new pdb chains. Edit: 1 2 segment A { pdb model_chainA . pdb } segment B { pdb model_chainB . pdb } to read: 1 2 segment A { pdb 7 hvp_chainA . pdb } segment B { pdb 7 hvp_chainB . pdb } We also need to change the \u201ccoordpdb\u201d lines to reflect our new chains. Edit: 1 2 coordpdb model_chainA . pdb A coordpdb model_chainB . pdb B to read: 1 2 coordpdb 7 hvp_chainA . pdb A coordpdb 7 hvp_chainB . pdb B Between the \u2018segment\u2019 and \u2018coordpdb\u2019 lines we can apply patches to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won\u2019t make any modifications for this example. Save and close the build_example script. We will now see if we can build the model using VMD from the command line. Type: 1 vmd - dispdev text - e build_example You should see some errors. This is because in the original chains A and B there are some modified alanine residues labeled ABA . Since the residues ABA are not defined in the topology files, vmd psfgen does not know how to build this model. Edit the 7hvp_chainA.pdb and 7hvp_chainB.pdb files and carefully change any occurrence of ABA to ALA . Note : spacing in pdb files is really important so don\u2019t mess it up! Re-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory: 1 2 model_temp_x . psf model_temp_x . pdb Note : here we use a \u201c_x\u201d notation to specify temporary files. Next load these files into VMD. From BUILD_DIR start vmd: 1 vmd model_temp_x . psf model_temp_x . pdb We will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of 80 x 64 x 64 \u00c5. Open the solvation window from the main panel: Extensions \u2192 Modeling \u2192 Add Solvation Box In this window do the following: toggle on the \u201crotate to minimize volume\u201d button. Change the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d untoggle the \u2018use molecular Dimensions\u201d button. In the Box Size add: min: x: -40 y: -32 z: -32 max: x: 40 y: 32 z: 32 click \u201cSolvate\u201d We now should have two new files, solvate.psf and solvate.pdb , the solvated version of your original input. You should also your newly solvated system in the VMD display . Tip : you can quickly hide \u201cmodel_temp_x.psf\u201d and \u201cmodel_temp_x.pdb\u201d from the VMD display by double-clicking on \u2018D\u2019 (Drawn) next to these in the VMD main panel. This helps visualise the solvate.psf system. Now we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window: Extensions \u2192 Modeling \u2192 add ions In the \u201cAutoionize\u201d window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files: ionized.psf and ionized.pdb . These are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command: 1 mv ionized . psf .. / InputFiles / hiv_protease . psf and 1 mv ionized . pdb .. / InputFiles / hiv_protease . pdb You can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy. 4 - Preparing the configuration files \u00b6 By now we have prepared two new input files for a NAMD simulation called hiv_protease.psf and hiv_protease.pdb and placed them in the folder /InputFiles . We now need to make changes to the NAMD configuration files to match our new inputs. In the main directory (Namd_intermediate_template) we have two configuration files and two sbatch files: 1 2 3 4 sim_opt . conf sim_production . conf sbatch_start sbatch_production a) Edit the .conf files \u00b6 Let us first edit the .conf files. Open the sim_opt.conf file. You can see that these two lines match our inputs (change them if you used a different name of the psf and pdb files): 1 2 structure InputFiles / hiv_protease . psf coordinates InputFiles / hiv_protease . pdb The next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine: 1 2 paraTypeCharmm on parameters InputFiles / Parameters / par_all27_prot_na . prm If we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example: 1 2 parameters InputFiles / Parameters / par_all27_prot_lipid . prm parameters InputFiles / Parameters / par_for_ligand . prm We also need to make changes to match our periodic boundary conditions (PBC). The way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn\u2019t interact with itself should it wander too close to a boundary. Since our box ended up being of dimensions 80 x 64 x 64 \u00c5, this is reflected in the PBC parameters here: 1 2 3 4 cellBasisVector1 80 . 0 . 0 . cellBasisVector2 0 . 64 . 0 . cellBasisVector3 0 . 0 . 64 . cellOrigin 0 0 0 That should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase. The sim_production.conf file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file MaxJobNumber.txt determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time. Open the sim_production.conf file: Since we would like to run a relatively short job segment for this exercise, we will change the line: 1 set NumberSteps 2500 to: 1 set NumberSteps 20000 This segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation. Edit the counter.txt file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use: 1 echo 0 > counter . txt Then edit the MaxJobNumber.txt file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value. 1 echo 5 > MaxJobNumber . txt We can always make this number bigger later and restart the jobs if we want things to go longer. For this short example we will also change more lines in the sim_production.conf file: 1 2 3 4 5 6 restartfreq 2500 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 to: 1 2 3 4 5 6 restartfreq 25000 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 We don\u2019t have to change the periodic boundary conditions in the sim_production.conf file as we read in the restart files from the previous simulation namely: 1 2 3 4 set inputname generic_restartfile binCoordinates $ inputname . coor ; # Coordinates from last run ( binary ) binVelocities $ inputname . vel ; # Velocities from last run ( binary ) extendedSystem $ inputname . xsc ; # Cell dimensions from last run There are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don\u2019t change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values. Save and close your .conf files. b) Edit the sbatch scripts \u00b6 The sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated than the ones we saw in the introductory tutorial , but most of the details you need to worry about are all in the first few lines. In a sbatch script we need to pass arguments to the Slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line: # SBATCH --nodes=2 \u2190 this works! # SBATCH --nodes=2 \u2190 this doesn\u2019t because of the space between \u201c#\u201d and \u201cSBATCH\u201d Note : people often get confused with this as the \u201c # \u201d symbol usually denotes a comment line. PBS scripts work in a similar way, but with the code word \u201c #PBS \u201d Set the number of nodes used for a job on sbatch_start and sbatch_production to 4, as shown below: 1 # SBATCH --nodes=4 Remember, more nodes is not necessarily faster and can be dramatically slower! It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine. To set the production job runtime change this line on sbatch_production : # SBATCH --time=2:0:0 \u2190 (hours:minutes:seconds) The time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus 10%. If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster (but make sure your walltime is appropriate for your configuration file!). 5 - Launching the job on the cluster \u00b6 We are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make. Upload the entire directory to your account. Under Linux this might be: 1 scp - r Namd_intermediate_template < username > @snowy . vlsci . unimelb . edu . au : Log into your account on Snowy and change into the top of the Namd_intermediate_template/ directory: 1 ssh < username > @snowy . vlsci . unimelb . edu . au Launch the start script: 1 sbatch sbatch_start This should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase. This might take an hour or two to complete . All text output is directed to the /OutputText folder. You can take a peek at how your job is going by using the command tail < filename > which prints out the last few lines of < filename > . For the purpose of this exercise , we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file. It will look something like this: 1 slurm - 123456 . out The number represents the job number on the cluster. Now use scancel to stop that job (i.e. for above you would use: scancel 123456 ). 1 scancel < jobnumber > Now that your job has finished, we will copy across a completed job run. From your top directory on Snowy: 1 cp - r / vlsci / examples / namd / Namd_intermediate_template_finished /* Namd_intermediate_template/ Once the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our local computer for analysis. If you don\u2019t have much memory on your laptop, you can do the analysis remotely on the cluster. A smart way to copy files back to your local computer is to use rsync . This way you only copy new or changed files back to your computer. In Linux from the local computer terminal this would be: 1 rsync - avzt < username > @snowy . vlsci . unimelb . edu . au : Namd_intermediate_template . Note : the dot is important! Now that you have your data, we are ready to visualize the results. 6 - Visualization of the MD trajectory \u00b6 Hopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local computer. We will now have a look at the data you generated. Note : if for some reason you didn\u2019t manage to run a successful MD simulation, you can copy a directory containing the precomputed data from the folowing Snowy folder: /vlsci/examples/namd/Namd_intermediate_template_finished . You can do this with the command below: 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_intermediate_template_finished . a) Combining the trajectory files \u00b6 When we run segmented jobs as in this template, we end up with a series of output files in /OutputFiles such as: 1 2 3 4 5 6 7 8 9 10 11 [ train01@snowy OutputFiles ] $ ls - lrt total 13184 - rwxr - xr - x 1 train01 TRAINING 1477 Mar 21 10 : 18 create_dcd_loader_script - rw - r --r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd - rw - r --r-- 1 train01 TRAINING 195 Mar 21 11:15 dcd_list.txt - rw - r --r-- 1 train01 TRAINING 742 Mar 21 11:15 combined_dcd_file_loader.vmd The main output files have the .dcd extension. We can see that things went well as the sizes of these files are identical as expected. If you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run: 1 . / create_dcd_loader_script This creates the file: combined_dcd_file_loader.vmd From the main directory on your local computer, we can load in our trajectory using: 1 vmd InputFiles / hiv_protease . psf InputFiles / hiv_protease . pdb then from the main panel: File \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd Click on the \u201cplay\u201d button at the bottom right hand corner of the VMD main panel and watch the simulation run! Note : it is possible to restart the simulations of any segment as the restart files are saved under /RestartFiles . b) Molecular dynamics trajectory smoothing \u00b6 The MD example presented here has not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult. Luckily, there is an easy way to center and visualize our simulations which we will cover next. Now display only the protein backbone, in the VMD main panel: Graphics \u2192 Representations\u2026 In the graphical representations window: Selected Atoms (protein) + Drawing method (NewRibbons) You may notice the protein jiggles around when you play the trajectory. This is Brownian motion, and this is more prominent in longer sampling. The first thing we might try to ease the jiggling is to increase the trajectory smoothing window size . In the VMD Graphical representations window, select your protein representation and toggle the Trajectory tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values. Although this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box. c) Centering the protein for analysis \u00b6 We will now use the RMSD Trajectory Tool to center our protein frames. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 RMSD Trajectory Tool This should open up a new window. Towards the top left we have the selection, for the \u2018Selection modifiers\u2019 tick \u2018Backbone\u2019. In the top right, click \u201cRMSD\u201d . When you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much your selection drifts through space. At this point nothing has changed in the trajectory yet. Next, click the \u201cALIGN\u201d button. This will translate each frame to minimize the RMSD of the selection based on the first frame (in this case, our original input files). In other words, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis. Click \u201cRMSD\u201d again and you\u2019ll see the value becomes much smaller. d) Using Volmap to map ligand density. \u00b6 Now that we have a nicely centered protein dataset we can do something useful like plot the water density. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 VolMap Tool A new VolMap window should open up. In the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d, click \u201cCreate Map\u201d. This will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical representation window and select the new \u201cIsosurface\u201d representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points (try 1.2). What you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don\u2019t display waters in simulations for clarity, and often forget that they are there. If all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation. Exercise : see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail. You can also do this sort of view for ligands to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense! So concludes the intermediate tutorial. Note : a more advanced template that can be used to organise the MD simulations ran on HPC clusters called MD_workflow_py was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.","title":"Molecular Dynamics - Building input files, visualising the trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#molecular-dynamics-tutorial-building-input-files-visualising-the-trajectory","text":"","title":"Molecular Dynamics Tutorial - Building input files, visualising the trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#overview","text":"In the following tutorials we will be logging on a high performance computer (HPC) to submit NAMD molecular dynamics (MD) jobs and visualising the results with the molecular visualization program VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple. MD tutorial - Introduction to cluster computing : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD. MD tutorial - Building input files (this tutorial) : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.","title":"Overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#learning-objectives","text":"At the end of the course, you will be able to: Log onto a high performance computer cluster Upload files to the cluster Submit a basic molecular dynamics job on the cluster Download the output and visualise the preliminary results","title":"Learning Objectives"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#requirements","text":"This workshop is intended for scientists interested in learning the basics of running molecular dynamics on a HPC cluster. It is recommended that participants attend Introduction to High Performance Computing prior to this workshop. If you are not familiar with using the command line, please also attend the Introduction to Unix workshop. Please bring along your laptops with the following installed before arriving: VMD ( http://www.ks.uiuc.edu/Research/vmd/ ) Putty ( https://www.putty.org/ ) \u2013 only if you use Windows. Important : NAMD has specific licencing requirements. Users are required to agree to these requirements to use NAMD on the Melbourne Bioinformatics HPC clusters. Login on https://my.vlsci.org.au/ , select \u2018Software agreement\u2019, \u2018Add software\u2019, \u2018NAMD\u2019. Note : this tutorial describes the use of Snowy, but all these steps can be carried out on Barcoo by substituting every instance of snowy for barcoo . Tutorials created by Mike Kuiper, edited by Thomas Coudrat.","title":"Requirements"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#1-overview","text":"The aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in our introductory molecular dynamics tutorial . Tip : in conjunction with this tutorial there are some excellent NAMD tutorials available that are worth working through.","title":"1 - Overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#2-namd-overview","text":"","title":"2 - NAMD overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-recap-what-weve-done-so-far","text":"In the previous introductory tutorial we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution. To run a molecular dynamics simulation on a cluster, the minimum files we need are: < filename > . psf - protein structure file . A list of the atoms, masses, charges and connections between atoms. < filename > . pdb - protein database file . The actual starting coordinates of the models. This has to be the same order as the psf file. < filename > . conf - NAMD configuration file . Tells NAMD how to run the job. par_all27_prot_na . par - a parameter file . (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). It is used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms. sbatch_batchfile - a script file to launch the job on the cluster depending on the scheduler used (i.e. PBS or Slurm). Tells the cluster how to run the NAMD job and how many cores to use. In our introductory tutorial all we had to do was launch the job. We will now go through the process of building a NAMD input files from scratch.","title":"a) Recap: what we've done so far"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-building-namd-input-files-overview","text":"In order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated! To generate NAMD input files, we will use the psfgen module within VMD, together with pdb and topology files , to generate a new pdb file and psf file. In a flowchart, the process looks something like this:","title":"b) Building NAMD input files overview"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#c-the-namd_intermediate_template-directory-structure","text":"Note : one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the introductory tutorial you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect outputs. In the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories. This directory is found on Snowy, under /vlsci/examples/namd/Namd_intermediate_template . It has a particular directory structure as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 / Namd_intermediate_template sbatch_start \u2190 script to start equilibration phase sbatch_production \u2190 script to start production phase sim_opt . conf \u2190 Namd configuration file for optimization sim_production . conf \u2190 Namd configuration file for production run project_plan . txt \u2190 A guide to thinking about your simulation counter . txt \u2190 File for keeping track of job number max_jobnumber . txt \u2190 Defines maximum number of jobs / BUILD_DIR \u2190 this is where we will build our models / Errors \u2190 errors go here / Examples \u2190 find some example files here / InputFiles \u2190 our input files go here / Parameters / JobLog \u2190 details of our running jobs end up here / LastRestart \u2190 If we need to restart a job / OutputFiles \u2190 Our NAMD output goes here / OutputText \u2190 Text output from the job / RestartFiles \u2190 Generic restart files saved here Rather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching: sbatch sbatch_start (don\u2019t do this just yet!) This will launch a job using the sim_opt.conf configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run: sbatch sbatch_production Production runs are designed to run less than 24 hours at a time, at the end of which restart files are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a counter.txt file by 1. Once that reaches the number in the MaxJobNumber.txt file, the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we\u2019d set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure. The script also date stamps and directs the output to the appropriate folders. A complete dcd trajectory can be reconstructed from the ordered files in the /OutputFile directory. More of running a job later. First we need to build input files!","title":"c) The Namd_intermediate_template directory structure"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#3-building-a-hiv-protease-model","text":"Download a copy of the Namd_intermediate_template to your local computer . We will prepare our files here and then upload them to the cluster once ready. 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_intermediate_template . Note : don\u2019t forget the dot at the end Change into the build directory: 1 cd Namd_intermediate_template / BUILD_DIR We are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the RCSB entry 7hvp . Download the pdb file by clicking on \u201cDownload Files\u201d (right hand corner), PDB Format. On your local computer start VMD and load the newly downloaded 7hvp.pdb file by doing the following in the VMD main panel : File \u2192 New Molecule \u2192 Browse\u2026 \u2192 Load In this structure there are 3 \u201cchains\u201d. Chain A and B are the monomers and chain C is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective. Highlight the protein selection in the VMD main panel and then click: File \u2192 save coordinates In the \u201cselected atoms\u201d box of the save trajectory window type: 1 chain A and protein Save the chain as a pdb file with a sensible name and a pdb extension into BUILD_DIR . (e.g. 7hvp_chainA.pdb) Repeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise. We should now have two pdb files in the /BUILD_DIR which will be the basis of our model building, i.e.: 1 2 7 hvp_chainA . pdb 7 hvp_chainB . pdb We now have to use a text editor to change the build_example script to point to these files. Open the build_example file with a text editor. First thing to look for is that we are calling the right topology files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids). 1 2 package require psfgen topology .. / InputFiles / Parameters / top_all27_prot_na . rtf If we were to be building a model with a lipid bilayer we would need to also include the topology file referring to lipids, i.e.: 1 topology .. / InputFiles / Parameter / top_all27_prot_lipid . rtf We now need to change and add \u201csegment\u201d lines to point to our new pdb chains. Edit: 1 2 segment A { pdb model_chainA . pdb } segment B { pdb model_chainB . pdb } to read: 1 2 segment A { pdb 7 hvp_chainA . pdb } segment B { pdb 7 hvp_chainB . pdb } We also need to change the \u201ccoordpdb\u201d lines to reflect our new chains. Edit: 1 2 coordpdb model_chainA . pdb A coordpdb model_chainB . pdb B to read: 1 2 coordpdb 7 hvp_chainA . pdb A coordpdb 7 hvp_chainB . pdb B Between the \u2018segment\u2019 and \u2018coordpdb\u2019 lines we can apply patches to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won\u2019t make any modifications for this example. Save and close the build_example script. We will now see if we can build the model using VMD from the command line. Type: 1 vmd - dispdev text - e build_example You should see some errors. This is because in the original chains A and B there are some modified alanine residues labeled ABA . Since the residues ABA are not defined in the topology files, vmd psfgen does not know how to build this model. Edit the 7hvp_chainA.pdb and 7hvp_chainB.pdb files and carefully change any occurrence of ABA to ALA . Note : spacing in pdb files is really important so don\u2019t mess it up! Re-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory: 1 2 model_temp_x . psf model_temp_x . pdb Note : here we use a \u201c_x\u201d notation to specify temporary files. Next load these files into VMD. From BUILD_DIR start vmd: 1 vmd model_temp_x . psf model_temp_x . pdb We will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of 80 x 64 x 64 \u00c5. Open the solvation window from the main panel: Extensions \u2192 Modeling \u2192 Add Solvation Box In this window do the following: toggle on the \u201crotate to minimize volume\u201d button. Change the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d untoggle the \u2018use molecular Dimensions\u201d button. In the Box Size add: min: x: -40 y: -32 z: -32 max: x: 40 y: 32 z: 32 click \u201cSolvate\u201d We now should have two new files, solvate.psf and solvate.pdb , the solvated version of your original input. You should also your newly solvated system in the VMD display . Tip : you can quickly hide \u201cmodel_temp_x.psf\u201d and \u201cmodel_temp_x.pdb\u201d from the VMD display by double-clicking on \u2018D\u2019 (Drawn) next to these in the VMD main panel. This helps visualise the solvate.psf system. Now we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window: Extensions \u2192 Modeling \u2192 add ions In the \u201cAutoionize\u201d window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files: ionized.psf and ionized.pdb . These are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command: 1 mv ionized . psf .. / InputFiles / hiv_protease . psf and 1 mv ionized . pdb .. / InputFiles / hiv_protease . pdb You can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy.","title":"3 - Building a HIV protease model"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#4-preparing-the-configuration-files","text":"By now we have prepared two new input files for a NAMD simulation called hiv_protease.psf and hiv_protease.pdb and placed them in the folder /InputFiles . We now need to make changes to the NAMD configuration files to match our new inputs. In the main directory (Namd_intermediate_template) we have two configuration files and two sbatch files: 1 2 3 4 sim_opt . conf sim_production . conf sbatch_start sbatch_production","title":"4 - Preparing the configuration files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-edit-the-conf-files","text":"Let us first edit the .conf files. Open the sim_opt.conf file. You can see that these two lines match our inputs (change them if you used a different name of the psf and pdb files): 1 2 structure InputFiles / hiv_protease . psf coordinates InputFiles / hiv_protease . pdb The next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine: 1 2 paraTypeCharmm on parameters InputFiles / Parameters / par_all27_prot_na . prm If we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example: 1 2 parameters InputFiles / Parameters / par_all27_prot_lipid . prm parameters InputFiles / Parameters / par_for_ligand . prm We also need to make changes to match our periodic boundary conditions (PBC). The way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn\u2019t interact with itself should it wander too close to a boundary. Since our box ended up being of dimensions 80 x 64 x 64 \u00c5, this is reflected in the PBC parameters here: 1 2 3 4 cellBasisVector1 80 . 0 . 0 . cellBasisVector2 0 . 64 . 0 . cellBasisVector3 0 . 0 . 64 . cellOrigin 0 0 0 That should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase. The sim_production.conf file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file MaxJobNumber.txt determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time. Open the sim_production.conf file: Since we would like to run a relatively short job segment for this exercise, we will change the line: 1 set NumberSteps 2500 to: 1 set NumberSteps 20000 This segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation. Edit the counter.txt file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use: 1 echo 0 > counter . txt Then edit the MaxJobNumber.txt file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value. 1 echo 5 > MaxJobNumber . txt We can always make this number bigger later and restart the jobs if we want things to go longer. For this short example we will also change more lines in the sim_production.conf file: 1 2 3 4 5 6 restartfreq 2500 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 to: 1 2 3 4 5 6 restartfreq 25000 dcdfreq 5000 xstFreq 5000 outputEnergies 5000 outputPressure 5000 outputTiming 5000 We don\u2019t have to change the periodic boundary conditions in the sim_production.conf file as we read in the restart files from the previous simulation namely: 1 2 3 4 set inputname generic_restartfile binCoordinates $ inputname . coor ; # Coordinates from last run ( binary ) binVelocities $ inputname . vel ; # Velocities from last run ( binary ) extendedSystem $ inputname . xsc ; # Cell dimensions from last run There are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don\u2019t change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values. Save and close your .conf files.","title":"a) Edit the .conf files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-edit-the-sbatch-scripts","text":"The sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated than the ones we saw in the introductory tutorial , but most of the details you need to worry about are all in the first few lines. In a sbatch script we need to pass arguments to the Slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line: # SBATCH --nodes=2 \u2190 this works! # SBATCH --nodes=2 \u2190 this doesn\u2019t because of the space between \u201c#\u201d and \u201cSBATCH\u201d Note : people often get confused with this as the \u201c # \u201d symbol usually denotes a comment line. PBS scripts work in a similar way, but with the code word \u201c #PBS \u201d Set the number of nodes used for a job on sbatch_start and sbatch_production to 4, as shown below: 1 # SBATCH --nodes=4 Remember, more nodes is not necessarily faster and can be dramatically slower! It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine. To set the production job runtime change this line on sbatch_production : # SBATCH --time=2:0:0 \u2190 (hours:minutes:seconds) The time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus 10%. If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster (but make sure your walltime is appropriate for your configuration file!).","title":"b) Edit the sbatch scripts"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#5-launching-the-job-on-the-cluster","text":"We are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make. Upload the entire directory to your account. Under Linux this might be: 1 scp - r Namd_intermediate_template < username > @snowy . vlsci . unimelb . edu . au : Log into your account on Snowy and change into the top of the Namd_intermediate_template/ directory: 1 ssh < username > @snowy . vlsci . unimelb . edu . au Launch the start script: 1 sbatch sbatch_start This should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase. This might take an hour or two to complete . All text output is directed to the /OutputText folder. You can take a peek at how your job is going by using the command tail < filename > which prints out the last few lines of < filename > . For the purpose of this exercise , we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file. It will look something like this: 1 slurm - 123456 . out The number represents the job number on the cluster. Now use scancel to stop that job (i.e. for above you would use: scancel 123456 ). 1 scancel < jobnumber > Now that your job has finished, we will copy across a completed job run. From your top directory on Snowy: 1 cp - r / vlsci / examples / namd / Namd_intermediate_template_finished /* Namd_intermediate_template/ Once the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our local computer for analysis. If you don\u2019t have much memory on your laptop, you can do the analysis remotely on the cluster. A smart way to copy files back to your local computer is to use rsync . This way you only copy new or changed files back to your computer. In Linux from the local computer terminal this would be: 1 rsync - avzt < username > @snowy . vlsci . unimelb . edu . au : Namd_intermediate_template . Note : the dot is important! Now that you have your data, we are ready to visualize the results.","title":"5 - Launching the job on the cluster"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#6-visualization-of-the-md-trajectory","text":"Hopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local computer. We will now have a look at the data you generated. Note : if for some reason you didn\u2019t manage to run a successful MD simulation, you can copy a directory containing the precomputed data from the folowing Snowy folder: /vlsci/examples/namd/Namd_intermediate_template_finished . You can do this with the command below: 1 scp - r < username > @snowy . vlsci . unimelb . edu . au : / vlsci / examples / namd / Namd_intermediate_template_finished .","title":"6 - Visualization of the MD trajectory"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-combining-the-trajectory-files","text":"When we run segmented jobs as in this template, we end up with a series of output files in /OutputFiles such as: 1 2 3 4 5 6 7 8 9 10 11 [ train01@snowy OutputFiles ] $ ls - lrt total 13184 - rwxr - xr - x 1 train01 TRAINING 1477 Mar 21 10 : 18 create_dcd_loader_script - rw - r --r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd - rw - r --r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd - rw - r --r-- 1 train01 TRAINING 195 Mar 21 11:15 dcd_list.txt - rw - r --r-- 1 train01 TRAINING 742 Mar 21 11:15 combined_dcd_file_loader.vmd The main output files have the .dcd extension. We can see that things went well as the sizes of these files are identical as expected. If you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run: 1 . / create_dcd_loader_script This creates the file: combined_dcd_file_loader.vmd From the main directory on your local computer, we can load in our trajectory using: 1 vmd InputFiles / hiv_protease . psf InputFiles / hiv_protease . pdb then from the main panel: File \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd Click on the \u201cplay\u201d button at the bottom right hand corner of the VMD main panel and watch the simulation run! Note : it is possible to restart the simulations of any segment as the restart files are saved under /RestartFiles .","title":"a) Combining the trajectory files"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-molecular-dynamics-trajectory-smoothing","text":"The MD example presented here has not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult. Luckily, there is an easy way to center and visualize our simulations which we will cover next. Now display only the protein backbone, in the VMD main panel: Graphics \u2192 Representations\u2026 In the graphical representations window: Selected Atoms (protein) + Drawing method (NewRibbons) You may notice the protein jiggles around when you play the trajectory. This is Brownian motion, and this is more prominent in longer sampling. The first thing we might try to ease the jiggling is to increase the trajectory smoothing window size . In the VMD Graphical representations window, select your protein representation and toggle the Trajectory tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values. Although this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box.","title":"b) Molecular dynamics trajectory smoothing"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#c-centering-the-protein-for-analysis","text":"We will now use the RMSD Trajectory Tool to center our protein frames. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 RMSD Trajectory Tool This should open up a new window. Towards the top left we have the selection, for the \u2018Selection modifiers\u2019 tick \u2018Backbone\u2019. In the top right, click \u201cRMSD\u201d . When you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much your selection drifts through space. At this point nothing has changed in the trajectory yet. Next, click the \u201cALIGN\u201d button. This will translate each frame to minimize the RMSD of the selection based on the first frame (in this case, our original input files). In other words, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis. Click \u201cRMSD\u201d again and you\u2019ll see the value becomes much smaller.","title":"c) Centering the protein for analysis"},{"location":"tutorials/molecular_dynamics_201/molecular_dynamics_201/#d-using-volmap-to-map-ligand-density","text":"Now that we have a nicely centered protein dataset we can do something useful like plot the water density. In the VMD main panel, open: Extensions \u2192 Analysis \u2192 VolMap Tool A new VolMap window should open up. In the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d, click \u201cCreate Map\u201d. This will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical representation window and select the new \u201cIsosurface\u201d representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points (try 1.2). What you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don\u2019t display waters in simulations for clarity, and often forget that they are there. If all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation. Exercise : see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail. You can also do this sort of view for ligands to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense! So concludes the intermediate tutorial. Note : a more advanced template that can be used to organise the MD simulations ran on HPC clusters called MD_workflow_py was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.","title":"d) Using Volmap to map ligand density."},{"location":"tutorials/ngs_overview/","text":"PR reviewers and advice: Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/ngs_overview/NGS_Overview/","text":"Next Generation Sequencing Overview \u00b6 Sequencing technologies \u00b6 Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction) Applications \u00b6 Genomic \u00b6 Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a \u2018metasample\u2019) Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target Epigenomic \u00b6 CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment Transcriptomic (RNA-seq) \u00b6 Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis Types of outcome \u00b6 Variation Detection \u00b6 Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements De Novo assembly \u00b6 Concatemers (length?) Comparative gene expression \u00b6 Matrix of gene vs expression level Splicing detection \u00b6 List of alternative splice isoforms Resequencing alignment strategies \u00b6 Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex) Categories of analysis \u00b6 NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - \u2018base calling\u2019. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis. Sample preparation \u00b6 Fragment \u00b6 Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read Paired-end sequencing \u00b6 Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus \u2018related\u2019 and have extra information about the structure of the sample. The information is that the two reads from the single fragment are \u2018close\u2019 - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn Mate-pair \u00b6 Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn Barcoded fragments \u00b6 Multiple samples are prepared independently. Each sample is \u2018labelled\u2019 with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes. Enrichment for target sequences \u00b6 PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation Potential services in NGS \u00b6 Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data Appendix \u00b6 Paired end sequencing - empirical observations \u00b6 paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, ~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all. Mate-pair sequencing \u00b6 Illumina refers to \u201cpaired end\u201d as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the \u2018internal\u2019 ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard \u2018paired end\u2019 sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"NGS Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#next-generation-sequencing-overview","text":"","title":"Next Generation Sequencing Overview"},{"location":"tutorials/ngs_overview/NGS_Overview/#sequencing-technologies","text":"Sequencing by hybridisation and ligation (Complete Genomics, Life Technologies, Polonator) Emulsion PCR amplification Detection: Cyclic ligation of nucleotide dimers. Multiple rounds required. Sequencing by synthesis (Helicos, Illumina, Intelligent Bio-Sys) Bridge PCR amplification on slides Detection: cyclic addition of reversible polymerisation terminators Real-time sequencing by synthesis (Pacific BioSciences, VisiGen) Single molecule, no amplification Cyclic removal of nucleotide and measure change in conductance of micropore Pyrosequencing (Roche/454\u2122). Emulsion PCR amplification Detection: cyclic addition of single nucleotide species and detection of number of pyrophosphates released during polymerisation (through ATP to luciferase reaction)","title":"Sequencing technologies"},{"location":"tutorials/ngs_overview/NGS_Overview/#applications","text":"","title":"Applications"},{"location":"tutorials/ngs_overview/NGS_Overview/#genomic","text":"Whole genome resequencing for variation detection Whole genome sequencing for de novo assembly Metagenomics (sequencing of multiple genomes together in a \u2018metasample\u2019) Targetted resequencing for variation detection Exome sequencing /targetted exome capture Arbitrary PCR enrichment of target","title":"Genomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#epigenomic","text":"CHiP-Seq through sequencing of DNA enriched by immunoprecipitation DNA methylation analysis Sequencing of DNA enriched through immunoprecipitation of 5-methylcytosine () Methylation-sensitive restriction enzyme fragment enrichment","title":"Epigenomic"},{"location":"tutorials/ngs_overview/NGS_Overview/#transcriptomic-rna-seq","text":"Whole transcriptome expression profiling (comparative gene expression) Whole transcriptome sequencing for de novo assembly (identify genes) Strand-specific mRNA-seq Paired-end RNA-seq Whole transcriptome variation detection Differential splicing detection sRNA analysis","title":"Transcriptomic (RNA-seq)"},{"location":"tutorials/ngs_overview/NGS_Overview/#types-of-outcome","text":"","title":"Types of outcome"},{"location":"tutorials/ngs_overview/NGS_Overview/#variation-detection","text":"Copy number variation (CNV) Single Nucleotide Polymorphisms (SNPs) Insertions and deletions (Indels) Rearrangements","title":"Variation Detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#de-novo-assembly","text":"Concatemers (length?)","title":"De Novo assembly"},{"location":"tutorials/ngs_overview/NGS_Overview/#comparative-gene-expression","text":"Matrix of gene vs expression level","title":"Comparative gene expression"},{"location":"tutorials/ngs_overview/NGS_Overview/#splicing-detection","text":"List of alternative splice isoforms","title":"Splicing detection"},{"location":"tutorials/ngs_overview/NGS_Overview/#resequencing-alignment-strategies","text":"Align to transcriptome (simplest) Align to genome and exon-exon junction sequences (discover new transcripts, splice variants) De novo (most complex)","title":"Resequencing alignment strategies"},{"location":"tutorials/ngs_overview/NGS_Overview/#categories-of-analysis","text":"NGS informatics may be categorised into: 1. Primary analysis: Conversion of imagesets (output from NGS machines) into strings of bases - \u2018base calling\u2019. Inputs: Image files from NGS machine Outputs: Short reads Software: Firecrest and Bustard (for Illumina) 2. Secondary analysis: Assembly of short strings of bases (output of primary analysis) into contigs, and/or -Li Jason 11/18/09 2:43 PM alignment onto a reference genome and extraction of the first level of genomic meaning (SNP, rearrangements) Inputs: Short reads (20-30million/run?) Outputs: Aligned sequences against a reference genome SNP analysis Copy number variation data Quantified expression data Software: E.g. Eland, BWA, MAQ for alignment. Velvet, ABySS for de-novo assembly. References: http://www.illumina.com/Documents/products/technotes/technote_denovo_assembly.pdf 3. Tertiary analysis Extraction of biological information from the aligned sequences. Annotation of sequence data. Inputs: *E.g. Aligned sequences, SNP analysis * Outputs: Biological and statistical interpretations and visualisations At each stage there are multiple options for processing - vendor software, open source software, each with varying levels of accuracy and computational expense. There are also large amounts of data produced at each stage. Additionally, primary and secondary analysis are dependent on the NGS platform being supported - Illumina Solexa pipeline is different to an Applied Biosystems SOLiD pipeline. Anecdotally, 80% of processing in a NGS project is manual and occurs mainly in tertiary analysis.","title":"Categories of analysis"},{"location":"tutorials/ngs_overview/NGS_Overview/#sample-preparation","text":"","title":"Sample preparation"},{"location":"tutorials/ngs_overview/NGS_Overview/#fragment","text":"Each read is independent - an arbitrary fragment of DNA with no meta-information about the relationship of that read to any other read","title":"Fragment"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing","text":"Each fragment is sequenced twice, once from each end, in succession. Probably the reads will not overlap, as the average length of a fragment is normally >2x that of a read. The two reads are thus \u2018related\u2019 and have extra information about the structure of the sample. The information is that the two reads from the single fragment are \u2018close\u2019 - depending on the length of fragments that were prepared for the run. http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn","title":"Paired-end sequencing"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair","text":"Similar to paired-end, but ends of sequenced fragment can be much further apart - up to 10kb (normally 2-5kb) http://www.illumina.com/technology/mate_pair_sequencing_assay.ilmn","title":"Mate-pair"},{"location":"tutorials/ngs_overview/NGS_Overview/#barcoded-fragments","text":"Multiple samples are prepared independently. Each sample is \u2018labelled\u2019 with a unique sequence adaptor (barcode). Samples are then mixed and sequenced together. Individual reads can be categorised to a particular sample by their barcodes.","title":"Barcoded fragments"},{"location":"tutorials/ngs_overview/NGS_Overview/#enrichment-for-target-sequences","text":"PCR Hybridisation to oligo arrays PolyA selection (mRNA-seq) Size selection Precipitation","title":"Enrichment for target sequences"},{"location":"tutorials/ngs_overview/NGS_Overview/#potential-services-in-ngs","text":"Courtesy Colorado State Uni Sequence Matching Analysis Align SOLiD colorspace reads with reference genomes Generate consensus sequences Mate-Pair Analysis Analyze paired reads from SOLiD mate-pair runs Improve accuracy of read matches to detect structural variations > between sample and reference genomes SNP / Indel Analysis Identify single nucleotide polymorphisms Identify insertion and deletion mutations in DNA sequences Copy Number Variation Analysis Detect copy number variations in DNA sequences Whole Transcriptome Analysis Map SOLiD reads from a transcript sample to reference genomes Assign tag counts to features of the reference genome Identify novel transcripts Inversion Analysis Identify paracentric and pericentric chromosomal inversions de Novo Assembly Create de novo assemblies from SOLiD colorspace reads Characterize genomic sequences for which no closely related reference genome exists Assemble SOLiD reads from microbial genomes into nucleotide sequence contigs of several thousand bases, and scaffolds of tens of thousands of bases. Assemble SOLiD reads from microbial genomes into scaffolds of tens of thousands of bases smRNA Analysis Whole genome analysis of SOLiD RNA library reads Includes filtering, matching against miRBase sequences, and matching against reference genomes NextGENe Analysis NextGENe is a comprehensive NGS bioinformatics package NextGENe includes de novo assembly, target assembly, SNP/Indel discovery, digital gene expression analysis, whole transcriptome analysis, ChiPSeq analysis, miRNA discovery and quantification, and a condensation assembly tool for reducing instrument error System Alignment Browser (SAB) SAB is a graphical genome annotation viewer Used for viewing basespace and colorspace reads aligned to reference genomes GFF Conversion Convert SOLiD system mapping files into GFF-format files GFF is a standardized file format for describing DNA, RNA, and proteins SRF Conversion Convert SOLiD system reads into SRF format SRF is a standardized file format for describing DNA sequence data","title":"Potential services in NGS"},{"location":"tutorials/ngs_overview/NGS_Overview/#appendix","text":"","title":"Appendix"},{"location":"tutorials/ngs_overview/NGS_Overview/#paired-end-sequencing-empirical-observations","text":"paired end or mate pair refers to how the library is made, and then how it is sequenced. Both are methodologies that, in addition to the sequence information, give you information about the physical distance between the two reads in your genome. For example, you shear up some genomic DNA, and cut a region out at ~500bp. Then you prepare your library, and sequence 35bp from each end of each molecule. Now you have three pieces of information: the tag 1 sequence the tag 2 sequence* that they were 500bp \u00b1 (some) apart in your genome When we do 2x 50 bp paired-end runs on a GAIIx using the current gel purification step we get read distances of between that vary by about 100 bp in a nice tight bell shaped curve starting between 160-200 bp. So the first thing to bear in mind is that L is not fixed within or between runs. Either way this group accounts for >99.99% of paired-end reads in an assembly. Because of the way fragments are generated for sequencing 1 and 2 can align either F-B of B-F. If you want to be more realistic there are always a tiny proportion of reads <0.1% that align with much longer read distances, some of which is due to bioinformatics but some of which is real and simply reflects biology. Likewise a tiny proportion of reads at all read distances will be F-F or B-B. Also there appear to often be a tiny proportion of reads that come out overlapping where the read distance is the same as a read length ie 1+L+2 is, in this case, ~50-100. I have no idea of the prevelance of such reads but you can often find them if you look. Lastly if its not going to be part of the assembler, end trimming and quality trimming can often mean that 1 and 2 are different lengths and that a substantial number of reads from a paired end run end up with no partner at all.","title":"Paired end sequencing - empirical observations"},{"location":"tutorials/ngs_overview/NGS_Overview/#mate-pair-sequencing","text":"Illumina refers to \u201cpaired end\u201d as the original library preparation method they use, where you sequence each end of the same molecule. Because of the way the cluster generation technology works, it is limited to an inter-pair distance of ~300bp ( 200-600bp). Illumina refers to mate pairs as sequences derived from their newer library prep method which is designed to provide paired sequences separated by a greater distance (between about 2 and 10kb). This method still actually only sequences the ends of ~400bp molecules, but this template is derived from both ends of a 2-10kb fragment that has had the middle section cut out and the \u2018internal\u2019 ends ligated in the middle. Basically, you take your 2-10kb random fragments, biotinylate the end, circularise them, shear the circles to ~400bp, capture biotinylated molecules, and then sequence those (they go into what is essentially a standard \u2018paired end\u2019 sample prep procedure). Source: http://seqanswers.com/forums/showthread.php?t=503","title":"Mate-pair sequencing"},{"location":"tutorials/pacbio/","text":"Long read assembly workshop \u00b6 This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data. Overview \u00b6 Simplified version of workflow: 1. Get started \u00b6 Your workshop trainers will provide you with the address of a virtual machine. Mac users \u00b6 Open the Terminal. Type in 1 ssh researcher @ [ your virtual machine address ] Type in the password provided. Windows users \u00b6 If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under \u201cHost Name (or IP address)\u201d enter in the address of your virtual machine. Under \u201cPort\u201d type in 22 Under \u201cConnection Type\u201d select \u201cSSH\u201d Click \u201cOpen\u201d Under \u201cLogin as:\u201d enter \u201cresearcher\u201d Type in the password provided. Activate the conda environment \u00b6 Type in: 1 source / mnt / gvl / apps / conda / bin / activate (This points us to some different directories for the software we need). Create a new working directory on your remote computer. \u00b6 Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called \u201cWorkshop\u201d 1 mkdir Workshop Change to that directory 1 cd Workshop NOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again. Find your current directory by typing: 1 pwd 2. Get data \u00b6 The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the \u201cDownload\u201d button, and select \u201cCopy link address\u201d. Back in your terminal, enter 1 wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files. Shorten each of these files names with the mv command: 1 2 3 mv R1 . fq \\ ? download \\ = 1 R1 . fq mv R2 . fq \\ ? download \\ = 1 R2 . fq mv pacbio . fq \\ ? download \\ = 1 pacbio . fq Type in ls to check the files are present and correctly-named. We should have R1.fq , R2.fq and pacbio.fq . 3. Assemble \u00b6 We will use the assembly software called Canu , version 1.7. Run Canu with these commands: 1 canu -p canu -d canu_outdir genomeSize=0.03m corThreads=3 -pacbio-raw pacbio.fq the first canu tells the program to run - p canu names prefix for output files (\u201ccanu\u201d) - d canu_outdir names output directory (\u201ccanu_outdir\u201d) genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). corThreads = 3 sets the number of available threads. Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say \u201cFinished\u201d but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: 1 2 3 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row. 4. Check assembly output \u00b6 Move into the canu output folder: 1 cd canu_outdir View the list of files: 1 ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) 1 infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 47997 \u201ctig00000001\u201d is the name given to the contig \u201c47997\u201d is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: 1 less canu . report \u201cless\u201d is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report. Questions \u00b6 How do long- and short-read assembly methods differ? Answer (click to reveal) Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Where can we find out the what the approximate genome size should be for the species being assembled? Answer (click to reveal) Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. In the assembly output, what are the unassembled reads? Answer (click to reveal) Reads and low-coverage contigs that were not used in the assembly. What are the corrected reads? How did canu correct the reads? Answer (click to reveal) Canu builds overlaps between reads. The consensus is used to correct the reads. Where could you view the output .gfa and what would it show? Answer (click to reveal) A useful program is [Bandage](https://rrwick.github.io/Bandage/). If the assembly has multiple contigs, the assembly graph shows how these are connected. 5. Trim and circularise \u00b6 Bacteria have circular chromosomes. Because of sequencing errors, there may be some \u201coverhang\u201d in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: 1 cd .. Run Circlator \u00b6 1 circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir / canu . contigs . fasta is the file path to the input Canu assembly canu_outdir / canu . correctedReads . fasta . gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \u201cCircularized x of x contig(s)\u201d. Check the output \u00b6 Move into the Circlator output directory: 1 cd circlator_outdir List the files: 1 ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? 1 less 04 . merge . circularise . log \u201cless\u201d is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? 1 infoseq 06 . fixstart . fasta The contig \u201ctig00000001\u201d has a length of 30019. This is about 18,000 bases shorter than before circularisation. This was the \u201coverhang\u201d and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: 1 cp 06 . fixstart . fasta .. / contig1 . fasta Move back into the main folder: 1 cd .. Questions \u00b6 Were all the contigs circularised? Answer (click to reveal) In this example, yes, the contig was circularised. Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer (click to reveal) Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this. 6. Find smaller plasmids \u00b6 Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn\u2019t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim. Align Illumina reads to the PacBio contig \u00b6 Index the contigs file: 1 bwa index contig1.fasta Align Illumina reads using using bwa mem: 1 bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam bwa mem is the alignment tool - t 4 is the number of cores contig1 . fasta is the input assembly file R1 . fq R2 . fq are the Illumina reads | samtools sort pipes the output to samtools to sort > aln . bam sends the alignment to the file aln.bam Extract unmapped Illumina reads \u00b6 Index the alignment file: 1 samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \u201cunmapped\u201d files: 1 samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format - f 4 : only output unmapped reads - 1 : put R1 reads into a file called unmapped.R1.fastq - 2 : put R2 reads into a file called unmapped.R2.fastq - s : put singleton reads into a file called unmapped.RS.fastq aln . bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq . Assemble the unmapped reads \u00b6 Assemble with Spades: 1 spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) - 1 is input file forward - 2 is input file reverse - s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) - o is the output directory Move into the output directory: 1 cd spades_assembly Look at the contigs: 1 infoseq contigs . fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: 1 cp contigs . fasta contig2 . fasta Trim the plasmid \u00b6 To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: 1 head - n 10 contig2 . fasta > contig2 . fa . head - head - n 10 takes the first ten lines of contig2.fasta - > sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: 1 makeblastdb - in contig2 . fasta - dbtype nucl - makeblastdb makes a database for the tool Blast - This will generate three new files in the directory with suffixes .nhr, .nin and .nsq - - in sets the input file as contig2.fasta - - dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: 1 blastn - query contig2 . fa . head - db contig2 . fasta - evalue 1 e - 3 - dust no - out contig2 . bls blastn is the tool Blast, set as blast**n** to compare sequences of nucleotides to each other - query sets the input sequence as contig2.fa.head - db sets the database as that of the original sequence contig2.fasta . We don\u2019t have to specify the other files that were created when we formatted this file, but they need to present in our current directory. - evalue is the number of hits expected by chance, here set as 1e-3 - dust no turns off the masking of low-complexity regions - out sets the output file as contig2.bls Look at the hits (the matches): 1 less contig2 . bls The first hit is at the start, as expected. We can see that \u201cQuery 1\u201d (the start of the contig) is aligned to \u201cSbject 1\u201d (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows \u201cQuery 1\u201d (the start of the contig) also matches to \u201cSbject 1\u201d (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: 1 nano contig2 . fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won\u2019t work.) At the first line, delete the text, which will be something like \u201c>NODE_1_length_2359_cov_3.320333\u201d Type in \u201c>contig2\u201d Don\u2019t forget the > symbol Press Control-X \u201cSave modified buffer ?\u201d - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): 1 samtools faidx contig2 . fasta faidx means index the fasta file Trim the contig: 1 samtools faidx contig2 . fasta contig2 : 1 - 2252 > plasmid . fasta this extracts contig2 from position 1-2252 > plasmid . fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: 1 cp plasmid . fasta .. / Move file back into main folder: 1 cd .. Collect contigs \u00b6 Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): 1 cat contig1.fasta plasmid.fasta > genome.fasta See the contigs and sizes: 1 infoseq genome.fasta chromosome: 30019 plasmid: 2252 Questions \u00b6 Why is this section so complicated? Answer (click to reveal) Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Why can PacBio sequencing miss small plasmids? Answer (click to reveal) Library prep size selection. We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer (click to reveal) Repeats that have mapped to the PacBio assembly. How do you find a plasmid in a Bandage graph? Answer (click to reveal) It is probably circular, matches the size of a known plasmid, and has a rep gene. Are there easier ways to find plasmids? Answer (click to reveal) Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler 7. Correct the assembly \u00b6 Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly. Make an alignment file \u00b6 Index the fasta file: 1 bwa index genome.fasta Align the Illumina reads: 1 bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) > pilon_aln . bam redirects the sorted bam to this file Index the files: 1 samtools index pilon_aln.bam 1 samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam Run Pilon \u00b6 Run: 1 pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: 1 less pilon1 . changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: 1 infoseq pilon1 . fasta chromosome - 30059 (net +40 bases) plasmid - 2252 (no change) Change the file name: 1 cp pilon1 . fasta assembly . fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid. Questions \u00b6 Why don\u2019t we correct earlier in the assembly process? Answer (click to reveal) We need to circularise the contigs and trim overhangs first. Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer (click to reveal) Illumina reads have higher accuracy. Could we just use PacBio reads to assemble the genome? Answer (click to reveal) Yes, if accuracy adequate. 8. Comparative Genomics \u00b6 In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine a complete bacterial genome. Assemblies \u00b6 This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Does it matter that an assembly is in many contigs? Answer (click to reveal) Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure (*e.g.* the number of plasmids) and the location of genes of interest (*e.g.* gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information). Annotations \u00b6 Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Why are there more CDS identified in the PacBio assembly? Answer (click to reveal) The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Why are there more rRNA identified in the PacBio assembly? Answer (click to reveal) There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly. 9. Summary \u00b6 In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/ and https://galaxy-au-training.github.io/tutorials/","title":"de novo assembly of PacBio reads (unix)"},{"location":"tutorials/pacbio/#long-read-assembly-workshop","text":"This is a tutorial for a workshop on long-read (PacBio) genome assembly. It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data.","title":"Long read assembly workshop"},{"location":"tutorials/pacbio/#overview","text":"Simplified version of workflow:","title":"Overview"},{"location":"tutorials/pacbio/#1-get-started","text":"Your workshop trainers will provide you with the address of a virtual machine.","title":"1. Get started"},{"location":"tutorials/pacbio/#mac-users","text":"Open the Terminal. Type in 1 ssh researcher @ [ your virtual machine address ] Type in the password provided.","title":"Mac users"},{"location":"tutorials/pacbio/#windows-users","text":"If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. Download putty here . Open. A configuration window will appear. Under \u201cHost Name (or IP address)\u201d enter in the address of your virtual machine. Under \u201cPort\u201d type in 22 Under \u201cConnection Type\u201d select \u201cSSH\u201d Click \u201cOpen\u201d Under \u201cLogin as:\u201d enter \u201cresearcher\u201d Type in the password provided.","title":"Windows users"},{"location":"tutorials/pacbio/#activate-the-conda-environment","text":"Type in: 1 source / mnt / gvl / apps / conda / bin / activate (This points us to some different directories for the software we need).","title":"Activate the conda environment"},{"location":"tutorials/pacbio/#create-a-new-working-directory-on-your-remote-computer","text":"Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop. In your terminal: Create a new directory called \u201cWorkshop\u201d 1 mkdir Workshop Change to that directory 1 cd Workshop NOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again. Find your current directory by typing: 1 pwd","title":"Create a new working directory on your remote computer."},{"location":"tutorials/pacbio/#2-get-data","text":"The sample used in this tutorial is from a bacteria called Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time. The files we need are: pacbio.fq : the PacBio reads R1.fq : the Illumina forward reads R2.fq : the Illumina reverse reads In a new tab, go to https://doi.org/10.5281/zenodo.1009308 . Next to the first file, right-click (or control-click) the \u201cDownload\u201d button, and select \u201cCopy link address\u201d. Back in your terminal, enter 1 wget [paste file link here] The file should download. Note: paste the link to the file, not to the webpage. Repeat this for the other two files. Shorten each of these files names with the mv command: 1 2 3 mv R1 . fq \\ ? download \\ = 1 R1 . fq mv R2 . fq \\ ? download \\ = 1 R2 . fq mv pacbio . fq \\ ? download \\ = 1 pacbio . fq Type in ls to check the files are present and correctly-named. We should have R1.fq , R2.fq and pacbio.fq .","title":"2. Get data"},{"location":"tutorials/pacbio/#3-assemble","text":"We will use the assembly software called Canu , version 1.7. Run Canu with these commands: 1 canu -p canu -d canu_outdir genomeSize=0.03m corThreads=3 -pacbio-raw pacbio.fq the first canu tells the program to run - p canu names prefix for output files (\u201ccanu\u201d) - d canu_outdir names output directory (\u201ccanu_outdir\u201d) genomeSize only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). corThreads = 3 sets the number of available threads. Canu will correct, trim and assemble the reads. Various output will be displayed on the screen. Note : Canu could say \u201cFinished\u201d but may still be running. In this case, type squeue to see if jobs are still running. If you run squeue you will see something like this: 1 2 3 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6 main canu_can research PD 0:00 1 (Dependency) 5_1 main cormhap_ research R 0:29 1 master You will know if Canu has completely finished when squeue shows no jobs listed under the header row.","title":"3. Assemble"},{"location":"tutorials/pacbio/#4-check-assembly-output","text":"Move into the canu output folder: 1 cd canu_outdir View the list of files: 1 ls The canu.contigs.fasta are the assembled sequences. The canu.unassembled.fasta are the reads that could not be assembled. The canu.correctedReads.fasta.gz are the corrected Pacbio reads that were used in the assembly. The canu.contigs.gfa is the graph of the assembly. The canu.report file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly. Display summary information about the contigs: ( infoseq is a tool from EMBOSS ) 1 infoseq canu.contigs.fasta This will show the contigs found by Canu. e.g., tig00000001 47997 \u201ctig00000001\u201d is the name given to the contig \u201c47997\u201d is the number of base pairs in that contig. This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. We should also look at the canu.report . To do this: 1 less canu . report \u201cless\u201d is a command to display the file on the screen. Use the up and down arrows to scroll up and down. You will see lots of histograms of read lengths before and after processing, final contig construction, etc. For a description of the outputs that Canu produces, see: http://canu.readthedocs.io/en/latest/tutorial.html#outputs Type q to exit viewing the report.","title":"4. Check assembly output"},{"location":"tutorials/pacbio/#questions","text":"How do long- and short-read assembly methods differ? Answer (click to reveal) Short reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods. Where can we find out the what the approximate genome size should be for the species being assembled? Answer (click to reveal) Go to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column. In the assembly output, what are the unassembled reads? Answer (click to reveal) Reads and low-coverage contigs that were not used in the assembly. What are the corrected reads? How did canu correct the reads? Answer (click to reveal) Canu builds overlaps between reads. The consensus is used to correct the reads. Where could you view the output .gfa and what would it show? Answer (click to reveal) A useful program is [Bandage](https://rrwick.github.io/Bandage/). If the assembly has multiple contigs, the assembly graph shows how these are connected.","title":"Questions"},{"location":"tutorials/pacbio/#5-trim-and-circularise","text":"Bacteria have circular chromosomes. Because of sequencing errors, there may be some \u201coverhang\u201d in the assembled linear sequence. Our assembly may have some overhang because it is 9000 bases longer than expected. Adapted from Figure 1. Hunt et al. Genome Biology 2015 A tool called Circlator identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu. Move back into your main analysis folder: 1 cd ..","title":"5. Trim and circularise"},{"location":"tutorials/pacbio/#run-circlator","text":"1 circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) --threads is the number of cores --verbose prints progress information to the screen canu_outdir / canu . contigs . fasta is the file path to the input Canu assembly canu_outdir / canu . correctedReads . fasta . gz is the file path to the corrected Pacbio reads - note, fastA not fastQ circlator_outdir is the name of the output directory. Some output will print to screen. When finished, it should say \u201cCircularized x of x contig(s)\u201d.","title":"Run Circlator"},{"location":"tutorials/pacbio/#check-the-output","text":"Move into the Circlator output directory: 1 cd circlator_outdir List the files: 1 ls Circlator has named the output files with numbers as prefixes. Were the contigs circularised? 1 less 04 . merge . circularise . log \u201cless\u201d is a command to display the file on the screen. 04.merge.circularise.log is the name of the file. Yes, the contig was circularised (last column). Type q to exit. What are the trimmed contig sizes? 1 infoseq 06 . fixstart . fasta The contig \u201ctig00000001\u201d has a length of 30019. This is about 18,000 bases shorter than before circularisation. This was the \u201coverhang\u201d and has now been trimmed. Copy the circularised contigs file to the main analysis directory with a new name: 1 cp 06 . fixstart . fasta .. / contig1 . fasta Move back into the main folder: 1 cd ..","title":"Check the output"},{"location":"tutorials/pacbio/#questions_1","text":"Were all the contigs circularised? Answer (click to reveal) In this example, yes, the contig was circularised. Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? Answer (click to reveal) Circlator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.","title":"Questions"},{"location":"tutorials/pacbio/#6-find-smaller-plasmids","text":"Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads. This section involves several steps: Use the Canu+Circlator output of a trimmed assembly contig. Map all the Illumina reads against this Pacbio-assembled contig. Extract any reads that didn\u2019t map and assemble them together: this could be a plasmid, or part of a plasmid. Look for overhang: if found, trim.","title":"6. Find smaller plasmids"},{"location":"tutorials/pacbio/#align-illumina-reads-to-the-pacbio-contig","text":"Index the contigs file: 1 bwa index contig1.fasta Align Illumina reads using using bwa mem: 1 bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam bwa mem is the alignment tool - t 4 is the number of cores contig1 . fasta is the input assembly file R1 . fq R2 . fq are the Illumina reads | samtools sort pipes the output to samtools to sort > aln . bam sends the alignment to the file aln.bam","title":"Align Illumina reads to the PacBio contig"},{"location":"tutorials/pacbio/#extract-unmapped-illumina-reads","text":"Index the alignment file: 1 samtools index aln.bam Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \u201cunmapped\u201d files: 1 samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam fastq is a command that coverts a .bam file into fastq format - f 4 : only output unmapped reads - 1 : put R1 reads into a file called unmapped.R1.fastq - 2 : put R2 reads into a file called unmapped.R2.fastq - s : put singleton reads into a file called unmapped.RS.fastq aln . bam : input alignment file We now have three files of the unampped reads: unmapped.R1.fastq , unmapped.R2.fastq , unmapped.RS.fastq .","title":"Extract unmapped Illumina reads"},{"location":"tutorials/pacbio/#assemble-the-unmapped-reads","text":"Assemble with Spades: 1 spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.) - 1 is input file forward - 2 is input file reverse - s is unpaired --careful minimizes mismatches and short indels --cov-cutoff auto computes the coverage threshold (rather than the default setting, \u201coff\u201d) - o is the output directory Move into the output directory: 1 cd spades_assembly Look at the contigs: 1 infoseq contigs . fasta 1 contig has been assembled with a length of 2359 bases. Copy it to a new file: 1 cp contigs . fasta contig2 . fasta","title":"Assemble the unmapped reads"},{"location":"tutorials/pacbio/#trim-the-plasmid","text":"To trim any overhang on this plasmid, we will blast the start of contig2 against itself. Take the start of the contig: 1 head - n 10 contig2 . fasta > contig2 . fa . head - head - n 10 takes the first ten lines of contig2.fasta - > sends that output to a new file called contig2.fa.head We want to see if the start of the contig matches the end (overhang). Format the assembly file for blast: 1 makeblastdb - in contig2 . fasta - dbtype nucl - makeblastdb makes a database for the tool Blast - This will generate three new files in the directory with suffixes .nhr, .nin and .nsq - - in sets the input file as contig2.fasta - - dbtype nucl sets the type to nucleotide (rather than protein) Blast the start of the assembly (.head file) against all of the assembly: 1 blastn - query contig2 . fa . head - db contig2 . fasta - evalue 1 e - 3 - dust no - out contig2 . bls blastn is the tool Blast, set as blast**n** to compare sequences of nucleotides to each other - query sets the input sequence as contig2.fa.head - db sets the database as that of the original sequence contig2.fasta . We don\u2019t have to specify the other files that were created when we formatted this file, but they need to present in our current directory. - evalue is the number of hits expected by chance, here set as 1e-3 - dust no turns off the masking of low-complexity regions - out sets the output file as contig2.bls Look at the hits (the matches): 1 less contig2 . bls The first hit is at the start, as expected. We can see that \u201cQuery 1\u201d (the start of the contig) is aligned to \u201cSbject 1\u201d (the whole contig), for the first 540 bases. Scroll down with the down arrow. The second hit shows \u201cQuery 1\u201d (the start of the contig) also matches to \u201cSbject 1\u201d (the whole contig) at position 2253, all the way to the end, position 2359. This is the overhang. Therefore, in the next step, we need to trim the contig to position 2252. Type q to exit. First, change the name of the contig within the file: 1 nano contig2 . fasta nano opens up a text editor. Use the arrow keys to navigate. (The mouse won\u2019t work.) At the first line, delete the text, which will be something like \u201c>NODE_1_length_2359_cov_3.320333\u201d Type in \u201c>contig2\u201d Don\u2019t forget the > symbol Press Control-X \u201cSave modified buffer ?\u201d - type Y Press the Enter key Index the file (this will allow samtools to edit the file as it will have an index): 1 samtools faidx contig2 . fasta faidx means index the fasta file Trim the contig: 1 samtools faidx contig2 . fasta contig2 : 1 - 2252 > plasmid . fasta this extracts contig2 from position 1-2252 > plasmid . fasta sends the extracted section to a new file We now have a trimmed plasmid. Copy the plasmid file into the main folder: 1 cp plasmid . fasta .. / Move file back into main folder: 1 cd ..","title":"Trim the plasmid"},{"location":"tutorials/pacbio/#collect-contigs","text":"Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): 1 cat contig1.fasta plasmid.fasta > genome.fasta See the contigs and sizes: 1 infoseq genome.fasta chromosome: 30019 plasmid: 2252","title":"Collect contigs"},{"location":"tutorials/pacbio/#questions_2","text":"Why is this section so complicated? Answer (click to reveal) Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744 Why can PacBio sequencing miss small plasmids? Answer (click to reveal) Library prep size selection. We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? Answer (click to reveal) Repeats that have mapped to the PacBio assembly. How do you find a plasmid in a Bandage graph? Answer (click to reveal) It is probably circular, matches the size of a known plasmid, and has a rep gene. Are there easier ways to find plasmids? Answer (click to reveal) Possibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler","title":"Questions"},{"location":"tutorials/pacbio/#7-correct-the-assembly","text":"Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly.","title":"7. Correct the assembly"},{"location":"tutorials/pacbio/#make-an-alignment-file","text":"Index the fasta file: 1 bwa index genome.fasta Align the Illumina reads: 1 bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam Aligns Illumina R1.fq and R2.fq to the PacBio assembly genome.fasta . This produces a .bam file | pipes the output to samtools to sort (required for downstream processing) > pilon_aln . bam redirects the sorted bam to this file Index the files: 1 samtools index pilon_aln.bam 1 samtools faidx genome.fasta Now we have an alignment file to use with the tool Pilon : pilon_aln.bam","title":"Make an alignment file"},{"location":"tutorials/pacbio/#run-pilon","text":"Run: 1 pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4 --genome is the name of the input assembly to be corrected --frags is the alignment of the reads against the assembly --output is the name of the output prefix --fix is an option for types of corrections --mindepth gives a minimum read depth to use --changes produces an output file of the changes made --verbose prints information to the screen during the run --threads : number of cores Look at the changes file: 1 less pilon1 . changes Example: We can see lots of cases where a deletion (represented by a dot) has been corrected to a base. Type q to exit. Look at the details of the fasta file: 1 infoseq pilon1 . fasta chromosome - 30059 (net +40 bases) plasmid - 2252 (no change) Change the file name: 1 cp pilon1 . fasta assembly . fasta We now have the corrected genome assembly of Staphylococcus aureus in .fasta format, containing a chromosome and a small plasmid.","title":"Run Pilon"},{"location":"tutorials/pacbio/#questions_3","text":"Why don\u2019t we correct earlier in the assembly process? Answer (click to reveal) We need to circularise the contigs and trim overhangs first. Why can we use some reads (Illumina) to correct other reads (PacBio) ? Answer (click to reveal) Illumina reads have higher accuracy. Could we just use PacBio reads to assemble the genome? Answer (click to reveal) Yes, if accuracy adequate.","title":"Questions"},{"location":"tutorials/pacbio/#8-comparative-genomics","text":"In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine a complete bacterial genome.","title":"8. Comparative Genomics"},{"location":"tutorials/pacbio/#assemblies","text":"This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). Assembly graphs: Look at the assembly graph (usually has a suffix .gfa), in the program Bandage . This shows how contigs are related, albeit with ambiguity in some places. The assembly graph from Illumina reads (Spades assembly): The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid: Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. Does it matter that an assembly is in many contigs? Answer (click to reveal) Yes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure (*e.g.* the number of plasmids) and the location of genes of interest (*e.g.* gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information).","title":"Assemblies"},{"location":"tutorials/pacbio/#annotations","text":"Genomic features such as genes can be identified with annotation tools. We have used a tool called Prokka to annotate the two genomes described above. Some of the output data is displayed here: assembly: PacBio Illumina size 2,825,804 2,792,905 contigs 2 123 CDS 2614 2575 tRNA 61 65 rRNA 19 4 Why are there more CDS identified in the PacBio assembly? Answer (click to reveal) The PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results. Why are there more rRNA identified in the PacBio assembly? Answer (click to reveal) There may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly.","title":"Annotations"},{"location":"tutorials/pacbio/#9-summary","text":"In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. Procedure and tools: Canu to assemble long-read PacBio data Circlator to trim and circularise contigs BWA-MEM to map shorter Illumina reads to the PacBio assembly Spades to assemble any unmapped, leftover Illumina reads (the plasmid) Pilon to correct the PacBio assembly with the more accurate Illumina reads We also looked at comparative genomics: Bandage to examine assembly graphs Prokka to annotate genomes with features such as genes Further research: Align genomes with Mauve: tutorial link Find core and pan genomes with Roary and Phandango: tutorial link Melbourne Bioinformatics tutorials: https://www.melbournebioinformatics.org.au/tutorials/ Additional microbial genomics tutorials: http://sepsis-omics.github.io/tutorials/ and https://galaxy-au-training.github.io/tutorials/","title":"9. Summary"},{"location":"tutorials/proteomics_basic/","text":"PR reviewers and advice: Ira Cooke Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/proteomics_basic/background_data_formats/","text":"Data Formats and Pre-processing \u00b6 Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don\u2019t produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats Background"},{"location":"tutorials/proteomics_basic/background_data_formats/#data-formats-and-pre-processing","text":"Mass Spectrometry data analysis is plagued by an overabundance of file formats. The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, mzML . The bad news is that many of the old formats are still in widespread use, and most instruments don\u2019t produce it natively. The reference implementation of the mzML standard is a software suite called ProteoWizard . ProteoWizard includes a very handy tool called msconvert that is capable of converting raw data from most instruments into mzML or into one of many other formats. In addition to format conversion, msconvert can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis. A typical pre-processing involves; Conversion from instrument .raw to mzML Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert) Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window Convert spectrum identifiers into a standardized format To convert files from raw instrument native formats to mzML a windows PC is required. If you need to do this, be sure to download ProteoWizard with vendor reader support . This package comes with MSConvertGUI which allows conversion of raw files using a graphical interface. Once files are in mzML or mgf format they can be converted to various other formats using the msconvert3 tool in Galaxy.","title":"Data Formats and Pre-processing"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/","text":"Create a workflow \u00b6 Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create a workflow"},{"location":"tutorials/proteomics_basic/background_findcountitems_workflow/#create-a-workflow","text":"Click the Workflow menu item at the top of galaxy Click the button called Create New Workflow Enter a name and description for the new workflow and click Create You should now have a blank workflow canvas. Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu. Under Workflow Control and then under Inputs you should see an Input dataset item. Click it to create a blank input box Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are; The Select tool from the Find and Sort submenu The Line/Word/Character count tool from the Text Manipulation submenu After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next Save your workflow After saving the workflow return to the main Workflow menu (top of Galaxy) and select your new workflow to run it. Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.","title":"Create a workflow"},{"location":"tutorials/proteomics_basic/background_galaxy/","text":"How to use Galaxy \u00b6 This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki . Create a new History \u00b6 Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \u201cUnnamed History\u201d. Click its title to rename it. Rename a history item \u00b6 Locate the item in your history and click its pencil icon Enter a new name in the Name : field and click Save Find someone\u2019s exact username \u00b6 Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username Share a History \u00b6 Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user\u2019s full username Import a History \u00b6 Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name Copy Datasets \u00b6 Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories Multi File Inputs \u00b6 Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected. Saved Histories \u00b6 To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"How to use Galaxy"},{"location":"tutorials/proteomics_basic/background_galaxy/#how-to-use-galaxy","text":"This background wiki gives very brief guides on performing specific tasks in Galaxy. For much more extensive documentation including many videos, online tutorials and discussion forums please consult the galaxy wiki .","title":"How to use Galaxy"},{"location":"tutorials/proteomics_basic/background_galaxy/#create-a-new-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Create new The new history will be called \u201cUnnamed History\u201d. Click its title to rename it.","title":"Create a new History"},{"location":"tutorials/proteomics_basic/background_galaxy/#rename-a-history-item","text":"Locate the item in your history and click its pencil icon Enter a new name in the Name : field and click Save","title":"Rename a history item"},{"location":"tutorials/proteomics_basic/background_galaxy/#find-someones-exact-username","text":"Click the User menu at the top of Galaxy. The menu that appears will show the currently logged in username","title":"Find someone's exact username"},{"location":"tutorials/proteomics_basic/background_galaxy/#share-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Share or Publish Then select Share with another user and enter the user\u2019s full username","title":"Share a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#import-a-history","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me Select and view the desired history by clicking on its name","title":"Import a History"},{"location":"tutorials/proteomics_basic/background_galaxy/#copy-datasets","text":"Click the History Options menu (cog icon) in the top-right corner of Galaxy. Select Histories Shared with Me The screen that appears allows copying of specific datasets between any of your histories","title":"Copy Datasets"},{"location":"tutorials/proteomics_basic/background_galaxy/#multi-file-inputs","text":"Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input. After clicking the multi-file icon the display should change to allow multiple inputs to be selected.","title":"Multi File Inputs"},{"location":"tutorials/proteomics_basic/background_galaxy/#saved-histories","text":"To view a list of your saved histories click the History Options menu (cog icon) in the top-right corner of Galaxy and select Saved Histories Once the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags","title":"Saved Histories"},{"location":"tutorials/proteomics_basic/background_protein_databases/","text":"Protein Databases \u00b6 In a perfect experiment we would obtain fragment ions for all the b , y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database. Large vs Small Database \u00b6 Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue. Typical sources of data for search databases \u00b6 Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome. Should I include decoys? \u00b6 Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to \u2018pin down\u2019 the negative distribution.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#protein-databases","text":"In a perfect experiment we would obtain fragment ions for all the b , y pairs of each peptide. If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself. Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins. This database is usually just a FASTA formatted file containing amino acid sequences for all known proteins from your study organism. Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results. In order to detect a peptide, its exact sequence must be explicitly included in the database.","title":"Protein Databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#large-vs-small-database","text":"Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of NCBInr . There are two problems with this. The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits. Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. Note that very small databases can also cause problems. In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions. With a very small database (or with very few spectra) it may not be possible to model these distributions accurately. In most practical situations this is not an issue.","title":"Large vs Small Database"},{"location":"tutorials/proteomics_basic/background_protein_databases/#typical-sources-of-data-for-search-databases","text":"Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases SwissProt , which contains manually curated sequences and Trembl which contains sequences automatically generated from genomic and transcriptomic data. An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories. Some of these are well maintained and are the best source of data for that study organism. Examples include PlasmoDB for Malaria for Flybase for drosophila. Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it. If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database. Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome.","title":"Typical sources of data for search databases"},{"location":"tutorials/proteomics_basic/background_protein_databases/#should-i-include-decoys","text":"Decoys are often useful, but not always needed. Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of Peptide Prophet typically use decoys because it can use these to \u2018pin down\u2019 the negative distribution.","title":"Should I include decoys?"},{"location":"tutorials/proteomics_basic/background_protein_prophet/","text":"Protein Prophet \u00b6 The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It\u2019s citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let\u2019s look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1 . 0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr | Q3UN47 | Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp | O08600 | NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1 . 0 ) but probabilities group members are different. The first member of the group has a high probability 0 . 99 but all other members have probabilities of 0 . 0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam\u2019s razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_protein_prophet/#protein-prophet","text":"The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference. The original paper describing protein prophet is worth reading. It\u2019s citation is; Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003). The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging. Let\u2019s look at a few examples; Uniquely identified protein sp|P00761|TRYP_PIG Search for this protein in the Protein Prophet results file. It should have group_probability and protein_probability scores of 1 . 0 . All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group. Indistinguishable Protein sp|O08600|NUCG_MOUSE In this case there still just one entry for the protein, but Protein Prophet lists another protein tr | Q3UN47 | Q3UN47_MOUSE in the indistinguishable proteins column. This protein is indistinguishable from the primary entry sp | O08600 | NUCG_MOUSE because all of the identified peptides are shared between both. A well behaved protein group sp|O08677|KNG1_MOUSE This protein is part of a smallish group of similar proteins. The overall group probability is high ( 1 . 0 ) but probabilities group members are different. The first member of the group has a high probability 0 . 99 but all other members have probabilities of 0 . 0 . This is because all of the high scoring peptides are contained in the first entry. Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam\u2019s razor; plurality should not be posited with out necessity In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein. Anomalous groups In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge. This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.","title":"Protein Prophet"},{"location":"tutorials/proteomics_basic/background_search_engines/","text":"How Search Engines Work \u00b6 When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it\u2019s old, I recommend this presentation by Brian Searle","title":"Search Engines"},{"location":"tutorials/proteomics_basic/background_search_engines/#how-search-engines-work","text":"When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below. The next spectrum in the dataset is loaded for consideration The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database. This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space. Database search space size also depends on many other factors including The size of the protein database Variable modifications allowed on peptides Parent ion mass tolerance Number of allowed missed enzymatic cleavages Specificity of the enzyme used for digestion Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other. The highest scoring peptide spectrum match (PSM) is recorded along with its score. Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance There are many excellent presentations online that explain this in more detail. Although it\u2019s old, I recommend this presentation by Brian Searle","title":"How Search Engines Work"},{"location":"tutorials/proteomics_basic/proteomics_basic/","text":"Identifying proteins from mass spec data \u00b6 Overview \u00b6 This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \u201cprotein database search\u201d or even \u201cprotein sequencing\u201d, although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \u201cmystery\u201d organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together Login to Galaxy \u00b6 Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server Import mass spec data \u00b6 Create a new history in Galaxy and name it \u201cOrganelle Tutorial\u201d Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / OrganelleSample . mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \u201cEdit attributes\u201d. This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \u201c/\u201d Your item should then be named OrganelleSample.mzML Dont forget to click \u201cSave\u201d Basic properties of the data \u00b6 Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \u201cMS1 spectrum\u201d in the page using your web browser\u2019s search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \u201cspectrumList count\u201d. It should bring you to a line in the file that says spectrumList count=\u201d24941\u201d. There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \u201cMS1 spectrum\u201d in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \u201cMS1 spectrum\u201d also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable5\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable5\").toggleClass(\"showable-hidden\"); if ( (\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Alternate data formats \u00b6 Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \u201cBEGIN IONS\u201d and \u201cEND IONS\u201d statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / OrganelleSample . mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \u201cBEGIN IONS\u201d statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable12\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable12\").toggleClass(\"showable-hidden\"); if ( (\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Obtain a Search Database \u00b6 Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you\u2019ll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / UniprotMouseD_20140716 . fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \u201c>\u201d followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; 1 sp | Q9CQV8 | 1433 B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; 1 decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \u201cdecoy_\u201d. The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Run a search using X!Tandem \u00b6 A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716 . fasta MSMS File OrganelleSample . mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0 . 5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine\u2019s work . Convert Results to tabular format \u00b6 Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Sort tabular outputs \u00b6 Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys. Convert raw scores to probabilities \u00b6 Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM\u2019s with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM\u2019s and dividing by the total number of accepted PSM\u2019s. A widely used alternative to Peptide Prophet is Percolator . If you\u2019re curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM\u2019s have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \u201cgreater than or equal to\u201d in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. 1 c11 >= 0 . 95 To answer the second question use the Select tool on the filtered table to select lines matching \u201cdecoy_\u201d //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable21\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable21\").toggleClass(\"showable-hidden\"); if ( (\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable25\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable25\").toggleClass(\"showable-hidden\"); if ( (\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Perform Protein Inference \u00b6 Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we\u2019ve viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings. Functional enrichment analysis \u00b6 This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \u201cpipes\u201d that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Identifying proteins from mass spectrometry data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#identifying-proteins-from-mass-spec-data","text":"","title":"Identifying proteins from mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#overview","text":"This tutorial describes how to identify a list of proteins from tandem mass spectrometry data. Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \u201cprotein database search\u201d or even \u201cprotein sequencing\u201d, although amino acid sequences are not obtained de novo with this method. The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer. The sample itself corresponds to a purified organelle from Mouse cells. The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \u201cmystery\u201d organelle. This tutorial uses free software including; The X!Tandem search engine The Trans Proteomic Pipeline (TPP) for post-search validation The Protk tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier The Galaxy platform to bring all these tools together","title":"Overview"},{"location":"tutorials/proteomics_basic/proteomics_basic/#login-to-galaxy","text":"Open a browser and go to a Galaxy server. You can use a galaxy server of your own or Galaxy Tute at genome.edu.au Use a supported browser. Firefox/Safari/Chrome all work well If you use your own galaxy server you will need to make sure you have the protk proteomics tools installed. Register as a new user if you don\u2019t already have an account on that particular server","title":"Login to Galaxy"},{"location":"tutorials/proteomics_basic/proteomics_basic/#import-mass-spec-data","text":"Create a new history in Galaxy and name it \u201cOrganelle Tutorial\u201d Download datasets using the Galaxy uploader tool. Open this tool by clicking the button as shown below After opening the tool select Paste/Fetch data and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / OrganelleSample . mzML After the download is finished you should have a single item in your history. Rename the history item by clicking the pencil icon beside it to \u201cEdit attributes\u201d. This should bring up a dialog box where you can edit the name. Change the name by removing everything up to the last forward slash \u201c/\u201d Your item should then be named OrganelleSample.mzML Dont forget to click \u201cSave\u201d","title":"Import mass spec data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#basic-properties-of-the-data","text":"Format: Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about mass spectrometry data formats here 1) What format is the OrganelleSample.mzML file? //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> Hint Try clicking the title bar on the data in your galaxy history. This will toggle display of some additional information about the data. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer mzML //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink2\").click(function(e){ e.preventDefault(); $(\"#showable2\").toggleClass(\"showable-hidden\"); }); }); //-->]]> MS versus MS/MS: A key feature of Tandem mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide. These two types of measurements are called MS and MS/MS spectra respectively. The following schematic shows how an MS/MS scan results from the fragmentation of a selected product ion. Often multiple MS/MS spectra are obtained for each MS scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed. Number of spectra: Click the eye icon on the history item to view the mzML file as text. The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of MS and MS/MS spectra in the file. Now try searching for the text \u201cMS1 spectrum\u201d in the page using your web browser\u2019s search function. Looking closely you should see that this text appears once for every MS1 spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description). The file is large though and the browser can only see the first megabyte of it. Now search for the text \u201cspectrumList count\u201d. It should bring you to a line in the file that says spectrumList count=\u201d24941\u201d. There are a total of 24941 spectra in the entire file including both MS and MS/MS spectra. 2) How many MS spectra are there in this dataset? Hint To answer this question you will need to use the Select tool from the Filter and Sort submenu to select lines matching the text \u201cMS1 spectrum\u201d in the whole file. Then use the Line/Word/Character count tool from the Text Manipulation submenu to count the number of lines returned by running the Select tool. More <- and here to show more The text \u201cMS1 spectrum\u201d also appears at the top of the file as part of its description so you will need to subtract 1 from your answer //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink5\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable5\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable5\").toggleClass(\"showable-hidden\"); if ( (\"#showable5\").hasClass(\"showable-hidden\")) { $(\"#showablelink5\").text(\"More\"); } else { $(\"#showablelink5\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink4\").click(function(e){ e.preventDefault(); $(\"#showable4\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3142 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink6\").click(function(e){ e.preventDefault(); $(\"#showable6\").toggleClass(\"showable-hidden\"); }); }); //-->]]> In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. See the instructions here Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching MS peaks against the masses of whole peptides via Peptide Mass Fingerprinting . This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the MS/MS spectra. In this tutorial only the MS/MS spectra will be used. 3) How many MS/MS spectra are there in this dataset? Hint Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about MS spectra. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink8\").click(function(e){ e.preventDefault(); $(\"#showable8\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink9\").click(function(e){ e.preventDefault(); $(\"#showable9\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Basic properties of the data"},{"location":"tutorials/proteomics_basic/proteomics_basic/#alternate-data-formats","text":"Another format you are likely to encounter for tandem mass spectrometry data is Mascot Generic Format or mgf . Mascot Generic Format ( mgf ) is the data file format preferred by the Mascot search engine. It is a text based format is much easier to read by hand than the mzML file. Each spectrum appears between \u201cBEGIN IONS\u201d and \u201cEND IONS\u201d statements and simply consists of ( mz , intensity ) pairs. Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. Download the Organelle Sample data in mgf format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / OrganelleSample . mgf Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file. 4) How many spectra are there in this dataset and what type of spectra do you think they are? Hint Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \u201cBEGIN IONS\u201d statement in the file. More <- and here to show more Consider your answers to questions 3 and 4 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink12\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable12\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable12\").toggleClass(\"showable-hidden\"); if ( (\"#showable12\").hasClass(\"showable-hidden\")) { $(\"#showablelink12\").text(\"More\"); } else { $(\"#showablelink12\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink11\").click(function(e){ e.preventDefault(); $(\"#showable11\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 21799 MS/MS //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink13\").click(function(e){ e.preventDefault(); $(\"#showable13\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Alternate data formats"},{"location":"tutorials/proteomics_basic/proteomics_basic/#obtain-a-search-database","text":"Setting up a search database is a critical step. For this tutorial we have created a database for you, but if you need to create a database for your own data you\u2019ll need to consider the following key issues; Database size Whether to include decoys What types of variants to include if any How to format your database identifiers More details are provided here . Download a database of Mouse proteins in fasta format Use the Paste/Fetch data tool again and paste the following URL into the box that appears. Then click Start to initiate the download. 1 https : // swift . rc . nectar . org . au : 8888 / v1 / AUTH_ffb00634530a4c37a0b8b08c48068adf / proteomics_tutorial / UniprotMouseD_20140716 . fasta Inspect the first few items in the database in Galaxy. The file is in Fasta format which means that each entry has a single description line that starts with a \u201c>\u201d followed by a unique identifier and then some general descriptive information. The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this; 1 sp | Q9CQV8 | 1433 B_MOUSE The database also includes decoy sequences, appended at the end. They have identifiers like this; 1 decoy_rp75404 5) What is the ratio of decoys to non-decoys in the database? Hint Decoys are easiest to search for because they all start with \u201cdecoy_\u201d. The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title). //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink15\").click(function(e){ e.preventDefault(); $(\"#showable15\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 1:1 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink16\").click(function(e){ e.preventDefault(); $(\"#showable16\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Obtain a Search Database"},{"location":"tutorials/proteomics_basic/proteomics_basic/#run-a-search-using-xtandem","text":"A large number of search engines now exist for proteomics data. This exercise uses X!Tandem which is one of the fastest and most widely used. Other search engines include OMSSA , MS-GF+ and Mascot . Select the X!Tandem Search tool Enter parameters as shown in the table below (leave all others at their defaults) Click Execute Parameter Name Value Uploaded Fasta File UniprotMouseD_20140716 . fasta MSMS File OrganelleSample . mgf Variable Modifications Oxidation M Fixed Modifications Carbamidomethyl C Missed Cleavages Allowed 2 Enzyme Trypsin Fragment Ion Tolerance 0 . 5 Precursor Ion Tolerance 10 ppm The search should run for about 5-10 minutes and will produce an output file in X!Tandem xml format. A much more useful format is pepXML so the next step in the analysis will be to run a tool to convert from tandem to pepXML. Select the Tandem to pepXML tool Select the output from the previous step as input and click Execute While the search is running, read some background theory on how search engine\u2019s work .","title":"Run a search using X!Tandem"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-results-to-tabular-format","text":"Although the pepXML format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert pepXML into tabular (tab separated) format, which is much easier to read. Tabular format also has the advantage that it can be downloaded and opened using many other programs including Excel and R . Select the pepXML to Table tool Select the pepXML file produced in the previous step as input and click Execute To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest. Explore the column assignments in your tabular file by clicking on its title in your galaxy history. This will show extra details about the item, including a handy preview where column numbers are displayed. 6) In what column number is the assumed_charge of peptides in the pepXML tabular file? Answer 3 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink18\").click(function(e){ e.preventDefault(); $(\"#showable18\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert Results to tabular format"},{"location":"tutorials/proteomics_basic/proteomics_basic/#sort-tabular-outputs","text":"Examine the tabular output file from the previous step. It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results. These decoys should tend to have quite poor scores compared with real hits. The raw score for X!Tandem searches is an E-value . To push these decoys to the bottom of the list we can sort the data by raw score . Select the Sort data tool from the Filter and Sort menu in the left pane of Galaxy Choose to sort on raw_score . This is column c10 Select Ascending order for the sort direction (small E-values are good) and click Execute Browse the resulting dataset. The top of the file should now have very few decoys.","title":"Sort tabular outputs"},{"location":"tutorials/proteomics_basic/proteomics_basic/#convert-raw-scores-to-probabilities","text":"Raw X!Tandem scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct. A number of tools exist for this, and in this tutorial we use Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by Peptide Prophet can be used to set a threshold for acceptance. For example we could decide to accept only PSM\u2019s with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM\u2019s and dividing by the total number of accepted PSM\u2019s. A widely used alternative to Peptide Prophet is Percolator . If you\u2019re curious about how Peptide Prophet works, take a look at this explainer , or the original paper Select the Peptide Prophet tool Select the X!Tandem output in pepXML format generated earlier as input Check the box that says Use decoys to pin down the negative distribution . Convert the resulting pepXML file to tabular using the PepXML to Table tool Take a look at the resulting tabular file. Note that this time the peptideprophet_prob column is populated and contains numbers between 0 and 1. 7) How many PSM\u2019s have a peptideprophet probability greater than or equal to 0.95 Hint Use the Filter tool from the Filter and Sort submenu. Also remember that Peptide Prophet probability is given in a column called peptideprophet_prob . The syntax for \u201cgreater than or equal to\u201d in the Filter tool is >=. More <- and here to show more Use this text in match with condition field of the Filter and Sort tool. 1 c11 >= 0 . 95 To answer the second question use the Select tool on the filtered table to select lines matching \u201cdecoy_\u201d //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink21\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable21\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable21\").toggleClass(\"showable-hidden\"); if ( (\"#showable21\").hasClass(\"showable-hidden\")) { $(\"#showablelink21\").text(\"More\"); } else { $(\"#showablelink21\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink20\").click(function(e){ e.preventDefault(); $(\"#showable20\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 3808 21 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink22\").click(function(e){ e.preventDefault(); $(\"#showable22\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM) Hint Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3) More <- and here to show more To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink25\").click(function(e){ e.preventDefault(); <span><span class=\"MathJax_Preview\">(\"#showable25\").toggleClass(\"showable-hidden\"); if (</span><script type=\"math/tex\">(\"#showable25\").toggleClass(\"showable-hidden\"); if ( (\"#showable25\").hasClass(\"showable-hidden\")) { $(\"#showablelink25\").text(\"More\"); } else { $(\"#showablelink25\").text(\"Less\"); } }); }); //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink24\").click(function(e){ e.preventDefault(); $(\"#showable24\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 17.27% //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink26\").click(function(e){ e.preventDefault(); $(\"#showable26\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Convert raw scores to probabilities"},{"location":"tutorials/proteomics_basic/proteomics_basic/#perform-protein-inference","text":"Up to this point we have looked at peptide to spectrum matches PSMs . Each of the peptides observed will have come from a protein sequence in the fasta file that we used as a database, and this protein is recorded along with the PSM itself in all of the result tables we\u2019ve viewed so far. Unfortunately, the process of inferring the existence of proteins based on these PSMs is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one PSM . The Protein Prophet tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins. Select the Protein Prophet tool Choose the pepXML formatted output from Peptide Prophet as input and click Execute Convert the resulting protXML to tabular using the protXML to Table tool. 9) How many proteins are there with protein prophet probability greater than or equal to 0.99? Hint Filter on column 6 protein_probability //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink28\").click(function(e){ e.preventDefault(); $(\"#showable28\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer 601 //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink29\").click(function(e){ e.preventDefault(); $(\"#showable29\").toggleClass(\"showable-hidden\"); }); }); //-->]]> If you have time, read over these notes on the Protein Prophet output. Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.","title":"Perform Protein Inference"},{"location":"tutorials/proteomics_basic/proteomics_basic/#functional-enrichment-analysis","text":"This step will allow you to discover the identity of the Organelle that was used to create the sample. We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom). Start with unfiltered tabular protein prophet results Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains protein_name ). Convert the \u201cpipes\u201d that separate parts of the protein_name into separate columns using the Convert delimiters to TAB tool in the Text manipulation submenu of Galaxy. This should result in a file with 3 columns Use the Cut columns tool again to cut the second column from this dataset Download this file to your desktop and rename it to organelle.txt Open the GOrilla web page in your web browser Select Organism as Mouse Upload the organelle.txt file as a ranged gene list Choose Component for the ontology Submit 9) What intracellular organelle was enriched in the sample? Hint Ignore terms relating to exosomes In the resulting output look to the most enriched and most specific GO terms. //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink31\").click(function(e){ e.preventDefault(); $(\"#showable31\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Answer Mitochondria //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink32\").click(function(e){ e.preventDefault(); $(\"#showable32\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"Functional enrichment analysis"},{"location":"tutorials/python_overview/","text":"PR reviewers and advice: Bernard Pope Current slides: None Other slides: None yet","title":"Home"},{"location":"tutorials/python_overview/python_overview/","text":"Authors: \u00b6 Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne General information \u00b6 Python modules are stored in files containing a \u201c.py\u201d suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used. Python 2 versus Python 3 \u00b6 Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary. Indentation for grouping code blocks \u00b6 Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code. Style Guide \u00b6 A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: 1 2 3 4 5 6 7 8 9 10 11 # Compute factorial of n, # assuming n >= 0 def factorial ( n ): result = 1 while n > 0 : result *= n n -= 1 return result print ( factorial ( 10 )) C program for computing factorial: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial ( int n ) { int result = 1 ; while ( n > 0 ) { result *= n ; n -= 1 ; } return result ; } int main ( void ) { printf ( \"%d \\n \" , factorial ( 10 )); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer). Comments \u00b6 Program comments start with a hash character \u201c#\u201d and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: 1 2 3 4 5 6 7 8 9 10 # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. ''' Running a Python program \u00b6 There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented. Objects and types \u00b6 Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \u201c.\u201d operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type ( x ) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 >>> # Create a list , assign to the variable x >>> x = [ 3 , 1 , 2 , 3 ] >>> # Ask for the type of the value assigned to x >>> type ( x ) < class ' list ' > >>> # Ask for the type of the first item in the list ( an integer ) >>> type ( x [ 0 ] ) < class ' int ' > >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x . count ( 3 ) 2 >>> # Sort the contents of the list in - place . >>> # Note that this mutates the list object ! >>> # Also note that Python does not print the result in this case . >>> x . sort () >>> # Ask Python to show the value of the list >>> # assigned to the variable x ( note it is now sorted ) >>> x [ 1 , 2 , 3 , 3 ] >>> # Assign x to an object of a different type ( a float ) >>> x = 3 . 142 >>> type ( x ) < class ' float ' > Booleans \u00b6 Represent truth values Values: True , False Type: bool Operators: and , or , not bool ( x ) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0 . 0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: 1 2 3 4 5 6 7 8 9 10 11 12 >>> not True False >>> not False True >>> not () True >>> not [ 1 , 2 , 3 ] False >>> True and False False >>> True and () () Conditional Statements \u00b6 Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: 1 2 3 4 5 6 7 if expression : statement - block elif expression : statement - block ... else : statement - block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: 1 2 3 4 5 6 >>> if []: ... print ( \" Was considered True \" ) ... else : ... print ( \" Was considered False \" ) ... Was considered False Numbers and basic mathematics \u00b6 Integers \u00b6 Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means ( 4 * 10 ) + 2 ) Hexadecimal literals start with 0 x , octal literals start with 0 o , binary literals start with 0 b . int ( x ) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0 x10 16 >>> 0 b10 2 >>> - 0 == 0 True >>> int ( \" 123 \" ) 123 >>> int ( \" 3.142 \" ) Traceback ( most recent call last ) : File \" <stdin> \" , line 1 , in < module > ValueError : invalid literal for int () with base 10 : ' 3.142 ' Floating Point Numbers \u00b6 Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys . float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3 . 142 Exponential: 314 . 2 e - 2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float ( x ) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( 3 . 142 ) < class 'float' > >>> type ( 12 ) < class 'int' > >>> 3 . 142 + 12 15 . 142 >>> 3 . 142 == 314 . 2 e - 2 True >>> 3 . == 3 . 0 True >>> 1 / 0 Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > ZeroDivisionError : division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3 . 3333333333333335 >>> float ( \"123\" ) 123 . 0 >>> float ( \"3.142\" ) 3 . 142 Complex Numbers \u00b6 Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real \u00b1 imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: 1 2 3 4 5 6 >>> 5 j + 3 j 8 j >>> 2 - 5 j ( 2 - 5 j ) >>> 2 - 5 j + 3 j ( 2 - 2 j ) Numeric Operators \u00b6 Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> 3 + 4 * 5 23 >>> ( 3 + 4 ) * 5 35 >>> 10 / 3 3 . 3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> ( 2 ** 3 ) ** 4 4096 Strings \u00b6 Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: 1 2 3 4 5 6 7 8 >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn' t going to work though ' File \"<stdin>\", line 1 ' this isn 't going to work though' ^ SyntaxError : invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\ n newline \\ t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s [ 5 ] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 >>> type ( \"hello\" ) < class 'str' > >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\" . upper () 'BONJOUR' >>> len ( \"bonjour\" ) 7 >>> \"bonjour\" . startswith ( \"b\" ) True >>> \"cat,sat,flat\" . split ( \",\" ) [ 'cat' , 'sat' , 'flat' ] >>> # Print the first 5 Chinese unicode characters >>> print ( '\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04' ) \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x [ 0 ] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 # Prompt the user to input a string: input = raw_input ( \"Enter string: \" ) # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input : if char in vowels : count += 1 # Print the count to the standard output print ( count ) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels . py : 1 2 3 python vowels . py Enter string : abracadabra 5 Lists \u00b6 Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [ 1 , 2 , 3 ] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x [ 3 ] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( [ 1, 2, 3 ] ) < class 'list' > >>> x = [] >>> len ( x ) 0 >>> x . append ( \"hello\" ) >>> x [ 'hello' ] >>> len ( x ) 1 >>> x [ 0 ] 'hello' >>> x . insert ( 0 , True ) >>> x [ True, 'hello' ] >>> del x [ 1 ] >>> x [ True ] >>> x += [ 42, \"Newton\", 3.142 ] >>> x [ True, 42, 'Newton', 3.142 ] Dictionaries \u00b6 Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. { 12 : \"XII\" , 6 : \"VI\" } Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( { 12 : \"XII\" , 6 : \"VI\" } ) < class 'dict' > >>> friends = {} >>> friends [ 'Fred' ] = [ 'Barney' , 'Dino' ] >>> friends { 'Fred' : [ 'Barney' , 'Dino' ] } >>> friends [ 'Fred' ] [ 'Barney' , 'Dino' ] >>> friends [ 'Barney' ] Traceback ( most recent call last ): File \"\\<stdin\\>\" , line 1 , in \\ < module \\ > KeyError : 'Barney' >>> friends [ 'Wilma' ] = [ 'Betty' ] >>> friends { 'Fred' : [ 'Barney' , 'Dino' ], 'Wilma' : [ 'Betty' ] } >>> friends . keys () dict_keys ([ 'Fred' , 'Wilma' ]) >>> friends . values () dict_values ([[ 'Barney' , 'Dino' ], [ 'Betty' ]]) >>> 'Dino' in friends False Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys . stdin : # Parse the next input as an integer next_integer = int ( line ) # Update the histogram accordingly if next_integer in histogram : # We've seen this integer before histogram [ next_integer ] += 1 else : # First occurrence of this integer in the input histogram [ next_integer ] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted ( histogram ): print ( \"{} {}\" . format ( key , histogram [ key ])) s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo . py : 1 python histo . py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 1 2 3 4 5 6 7 8 3 43 12 19 3 12 12 43 Program prints its output: 1 2 3 4 3 2 12 3 19 1 43 2 Tuples \u00b6 Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: ( 1 , 2 , 3 ) The can be used as keys in dictionaries (unlike lists). Loops \u00b6 While loops \u00b6 Iterate until condition is False Syntax: 1 2 while expression : statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: 1 2 3 4 5 6 def factorial ( n ): result = 1 while n > 0 : result *= n n -= 1 return result For loops \u00b6 Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: 1 2 for variable in expression : statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range () function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: 1 2 3 4 5 def factorial ( n ) : result = 1 for item in range ( n + 1 ) : result *= item return result Break and continue \u00b6 Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly. Functions \u00b6 Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: 1 2 def variable ( parameter_list ): statement_block Anonymous function syntax: 1 lambda parameter_list : expression Example: 1 2 3 4 5 6 7 8 9 def is_leap_year ( year ): if year % 4 == 0 and year % 100 != 0 : return True else : return year % 400 == 0 for year in range ( 2000 , 2100 + 1 ): result = is_leap_year ( year ) print ( \"{} {}\" . format ( year , result )) Anonymous function example: 1 2 3 4 5 >>> squared = lambda x : x ** 2 >>> squared ( 2 ) 4 >>> list ( map ( lambda x : x + 1 , [ 1 , 2 , 3 ])) [ 2 , 3 , 4 ] Input and output \u00b6 The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys . argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys . argv [ 1 ] # Open the file file = open ( filename ) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file : num_lines += 1 num_words += len ( line . split ()) file . close () print ( \"Number of lines and words in {}: {} {}\" \\ . format ( filename , num_lines , num_words )) Advanced Topics \u00b6 Classes \u00b6 Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: 1 2 class variable ( superclass_list ): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Vector ( object ): def __init__ ( self , x = 0 , y = 0 , z = 0 ): self . x = x self . y = y self . z = z def magnitude ( self ): return sqrt ( self . x ** 2 + self . y ** 2 + self . z ** 2 ) def normalise ( self ): magnitude = self . magnitude () if magnitude == 0 : # Somehow we have a degenerate vector. return self else : return self / self . magnitude () def angle ( self , other ): dp = self . dot_product ( other ) return acos ( dp / self . magnitude () * other . magnitude ()) def dot_product ( self , other ): return self . x * other . x + self . y * other . y + self . z * other . z Exceptions \u00b6 Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: 1 2 3 4 5 try : statement_block except exception_type as variable : statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: 1 2 3 4 5 6 7 8 # alternative version of the histogram code from the section on # dictionaries for line in sys . stdin : next_integer = int ( line ) try : histogram [ next_integer ] += 1 except KeyError : histogram [ next_integer ] = 1 Modules \u00b6 A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> import math >>> math . sqrt ( 100 ) 10.0 >>> sqrt ( 100 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > NameError : name 'sqrt' is not defined >>> from math import sqrt >>> sqrt ( 100 ) 10.0 >>> import math as m >>> m . sqrt ( 100 ) 10.0 Packages \u00b6 A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \u201csandboxed\u201d Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Introduction to Python"},{"location":"tutorials/python_overview/python_overview/#authors","text":"Bernie Pope, Melbourne Bioinformatics (formerly VLSCI) Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne","title":"Authors:"},{"location":"tutorials/python_overview/python_overview/#general-information","text":"Python modules are stored in files containing a \u201c.py\u201d suffix (e.g solver.py). The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted. Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates it immediately and then prints the result before prompting for a new input. The interactive prompt is usually rendered as the chevron >>> . In scripted mode your program is stored in one or more files which are executed as one monolithic entity. Such programs behave like ordinary applications. Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed automatically when no longer used.","title":"General information"},{"location":"tutorials/python_overview/python_overview/#python-2-versus-python-3","text":"Currently there are two distinct flavours of Python available: Python 2 (2.7.10 at the time of writing) Python 3 (3.4.3 at the time of writing) Python 3 is the new and improved version of the language. Python 3 is not entirely backwards compatible, but the two versions share much in common. Version 2 is now in maintenance mode; new features will only be added to version 3. The public transition from 2 to 3 has been slower than some people would like. You are encouraged to use version 3 where possible. These notes are generally compatible with both versions, but we will point out key differences where necessary.","title":"Python 2 versus Python 3"},{"location":"tutorials/python_overview/python_overview/#indentation-for-grouping-code-blocks","text":"Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping. The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth). You are encouraged not to use tabs for indentation because there is no standard width for a tab. Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.","title":"Indentation for grouping code blocks"},{"location":"tutorials/python_overview/python_overview/#style-guide","text":"A popular style guide for Python is known as PEP 0008 , there is a corresponding tool called pep8 which will check your code against the guide and report any transgressions. Example, Python compared to C: Python program for computing factorial: 1 2 3 4 5 6 7 8 9 10 11 # Compute factorial of n, # assuming n >= 0 def factorial ( n ): result = 1 while n > 0 : result *= n n -= 1 return result print ( factorial ( 10 )) C program for computing factorial: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include <stdio.h> /* Compute factorial of n, assuming n >= 0 */ int factorial ( int n ) { int result = 1 ; while ( n > 0 ) { result *= n ; n -= 1 ; } return result ; } int main ( void ) { printf ( \"%d \\n \" , factorial ( 10 )); } Things to note: The difference in commenting style. C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed. Code blocks in C are grouped by braces { }; Python uses indentation for grouping. The C program must have a main function. Python does not require a main function, it just executes the top-level statements of the module. The result returned by the C function is limited to the size of a machine integer (say 32 bits). However, the result returned by the Python function is unlimited in its size - it can compute arbitrarily large factorials (up to the limit of the available memory in your computer).","title":"Style Guide"},{"location":"tutorials/python_overview/python_overview/#comments","text":"Program comments start with a hash character \u201c#\u201d and continue until the end of the line. There are no multi-line comment markers, but that can sometimes be faked with multi-line string literals. Examples: 1 2 3 4 5 6 7 8 9 10 # This is a comment. # This is another comment. x = 5 # This is a comment that follows some code. '''This is a multi-line string literal which can sometimes act like a comment. '''","title":"Comments"},{"location":"tutorials/python_overview/python_overview/#running-a-python-program","text":"There are many ways to run Python code: You can run the interpreter in interactive mode. On Unix (Linux, OS X) you can run the python command at the command line. If you have Python code stores in a file, say example.py, you can run it from the command line like so: python example.py You can use one of several integrated programming environments. Python ships with a fairly minimal one called IDLE , though many scientists prefer the more comprehensive IPython . If your Python code was installed as a package (see below), then it may be executed like an ordinary application without the user being aware of how the program was implemented.","title":"Running a Python program"},{"location":"tutorials/python_overview/python_overview/#objects-and-types","text":"Every value in Python is an object (including functions!). Objects can have attributes and methods, which are accessed via the dot \u201c.\u201d operator. All objects have a type. Types are also objects! Python is dynamically typed: you may get type errors at runtime but never at compile time. type ( x ) returns the type of x. Python variables may be assigned to values of different types at different points in the program. Interactive examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 >>> # Create a list , assign to the variable x >>> x = [ 3 , 1 , 2 , 3 ] >>> # Ask for the type of the value assigned to x >>> type ( x ) < class ' list ' > >>> # Ask for the type of the first item in the list ( an integer ) >>> type ( x [ 0 ] ) < class ' int ' > >>> # Count the number of times 3 appears in the list >>> # by calling the count method >>> x . count ( 3 ) 2 >>> # Sort the contents of the list in - place . >>> # Note that this mutates the list object ! >>> # Also note that Python does not print the result in this case . >>> x . sort () >>> # Ask Python to show the value of the list >>> # assigned to the variable x ( note it is now sorted ) >>> x [ 1 , 2 , 3 , 3 ] >>> # Assign x to an object of a different type ( a float ) >>> x = 3 . 142 >>> type ( x ) < class ' float ' >","title":"Objects and types"},{"location":"tutorials/python_overview/python_overview/#booleans","text":"Represent truth values Values: True , False Type: bool Operators: and , or , not bool ( x ) will convert x to a boolean. The heuristic is that empty things and zero-ish things are False , everything else is True (but the user can override for their own types). False values: False 0 (zero integer) 0 . 0 (zero float) {} (empty dictionary) () (empty tuple) [] (empty list) '' (empty string) None True values: everything else In numerical contexts True is considered equal to the integer 1 and False is considered equal to the integer 0 . However, these conversions are a common cause of bugs and should be avoided. Python will automatically test the truthiness of a value if it appears in a boolean context. Interactive examples: 1 2 3 4 5 6 7 8 9 10 11 12 >>> not True False >>> not False True >>> not () True >>> not [ 1 , 2 , 3 ] False >>> True and False False >>> True and () ()","title":"Booleans"},{"location":"tutorials/python_overview/python_overview/#conditional-statements","text":"Conditional statements use the keywords: if , elif , else . The syntax for a conditional statement is: 1 2 3 4 5 6 7 if expression : statement - block elif expression : statement - block ... else : statement - block A conditional statement must have exactly one if part. It may have zero or more elif parts, and a single optional else part at the end. The if and elif parts test the value of their boolean expressions. If the expression evaluates to something which is True or can be converted to True (see the rules for Booleans above) then the statement block immediately beneath that part is executed. Otherwise the following condition (if any) is tried. The else part, if it exists, is always and only executed if no preceding condition was True . Interactive examples: 1 2 3 4 5 6 >>> if []: ... print ( \" Was considered True \" ) ... else : ... print ( \" Was considered False \" ) ... Was considered False","title":"Conditional Statements"},{"location":"tutorials/python_overview/python_overview/#numbers-and-basic-mathematics","text":"","title":"Numbers and basic mathematics"},{"location":"tutorials/python_overview/python_overview/#integers","text":"Represent whole negative and positive numbers (and zero). The range of integer values is unbounded (up to some limit defined by how much memory you have on your computer). Python 2 distinguishes between two integer types int and long , and automatically promotes int to long where necessary, whereas Python 3 considers them all one type called int . Base ten is the default literal notation: 42 (means ( 4 * 10 ) + 2 ) Hexadecimal literals start with 0 x , octal literals start with 0 o , binary literals start with 0 b . int ( x ) will try to convert x to an integer, x can be another numeric type (including booleans) or a string. You may specify an optional base for the conversion. Interactive examples (in Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> 2 ** 200 1606938044258990275541962092341162602522202993782792835301376 >>> 0 x10 16 >>> 0 b10 2 >>> - 0 == 0 True >>> int ( \" 123 \" ) 123 >>> int ( \" 3.142 \" ) Traceback ( most recent call last ) : File \" <stdin> \" , line 1 , in < module > ValueError : invalid literal for int () with base 10 : ' 3.142 '","title":"Integers"},{"location":"tutorials/python_overview/python_overview/#floating-point-numbers","text":"Represent a finite approximation to the real numbers. Type: float . (On most platforms) Python uses IEEE-754 double precision floating point numbers which provide 53 bits of precision. sys . float_info contains details about max, min, epsilon etcetera. Literals can be in ordinary notation or in exponential notation: Ordinary: 3 . 142 Exponential: 314 . 2 e - 2 Ordinary notation requires a point . , but digits following the point are optional. Exponential notation does not require a point unless you have a fractional component. float ( x ) will try to convert x to a floating point number, x can be another numeric type (including booleans) or a string. Numeric operators will automatically convert integer arguments to floating point in mixed-type expressions. In Python 3 the division operator / computes a floating point result for integer arguments. However, in Python 2 it computes an integer result for integer arguments. Interactive examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( 3 . 142 ) < class 'float' > >>> type ( 12 ) < class 'int' > >>> 3 . 142 + 12 15 . 142 >>> 3 . 142 == 314 . 2 e - 2 True >>> 3 . == 3 . 0 True >>> 1 / 0 Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > ZeroDivisionError : division by zero >>> # Integer divided by integer yields a float in Python 3 >>> 10 / 3 3 . 3333333333333335 >>> float ( \"123\" ) 123 . 0 >>> float ( \"3.142\" ) 3 . 142","title":"Floating Point Numbers"},{"location":"tutorials/python_overview/python_overview/#complex-numbers","text":"Represent a finite approximation to the complex numbers. Type: complex A pair of floating point numbers: real \u00b1 imaginary. The real part is optional (defaults to 0). The imaginary part is followed immediately by the character j . Interactive Examples: 1 2 3 4 5 6 >>> 5 j + 3 j 8 j >>> 2 - 5 j ( 2 - 5 j ) >>> 2 - 5 j + 3 j ( 2 - 2 j )","title":"Complex Numbers"},{"location":"tutorials/python_overview/python_overview/#numeric-operators","text":"Name Operation Precedence Associativity Notes + add low left Can also be used to concatenate strings together. * multiply medium left - subtract low left / divide medium left In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers. // floor-divide medium left divide then floor, result is an integer ** exponent high right % modulus medium left remainder after division Interactive Examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 >>> 3 + 4 * 5 23 >>> ( 3 + 4 ) * 5 35 >>> 10 / 3 3 . 3333333333333335 >>> 10 // 3 3 >>> 10 % 3 1 >>> 2 ** 3 ** 4 2417851639229258349412352 >>> ( 2 ** 3 ) ** 4 4096","title":"Numeric Operators"},{"location":"tutorials/python_overview/python_overview/#strings","text":"Represent text Type: str In Python 3, the str type contains Unicode characters. In Python 2, the str type contains ASCII characters (sometimes called byte strings). Python 2 has a separate type for unicode strings, the type is called unicode; literals of this type are prefixed by the letter u . String literals must be quoted. There are 3 quoting styles: single quote characters: 'hello' double quote characters: \"hello\" triple quote characters: '''hello''' (three single quotes in a row) or \"\"\"hello\"\"\" (three double quote characters in a row) The single quote and double quote versions of strings have the same value. The purpose of the different quotation styles is to make it convenient to have literal quotation marks inside strings (avoiding the need to escape the quote character). For example: 1 2 3 4 5 6 7 8 >>> \"This inverted comma won't be a problem inside quotation marks\" \"This inverted comma won't be a problem inside quotation marks\" >>> 'this \"quote\" will work' 'this \"quote\" will work' >>> 'this isn' t going to work though ' File \"<stdin>\", line 1 ' this isn 't going to work though' ^ SyntaxError : invalid syntax Triple quoted strings can be written on multiple lines. The line breaks will be preserved within the string. Useful for docstrings (see section on functions). The usual set of escape characters are supported: \\ n newline \\ t tab \\\\ backslash \\' single quote \\\" double quote and many more Python does not have a separate type for representing individual characters. Instead you use strings of length one. Strings are iterable. If you iterate over a string (using a for loop) you process it one character at a time from left to right. Strings can be indexed to obtain individual characters, e.g. s [ 5 ] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the string). Strings are immutable: you cannot modify a string once it has been created. Interactive Examples (Python 3): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 >>> type ( \"hello\" ) < class 'str' > >>> \"hello\" == 'hello' True >>> '''This string ... is on ... multiple ... lines''' 'This string\\\\nis on\\\\nmultiple\\\\nlines' >>> \"bonjour\" . upper () 'BONJOUR' >>> len ( \"bonjour\" ) 7 >>> \"bonjour\" . startswith ( \"b\" ) True >>> \"cat,sat,flat\" . split ( \",\" ) [ 'cat' , 'sat' , 'flat' ] >>> # Print the first 5 Chinese unicode characters >>> print ( '\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04' ) \u4e00\u4e01\u4e02\u4e03\u4e04 >>> x = \"floyd\" >>> x [ 0 ] 'f' >>> \"hello\" + \" \" + \"world\" 'hello world' Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 # Prompt the user to input a string: input = raw_input ( \"Enter string: \" ) # Count the number of vowels in the input string vowels = 'aeiou' count = 0 for char in input : if char in vowels : count += 1 # Print the count to the standard output print ( count ) Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called vowels . py : 1 2 3 python vowels . py Enter string : abracadabra 5","title":"Strings"},{"location":"tutorials/python_overview/python_overview/#lists","text":"Represent mutable ordered sequences of values. Type: list List literals are written in between square brackets, e.g. [ 1 , 2 , 3 ] List elements can be objects of any type (including other lists). Like strings, lists can be indexed like so: x [ 3 ] Indices are zero-based (but you may also use negative indices to access items with respect to the right end of the list). Lists are mutable. You can update items, delete items and add new items. Indexing into a list is a constant time (amortised) operation. Interactive Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( [ 1, 2, 3 ] ) < class 'list' > >>> x = [] >>> len ( x ) 0 >>> x . append ( \"hello\" ) >>> x [ 'hello' ] >>> len ( x ) 1 >>> x [ 0 ] 'hello' >>> x . insert ( 0 , True ) >>> x [ True, 'hello' ] >>> del x [ 1 ] >>> x [ True ] >>> x += [ 42, \"Newton\", 3.142 ] >>> x [ True, 42, 'Newton', 3.142 ]","title":"Lists"},{"location":"tutorials/python_overview/python_overview/#dictionaries","text":"Represent finite mappings from keys to values. Are implemented as hash tables . The key objects must be hashable (which rules out mutable objects, such as lists). Type: dict Dictionary literals are written inside curly brackets, with key-value pairs separated by colons: e.g. { 12 : \"XII\" , 6 : \"VI\" } Dictionaries can be indexed by keys. If the key exists in the dictionary its corresponding value is returned, otherwise a KeyError exception is raised. The cost of indexing a dictionary is proportional to the time taken to hash the key. For many keys this can be considered constant time. For variable sized objects, such as strings, this can be considered to be proportional to the size of the object. Iterating over a dictionary yields one key at a time. All keys in the dictionary are visited exactly once. The order in which the keys are visited is arbitrary. You may test if an object is a key of a dictionary using the in operator. Interactive Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 >>> type ( { 12 : \"XII\" , 6 : \"VI\" } ) < class 'dict' > >>> friends = {} >>> friends [ 'Fred' ] = [ 'Barney' , 'Dino' ] >>> friends { 'Fred' : [ 'Barney' , 'Dino' ] } >>> friends [ 'Fred' ] [ 'Barney' , 'Dino' ] >>> friends [ 'Barney' ] Traceback ( most recent call last ): File \"\\<stdin\\>\" , line 1 , in \\ < module \\ > KeyError : 'Barney' >>> friends [ 'Wilma' ] = [ 'Betty' ] >>> friends { 'Fred' : [ 'Barney' , 'Dino' ], 'Wilma' : [ 'Betty' ] } >>> friends . keys () dict_keys ([ 'Fred' , 'Wilma' ]) >>> friends . values () dict_values ([[ 'Barney' , 'Dino' ], [ 'Betty' ]]) >>> 'Dino' in friends False Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Compute and print a histogram of a sequence of integers entered # on standard input, one number per line import sys histogram = {} # Iterate over each line in the standard input for line in sys . stdin : # Parse the next input as an integer next_integer = int ( line ) # Update the histogram accordingly if next_integer in histogram : # We've seen this integer before histogram [ next_integer ] += 1 else : # First occurrence of this integer in the input histogram [ next_integer ] = 1 # Print each key: value pair in the histogram in ascending # sorted order of keys for key in sorted ( histogram ): print ( \"{} {}\" . format ( key , histogram [ key ])) s Example usage of the above program from the operating system command prompt, assuming the program is saved in a file called histo . py : 1 python histo . py User types in a sequence of integers to the program, one per line, and presses control-d to terminate the input: 1 2 3 4 5 6 7 8 3 43 12 19 3 12 12 43 Program prints its output: 1 2 3 4 3 2 12 3 19 1 43 2","title":"Dictionaries"},{"location":"tutorials/python_overview/python_overview/#tuples","text":"Represent immutable ordered sequences of values. Very much like lists except they cannot be modified once created. Type: tuple Literals are written in between parentheses: ( 1 , 2 , 3 ) The can be used as keys in dictionaries (unlike lists).","title":"Tuples"},{"location":"tutorials/python_overview/python_overview/#loops","text":"","title":"Loops"},{"location":"tutorials/python_overview/python_overview/#while-loops","text":"Iterate until condition is False Syntax: 1 2 while expression : statement_block The value of the boolean expression is tested. If it evaluates to True then the statement block is executed once, before repeating the loop. If it evaluates to False then the program continues execution immediately after the loop. Example: 1 2 3 4 5 6 def factorial ( n ): result = 1 while n > 0 : result *= n n -= 1 return result","title":"While loops"},{"location":"tutorials/python_overview/python_overview/#for-loops","text":"Iterate over each item in a collection (e.g. list, string, tuple, dictionary, file). Syntax: 1 2 for variable in expression : statement_block Each item from the iterator expression is selected and assigned to the variable, then the statement block is executed. The loop ends when every item in the iterator has been visited. The order of items visited in the iterator depends on the type of the iterator. Lists, strings and tuples proceed in a left-to-right fashion. Files proceed one line at a time. Dictionaries proceed in an arbitrary order. The range () function is useful for generating iterators of numbers within a range. Note that the lower bound is inclusive and the upper bound is exclusive. Example: 1 2 3 4 5 def factorial ( n ) : result = 1 for item in range ( n + 1 ) : result *= item return result","title":"For loops"},{"location":"tutorials/python_overview/python_overview/#break-and-continue","text":"Both types of loops support the break and continue keywords. break terminates the loop immediately. continue jumps immediately back to the start of the loop. They can sometimes simplify the conditional logic of a loop, but should be used sparingly.","title":"Break and continue"},{"location":"tutorials/python_overview/python_overview/#functions","text":"Allow you to define reusable abstractions. Sometimes called procedures . Are generally defined at the top level of a module, and can also be nested. Type: function Named functions are bound to a variable name and may have complex bodies. Anonymous functions are used in-line, and may only have expression bodies. Named function syntax: 1 2 def variable ( parameter_list ): statement_block Anonymous function syntax: 1 lambda parameter_list : expression Example: 1 2 3 4 5 6 7 8 9 def is_leap_year ( year ): if year % 4 == 0 and year % 100 != 0 : return True else : return year % 400 == 0 for year in range ( 2000 , 2100 + 1 ): result = is_leap_year ( year ) print ( \"{} {}\" . format ( year , result )) Anonymous function example: 1 2 3 4 5 >>> squared = lambda x : x ** 2 >>> squared ( 2 ) 4 >>> list ( map ( lambda x : x + 1 , [ 1 , 2 , 3 ])) [ 2 , 3 , 4 ]","title":"Functions"},{"location":"tutorials/python_overview/python_overview/#input-and-output","text":"The print function is useful for displaying text (and other values converted to text). In Python 2 print was a special keyword. In Python 3 it is a function defined in the builtins. Fancy string formatting can be done with the format method on strings. Older Python code uses the string interpolation operator for the same task % , but its use is now discouraged. Files must be opened before than can be manipulated. A file can be opened in different modes: read \"r\" , write \"w\" , read-write \"r+\" , and append \"a\" . Opening a new file in write or append modes creates a new file. Opening an existing file in write mode overwrites its contents from the start. Opening an existing file in append mode adds new content at the end of the old content. When you are finished processing a file you should close it as soon as possible. Closing a file releases limited operating system resources, and ensures that any pending buffered writes a flushed to the storage system. Certain file types have libraries for convenient processing. One example is the CSV (comma separated values) library for processing tabular data. It is very handy for working with spreadsheets. The command line arguments of a Python program are contained in a list called sys . argv (it is a variable exported from the sys module). For complex program you should consider using a command line argument parsing library such as argparse . Example program: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Count the number of words and lines in a file import sys # Get the input file name from the command line arguments filename = sys . argv [ 1 ] # Open the file file = open ( filename ) # Count the number of lines in the file num_lines = 0 num_words = 0 for line in file : num_lines += 1 num_words += len ( line . split ()) file . close () print ( \"Number of lines and words in {}: {} {}\" \\ . format ( filename , num_lines , num_words ))","title":"Input and output"},{"location":"tutorials/python_overview/python_overview/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"tutorials/python_overview/python_overview/#classes","text":"Classes allow you to define your own types. Class definitions may define methods for the type. A class may inherit, and possibly override, some functionality from a superclass. Syntax: 1 2 class variable ( superclass_list ): body The name of the class is given by the variable in the definition. The superclass list defines the superclasses of the new class (very often the base type object is used). The body of the class typically defines one or more methods. Instances of classes are created by calling the class name as if it were a function. If defined, the special method called __init__ is used to initialise a newly created instance of a class. The first parameter to each method is the object upon which the method was called. The convention is to use the variable called self, however any variable name will do. Many object oriented languages make this variable an implicit parameter called this. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Vector ( object ): def __init__ ( self , x = 0 , y = 0 , z = 0 ): self . x = x self . y = y self . z = z def magnitude ( self ): return sqrt ( self . x ** 2 + self . y ** 2 + self . z ** 2 ) def normalise ( self ): magnitude = self . magnitude () if magnitude == 0 : # Somehow we have a degenerate vector. return self else : return self / self . magnitude () def angle ( self , other ): dp = self . dot_product ( other ) return acos ( dp / self . magnitude () * other . magnitude ()) def dot_product ( self , other ): return self . x * other . x + self . y * other . y + self . z * other . z","title":"Classes"},{"location":"tutorials/python_overview/python_overview/#exceptions","text":"Exceptions allow Python programs to handle erroneous program conditions. An exception is raised (or thrown) at the point of the error and handled (or caught) at some other place in the program. Exception handlers have the syntax: 1 2 3 4 5 try : statement_block except exception_type as variable : statement_block ... The statement block after try is executed. If no exceptions are raised in that block the program continues immediately after the exception handler. If an exception is raised in the block then program control jumps to the innermost closing except clause. Except clauses may optionally specify the set of exception types that they can handle. If the raised exception is an instance of the handled type then the body of the except clause is executed, otherwise the next except clause (if any) is tried. If no matching exception handler is found then the program will terminate with an unhandled exception error. Python will normally print a stack trace at this point for error diagnosis. You may raise your own exceptions using the raise keyword. Example: 1 2 3 4 5 6 7 8 # alternative version of the histogram code from the section on # dictionaries for line in sys . stdin : next_integer = int ( line ) try : histogram [ next_integer ] += 1 except KeyError : histogram [ next_integer ] = 1","title":"Exceptions"},{"location":"tutorials/python_overview/python_overview/#modules","text":"A module is a file which contains Python code. Any Python file you create is automatically a module. It is considered good programming style to decompose complex programs into multiple modules. Each module should collect together code with similar purpose. Variables defined at the top level of a module (such as global variables, functions and classes) can be imported into other modules. Python comes with many standard modules. The import keyword is used to import an entire module. You may import a subset of things from a module using the from ... import ... syntax. You may import a module with a new name using the from ... import ... as ... or import ... as ... When a module is first imported in a program, all of its top-level statements are executed from top to bottom. Subsequent imports use a cached version of its definitions, its statements are not re-executed. A special module called builtins is imported into every other module by default, and it is automatically imported at the interactive prompt in the interpreter. Interactive Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 >>> import math >>> math . sqrt ( 100 ) 10.0 >>> sqrt ( 100 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > NameError : name 'sqrt' is not defined >>> from math import sqrt >>> sqrt ( 100 ) 10.0 >>> import math as m >>> m . sqrt ( 100 ) 10.0","title":"Modules"},{"location":"tutorials/python_overview/python_overview/#packages","text":"A package is a collection of modules in a hierarchy. Packages are the common way to structure Python libraries. The Python Package Index (PyPI) is a big collection of open source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of writing). Package installation tools such as pip , make it easy to install packages onto your computer. If you want to make your own Python code easy for others to install and use then you should consider making it a package. You can even upload it to PyPI. Many people use virtualenv to install packages into a local \u201csandboxed\u201d Python environment. This avoids conflicts with the central Python package database on your computer, and allows multiple different versions of packages to be used.","title":"Packages"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/","text":"Overview \u00b6 This workshop covers practical approaches for handling data in Python. We will use the Python library Pandas. This workshop is a recommended prerequisite for the Data Visualisation workshop. In order to do effective data analysis or visualisation, we usually need to have our data cleaned and in a consistent format. We will cover the concept of \u201ctidy\u201d, long-form, and wide-form data, and hands-on approaches for manipulating data and fixing common problems. This workshop concentrates on tabular data, like that found in spreadsheets or databases. Learning Objectives \u00b6 At the end of this workshop you will be able to: read tabular data into Python using Pandas, and manipulate it identify problems in datasets that will hinder analysis use Python to fix common problems understand and convert between different data layouts such as wide-form and \u201ctidy\u201d as appropriate for the problem being solved Requirements \u00b6 This workshop is designed for participants with a basic knowledge of Python, but is also appropriate for attendees who do not know Python but have significant experience using another programming language. If you are new to Python, you will probably want to refer to a Python primer. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren\u2019t able to install it prior to the workshop, we can work around this, but please contact us beforehand. Notebooks and Data \u00b6 This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Pandas_and_tidying.ipynb notebook.","title":"Python pandas tidy data"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#overview","text":"This workshop covers practical approaches for handling data in Python. We will use the Python library Pandas. This workshop is a recommended prerequisite for the Data Visualisation workshop. In order to do effective data analysis or visualisation, we usually need to have our data cleaned and in a consistent format. We will cover the concept of \u201ctidy\u201d, long-form, and wide-form data, and hands-on approaches for manipulating data and fixing common problems. This workshop concentrates on tabular data, like that found in spreadsheets or databases.","title":"Overview"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#learning-objectives","text":"At the end of this workshop you will be able to: read tabular data into Python using Pandas, and manipulate it identify problems in datasets that will hinder analysis use Python to fix common problems understand and convert between different data layouts such as wide-form and \u201ctidy\u201d as appropriate for the problem being solved","title":"Learning Objectives"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#requirements","text":"This workshop is designed for participants with a basic knowledge of Python, but is also appropriate for attendees who do not know Python but have significant experience using another programming language. If you are new to Python, you will probably want to refer to a Python primer. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren\u2019t able to install it prior to the workshop, we can work around this, but please contact us beforehand.","title":"Requirements"},{"location":"tutorials/python_pandas_tidy_data/python_pandas_tidy_data/#notebooks-and-data","text":"This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Pandas_and_tidying.ipynb notebook.","title":"Notebooks and Data"},{"location":"tutorials/python_viz/python_viz/","text":"Overview \u00b6 Python has a wide range of libraries for plotting and visualising data. Many of these are excellent, but it can be hard for a newcomer to know where to start. We will introduce the range of options available, then do hands-on visualisation exercises with some popular libraries: Matplotlib, Seaborn, and Altair. Seaborn builds on Matplotlib to easily create beautiful statistical visualisations. Altair is intended for interactive visualisation and makes it easy to create complex responsive visualisations. Learning Objectives \u00b6 At the end of this workshop you should be able to: be aware of the landscape of visualisation libraries create visualisations of data in Matplotlib, Seaborn and Altair know how to search the documentation for further visualisation functions Requirements \u00b6 This workshop is designed for participants with a basic knowledge of Python. The \u201cData tidying with Python and Pandas\u201d workshop is recommended as a prerequisite. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren\u2019t able to install it prior to the workshop, we can work around this, but please contact us beforehand. Notebooks and Data \u00b6 This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Seaborn_Matplotlib.ipynb and Altair.ipynb notebooks.","title":"Data visualisation with Python"},{"location":"tutorials/python_viz/python_viz/#overview","text":"Python has a wide range of libraries for plotting and visualising data. Many of these are excellent, but it can be hard for a newcomer to know where to start. We will introduce the range of options available, then do hands-on visualisation exercises with some popular libraries: Matplotlib, Seaborn, and Altair. Seaborn builds on Matplotlib to easily create beautiful statistical visualisations. Altair is intended for interactive visualisation and makes it easy to create complex responsive visualisations.","title":"Overview"},{"location":"tutorials/python_viz/python_viz/#learning-objectives","text":"At the end of this workshop you should be able to: be aware of the landscape of visualisation libraries create visualisations of data in Matplotlib, Seaborn and Altair know how to search the documentation for further visualisation functions","title":"Learning Objectives"},{"location":"tutorials/python_viz/python_viz/#requirements","text":"This workshop is designed for participants with a basic knowledge of Python. The \u201cData tidying with Python and Pandas\u201d workshop is recommended as a prerequisite. Attendees are required to bring their own laptop computers. You should install the Anaconda Python distribution before attending: Go to: https://www.anaconda.com/distribution/#download-section Select your operating system Select the Python 3.7 (not 2.7) option to download and install. This is a large download (over 600MB). If you aren\u2019t able to install it prior to the workshop, we can work around this, but please contact us beforehand.","title":"Requirements"},{"location":"tutorials/python_viz/python_viz/#notebooks-and-data","text":"This workshop is implemented as a set of Jupyter Notebooks, and we will use (and introduce) Jupyter during the workshop. You can find all notebooks and data in this github repository . For this workshop, we will use the Seaborn_Matplotlib.ipynb and Altair.ipynb notebooks.","title":"Notebooks and Data"},{"location":"tutorials/qiime2/","text":"QIIME2 regularly updates so links in tutorial will need to be checked and pointed to latest version. PR reviewers and advice: Gayle Philip Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/qiime2/qiime2/","text":"QIIME2 \u00b6 Anticipated workshop duration when delivered to a group of participants is 4 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ). Overview \u00b6 Topic \u00b6 Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills Skill level \u00b6 Beginner Intermediate Advanced This workshop is designed for participants with command-line knowledge. You will need to be able to ssh into a remote machine, navigate the directory structure and scp files from a remote computer to your local computer. Description \u00b6 What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities? Data: Illumina MiSeq v3 paired-end (2 \u00d7 300 bp) reads (FASTQ). Tools: QIIME 2 Pipeline: Section 1: Importing, cleaning and quality control of the data Section 2: Taxonomic Analysis Section 3: Building a phylogenetic tree Section 4: Basic visualisations and statistics Section 5: Exporting data for further analysis in R Learning Objectives \u00b6 At the end of this introductory workshop, you will: Take raw data from a sequencing facility and end with publication quality graphics and statistics Answer the question What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities? Requirements and preparation \u00b6 Important Attendees are required to use their own laptop computers. At least one week before the workshop, if required, participants should install the software below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems. Required Software \u00b6 Mac Users: No additional software needs to be installed for this workshop. Windows Users: 1. A terminal emulator such as PuTTY (free and open-source) will need to be downloaded. 2. Software for file transfers between a local computer and remote server such as WinSCP or FileZilla . Required Data \u00b6 No additional data needs to be downloaded for this workshop. It is located in the directory raw_data located in the home directory of the Nectar instance that you will log in to. If you wish to analyse the data independently at a later stage, it can be downloaded from here . This zipped folder contains both the FASTQs and associated metadata file. Mode of Delivery \u00b6 This workshop will be run on a Nectar Instance. An \u201cInstance\u201d is Nectar terminology for a virtual machine running on the Nectar Cloud OpenStack infrastructure. An \u201cInstance\u201d runs on a \u201ccompute node\u201d; i.e. a physical computer populated with processor chips, memory chips and so on. You will be given an individual IP address and password to log on to using the SSH client tool on your computer (Terminal on Mac or PuTTY on Windows). 1 ssh ubuntu@ip-address Should you wish to do this tutorial at a later stage independently, it is possible to apply for your own instance directly through a Nectar allocation . There are also many helpful Nectar Research Cloud tutorials . Author Information \u00b6 Written by: Ashley Dungan and Gayle Philip School of Biosciences, University of Melbourne; Melbourne Bioinformatics Created/Reviewed: May 2021 Background \u00b6 What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities? The Players \u00b6 Exaiptasia diaphana - a shallow-water, marine anemone that is often used in research as a model organism for corals. In this experiment, two genotypes (AIMS1 and AIMS4) of E. diaphana were grown in each of two different environments: sterile seawater OR unfiltered control seawater The anemone-associated bacterial communities or microbiome - these bacteria live on, or within E. diaphana , and likely consist of a combination of commensals, transients, and long-term stable members, and combined with their host, form a mutually beneficial, stable symbiosis. The Study \u00b6 The anemone microbiome contributes to the overall health of this complex system and can evolve in tandem with the anemone host. In this data set we are looking at the impact of intrinsic and extrinsic factors on anemone microbiome composition. After three weeks in either sterile or control seawater (environment), anemones were homogenized and DNA was extracted. There are 23 samples in this data set - 5 from each anemone treatment combination (2 genotypes x 2 environments) and 3 DNA extraction blanks as controls. This data is a subset from a larger experiment . Dungan AM, van Oppen MJH, and Blackall LL (2021) Short-Term Exposure to Sterile Seawater Reduces Bacterial Community Diversity in the Sea Anemone, Exaiptasia diaphana . Front. Mar. Sci. 7:599314. doi:10.3389/fmars.2020.599314 [Full Text] . QIIME 2 Analysis platform \u00b6 Quantitative Insights Into Microbial Ecology 2 ( QIIME 2\u2122 ) is a next-generation microbiome bioinformatics platform that is extensible, free, open source, and community developed. It allows researchers to: Automatically track analyses with decentralised data provenance Interactively explore data with beautiful visualisations Easily share results without QIIME 2 installed Plugin-based system \u2014 researchers can add in tools as they wish Viewing QIIME2 visualisations \u00b6 As this workshop is being run on a remote Nectar Instance, you will need to download the visual files ( *.qzv ) to your local computer and view them in QIIME 2 View (q2view). Attention We will be doing this step multiple times throughout this workshop to view visualisation files as they are generated. The syntax to do this depends on whether you are running the copying command on your local computer, or on the remote computer (Nectar cloud). When running the command from your local computer, the syntax for copying a file from Nectar is: 1 scp ubuntu@your_IP_address:FILENAME /PATH/TO/TARGET/FOLDER/ Running the command on the remote computer, the syntax for copying a file to your local computer is: 1 scp FILENAME username@your_IP_address:/PATH/TO/TARGET/FOLDER/ Alternatively, if you have QIIME2 installed and are running it on your own computer , you can use qiime tools view to view the results from the command line. qiime tools view opens a browser window with your visualization loaded in it. When you are done, you can close the browser window and press ctrl - c on the keyboard to terminate the command. 1 qiime tools view filename . qzv Section 1: Importing, cleaning and quality control of the data \u00b6 Import data \u00b6 These samples were sequenced on a single Illumina MiSeq run using v3 (2 \u00d7 300 bp) reagents at the Walter and Eliza Hall Institute (WEHI), Melbourne, Australia. Data from WEHI came as paired-end, demultiplexed, unzipped *.fastq files with adapters still attached. Following the QIIME2 importing tutorial , this is the Casava One Eight format. The files have been renamed to satisfy the Casava format as SampleID_FWDXX-REVXX_L001_R[1 or 2]_001.fastq e.g. CTRLA_Fwd04-Rev25_L001_R1_001.fastq.gz. The files were then zipped (.gzip). Here, the data files (two per sample i.e. forward and reverse reads R1 and R2 respectively) will be imported and exported as a single QIIME 2 artefact file. These samples are already demultiplexed (i.e. sequences from each sample have been written to separate files), so a metadata file is not initially required. Note To check the input syntax for any QIIME2 command, enter the command, followed by --help e.g. qiime tools import -- help Start by making a new directory analysis to store all the output files from this tutorial. In addition, we will create a subdirectory called seqs to store the exported sequences. 1 2 cd mkdir -p analysis/seqs Run the command to import the raw data located in the directory raw_data and export it to a single QIIME 2 artefact file, combined . qza . 1 2 3 4 5 qiime tools import \\ -- type 'SampleData[PairedEndSequencesWithQuality]' \\ -- input - path raw_data \\ -- input - format CasavaOneEightSingleLanePerSampleDirFmt \\ -- output - path analysis / seqs / combined . qza Remove primers \u00b6 Important Remember to ask you sequencing facility if the raw data you get has the primers attached - they may have already been removed. These sequences still have the primers attached - they need to be removed (using cutadapt ) before denoising. 1 2 3 4 5 6 7 qiime cutadapt trim - paired \\ -- i - demultiplexed - sequences analysis / seqs / combined . qza \\ -- p - front - f AGGATTAGATACCCTGGTA \\ -- p - front - r CRRCACGAGCTGACGAC \\ -- p - error - rate 0.20 \\ -- output - dir analysis / seqs_trimmed \\ -- verbose Attention The primers specified (784f and 1492r for the bacterial 16S rRNA gene) correspond to this specific experiment - they will likely not work for your own data analyses. Attention The error rate parameter, -- p - error - rate , will likely need to be adjusted for your own sample data to get 100% (or close to it) of reads trimmed. Create and interpret sequence quality data \u00b6 Create a viewable summary file so the data quality can be checked. Viewing the quality plots generated here helps determine trim settings. Things to look for: Where does the median quality drop below 30? Do any of the samples have only a few sequences e.g. <1000? If so, you may want to omit them from the analysis later on in R. Create a subdirectory in analysis called visualisations to store all files that we will visualise in one place. 1 mkdir analysis/visualisations 1 2 3 qiime demux summarize \\ -- i - data analysis / seqs_trimmed / trimmed_sequences . qza \\ -- o - visualization analysis / visualisations / trimmed_sequences . qzv Copy analysis / visualisations / trimmed_sequences . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: Read quality and demux output Alternatively, if you have QIIME2 locally installed and are running it on your own computer , you can view the visualisation from the command-line: 1 qiime tools view analysis / visualisations / trimmed_sequences . qzv Denoising the data \u00b6 Trimmed sequences are now quality assessed using the dada2 plugin within QIIME2. dada2 denoises data by modelling and correcting Illumina-sequenced amplicon errors, and infers exact amplicon sequence variants ( ASVs ), resolving differences of as little as 1 nucleotide. Its workflow consists of filtering, de-replication, reference\u2010free chimera detection, and paired\u2010end reads merging, resulting in a feature or ASV table. Note This step may long time to run (i.e. days), depending on files sizes and computational power. Remember to adjust p - trunc - len - f and p - trunc - len - r values according to your own data. Question: Based on your assessment of the quality plots from the trimmed_sequences.qzv file generated in the previous step, what values would you select for p - trunc - len - f and p - trunc - len - r in the command below? Answer p - trunc - len - f 212 and p - trunc - len - r 167 The specified output directory must not pre-exist. 1 2 3 4 5 6 7 qiime dada2 denoise - paired \\ -- i - demultiplexed - seqs analysis / seqs_trimmed / trimmed_sequences . qza \\ -- p - trunc - len - f xx \\ -- p - trunc - len - r xx \\ -- p - n - threads 0 \\ -- output - dir analysis / dada2out \\ -- verbose Generate summary files \u00b6 A metadata file is required which provides the key to gaining biological insight from your data. The file metadata.tsv is provided in the home directory of your Nectar instance. This spreadsheet has already been verified using the plugin for Google Sheets, keemei . Things to look for: How many features ( ASVs ) were generated? Are the communities high or low diversity? Do BLAST searches of the representative sequences make sense? Are the features what you would expect e.g. marine or terrestrial? Have a large number (e.g. >50%) of sequences been lost during denoising/filtering? If so, the settings might be too stringent. 1 2 3 4 5 qiime feature - table summarize \\ -- i - table analysis / dada2out / table . qza \\ -- m - sample - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_table . qzv \\ -- verbose Copy analysis / visualisations / 16 s_table . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: ASVs summary 1 2 3 4 qiime feature - table tabulate - seqs \\ -- i - data analysis / dada2out / representative_sequences . qza \\ -- o - visualization analysis / visualisations / 16 s_representative_seqs . qzv \\ -- verbose Copy analysis / visualisations / 16 s_rep_seqs . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Representative Sequences 1 2 3 4 qiime metadata tabulate \\ -- m - input - file analysis / dada2out / denoising_stats . qza \\ -- o - visualization analysis / visualisations / 16 s_denoising_stats . qzv \\ -- verbose Copy analysis / visualisations / 16 s_denoising_stats . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: dada2 output Section 2: Taxonomic Analysis \u00b6 Assign taxonomy \u00b6 Here we will classify each identical read or Amplicon Sequence Variant (ASV) to the highest resolution based on a database. Common databases for bacteria datasets are Greengenes , SILVA , Ribosomal Database Project , or Genome Taxonomy Database . See Porter and Hajibabaei, 2020 for a review of different classifiers for metabarcoding research. The classifier chosen is dependent upon: Previously published data in a field The target region of interest The number of reference sequences for your organism in the database and how recently that database was updated. A classifier has already been trained for you for the V5V6 region of the bacterial 16S rRNA gene using the SILVA database. The next step will take a while to run. The output directory cannot previously exist . n_jobs = 1 This runs the script using all available cores Note The classifier used here is only appropriate for the specific 16S rRNA region that this data represents. You will need to train your own - no worries, QIIME2 has a tutorial for that. 1 2 3 4 5 6 qiime feature - classifier classify - sklearn \\ -- i - classifier silva_132_16s_v5v6_classifier . qza \\ -- i - reads analysis / dada2out / representative_sequences . qza \\ -- p - n - jobs 1 \\ -- output - dir analysis / taxonomy \\ -- verbose Warning This step often runs out of memory on full datasets. Some options are to change the number of cores you are using (adjust --p-n-jobs ) or add --p-reads-per-batch 10000 and try again. The QIIME 2 forum has many threads regarding this issue so always check there was well. Generate a viewable summary file of the taxonomic assignments. \u00b6 1 2 3 4 qiime metadata tabulate \\ -- m - input - file analysis / taxonomy / classification . qza \\ -- o - visualization analysis / visualisations / taxonomy . qzv \\ -- verbose Copy analysis / visualisations / taxonomy . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Taxonomy Filtering \u00b6 Filter out reads classified as mitochondria and chloroplast. Unassigned ASVs are retained. Generate a viewable summary file of the new table to see the effect of filtering. According to QIIME developer Nicholas Bokulich, low abundance filtering (i.e. removing ASVs containing very few sequences) is not necessary under the ASV model. 1 2 3 4 5 6 qiime taxa filter - table \\ -- i - table analysis / dada2out / table . qza \\ -- i - taxonomy analysis / taxonomy / classification . qza \\ -- p - exclude Mitochondria , Chloroplast \\ -- o - filtered - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- verbose 1 2 3 4 5 qiime feature - table summarize \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- m - sample - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_table_filtered . qzv \\ -- verbose Copy analysis / visualisations / 16 s_table_filtered . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: 16s_table_filtered Section 3: Build a phylogenetic tree \u00b6 The next step does the following: Perform an alignment on the representative sequences. Mask highly variable regions of the alignment. Generate a phylogenetic tree. Apply mid-point rooting to the tree. A phylogenetic tree is necessary for any analyses that incorporates information on the relative relatedness of community members, by incorporating phylogenetic distances between observed organisms in the computation. This would include any beta-diversity analyses and visualisations from a weighted or unweighted Unifrac distance matrix. 1 mkdir analysis/tree Use one thread only (which is the default action) so that identical results can be produced if rerun. 1 2 3 4 5 6 7 8 qiime phylogeny align - to - tree - mafft - fasttree \\ -- i - sequences analysis / dada2out / representative_sequences . qza \\ -- o - alignment analysis / tree / aligned_16s_representative_seqs . qza \\ -- o - masked - alignment analysis / tree / masked_aligned_16s_representative_seqs . qza \\ -- o - tree analysis / tree / 16 s_unrooted_tree . qza \\ -- o - rooted - tree analysis / tree / 16 s_rooted_tree . qza \\ -- p - n - threads 1 \\ -- verbose Section 4: Basic visualisations and statistics \u00b6 Rarefaction curves \u00b6 Generate rarefaction curves to determine whether the samples have been sequenced deeply enough to capture all the community members. The max depth setting will depend on the number of sequences in your samples. Things to look for: Do the curves for each sample plateau? If they don\u2019t, the samples haven\u2019t been sequenced deeply enough to capture the full diversity of the bacterial communities, which is shown on the y-axis. At what sequencing depth (x-axis) do your curves plateau? This value will be important for downstream analyses, particularly for alpha diversity analyses. Make a directory for downstream analysis results. 1 mkdir analysis/downstream Note The value that you provide for \u2013p-max-depth should be determined by reviewing the \u201cFrequency per sample\u201d information presented in the 16s_table_filtered.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don\u2019t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth. 1 2 3 4 5 6 7 qiime diversity alpha - rarefaction \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- i - phylogeny analysis / tree / 16 s_rooted_tree . qza \\ -- p - max - depth 9062 \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_alpha_rarefaction . qzv \\ -- verbose Copy analysis / visualisations / 16 s_alpha_rarefaction . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Rarefaction ASV relative abundance bar charts \u00b6 Create bar charts to compare the relative abundance of ASVs across samples. 1 2 3 4 5 6 qiime taxa barplot \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- i - taxonomy analysis / taxonomy / classification . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / barchart . qzv \\ -- verbose Copy analysis / visualisations / barchart . qzv to your local computer and view in QIIME 2 View (q2view). Try selecting different taxonomic levels and metadata-based sample sorting. Visualisations: Taxonomy Barplots Alpha and beta diversity analysis \u00b6 The following is taken directly from the Moving Pictures tutorial and adapted for this data set. QIIME 2\u2019s diversity analyses are available through the q2 - diversity plugin, which supports computing alpha- and beta- diversity metrics, applying related statistical tests, and generating interactive visualisations. We\u2019ll first apply the core-metrics-phylogenetic method, which rarefies a FeatureTable[Frequency] to a user-specified depth, computes several alpha- and beta- diversity metrics, and generates principle coordinates analysis (PCoA) plots using Emperor for each of the beta diversity metrics. The metrics computed by default are: Alpha diversity (operate on a single sample (i.e. within sample diversity)). Shannon\u2019s diversity index (a quantitative measure of community richness) Observed OTUs (a qualitative measure of community richness) Faith\u2019s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features) Evenness (or Pielou\u2019s Evenness; a measure of community evenness) Beta diversity (operate on a pair of samples (i.e. between sample diversity)). Jaccard distance (a qualitative measure of community dissimilarity) Bray-Curtis distance (a quantitative measure of community dissimilarity) unweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features) weighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features) An important parameter that needs to be provided to this script is --p-sampling-depth , which is the even sampling (i.e. rarefaction) depth that was determined above. As most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if --p-sampling-depth 500 is provided, this step will subsample the counts in each sample without replacement, so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be excluded from the diversity analysis. Choosing this value is tricky. We recommend making your choice by reviewing the information presented in the 16s_table_filtered.qzv file that was created above. Choose a value that is as high as possible (so more sequences per sample are retained), while excluding as few samples as possible. 1 2 3 4 5 6 qiime diversity core - metrics - phylogenetic \\ -- i - phylogeny analysis / tree / 16 s_rooted_tree . qza \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- p - sampling - depth 5588 \\ -- m - metadata - file metadata . tsv \\ -- output - dir analysis / diversity_metrics Copy the .qzv files created from the above command into the visualisations subdirectory. 1 cp analysis/diversity_metrics/*.qzv analysis/visualisations To view the differences between sample composition using unweighted UniFrac in ordination space, copy analysis / visualisations / weighted_unifrac_emperor . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: Weighted UniFrac Emperor Ordination On q2view, select the \u201cColour\u201d tab and the heading \u201cEnvironment\u201d in the dropdown menu and then by \u201cGenotype\u201d in the \u201cShape\u201d tab. Next, we\u2019ll test for associations between categorical metadata columns and alpha diversity data. We\u2019ll do that here for the Faith Phylogenetic Diversity (a measure of community richness) and evenness metrics. 1 2 3 4 qiime diversity alpha - group - significance \\ -- i - alpha - diversity analysis / diversity_metrics / faith_pd_vector . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / faith - pd - group - significance . qzv Copy analysis / visualisations / faith - pd - group - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Faith Phylogenetic Diversity output 1 2 3 4 qiime diversity alpha - group - significance \\ -- i - alpha - diversity analysis / diversity_metrics / evenness_vector . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / evenness - group - significance . qzv Copy analysis / visualisations / evenness - group - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Evenness output Finally, we\u2019ll analyse sample composition in the context of categorical metadata using a permutational multivariate analysis of variance (PERMANOVA, first described in Anderson (2001)) test using the beta-group-significance command. The following commands will test whether distances between samples within a group, such as samples from the same genotype, are more similar to each other then they are to samples from the other groups. If you call this command with the --p-pairwise parameter, as we\u2019ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., AIMS1 and AIMS4) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise , since it is based on permutation tests. So, unlike the previous commands, we\u2019ll run beta-group-significance on specific columns of metadata that we\u2019re interested in exploring, rather than all metadata columns to which it is applicable. Here we\u2019ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows. 1 2 3 4 5 6 qiime diversity beta - group - significance \\ -- i - distance - matrix analysis / diversity_metrics / unweighted_unifrac_distance_matrix . qza \\ -- m - metadata - file metadata . tsv \\ -- m - metadata - column Genotype \\ -- o - visualization analysis / visualisations / unweighted - unifrac - genotype - significance . qzv \\ -- p - pairwise Copy analysis / visualisations / unweighted - unifrac - genotype - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Genotype significance output 1 2 3 4 5 6 qiime diversity beta - group - significance \\ -- i - distance - matrix analysis / diversity_metrics / unweighted_unifrac_distance_matrix . qza \\ -- m - metadata - file metadata . tsv \\ -- m - metadata - column Environment \\ -- o - visualization analysis / visualisations / unweighted - unifrac - environment - significance . qzv \\ -- p - pairwise Copy analysis / visualisations / unweighted - unifrac - environment - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Environmental significance output Section 5: Exporting data for further analysis in R \u00b6 You need to export your ASV table, taxonomy table, and tree file for analyses in R. Many file formats can be accepted. Export unrooted tree as .nwk format as required for the R package phyloseq . 1 2 3 qiime tools export \\ -- input - path analysis / tree / 16 s_unrooted_tree . qza \\ -- output - path analysis / export Create a BIOM table with taxonomy annotations. A FeatureTable[Frequency] artefact will be exported as a BIOM v2.1.0 formatted file. 1 2 3 qiime tools export \\ -- input - path analysis / taxonomy / 16 s_table_filtered . qza \\ -- output - path analysis / export Then export BIOM to TSV 1 2 3 4 biom convert \\ - i analysis / export / feature - table . biom \\ - o analysis / export / feature - table . tsv \\ -- to - tsv Export Taxonomy as TSV 1 2 3 qiime tools export \\ -- input - path analysis / taxonomy / classification . qza \\ -- output - path analysis / export Delete the header lines of the .tsv files 1 2 sed '1d' analysis/export/taxonomy.tsv > analysis/export/taxonomy_noHeader.tsv sed '1,2d' analysis/export/feature-table.tsv > analysis/export/feature-table_noHeader.tsv Some packages require your data to be in a consistent order, i.e. the order of your ASVs in the taxonomy table rows to be the same order of ASVs in the columns of your ASV table. It\u2019s recommended to clean up your taxonomy file. You can have blank spots where the level of classification was not completely resolved.","title":"QIIME2"},{"location":"tutorials/qiime2/qiime2/#qiime2","text":"Anticipated workshop duration when delivered to a group of participants is 4 hours . For queries relating to this workshop, contact Melbourne Bioinformatics ( bioinformatics-training@unimelb.edu.au ).","title":"QIIME2"},{"location":"tutorials/qiime2/qiime2/#overview","text":"","title":"Overview"},{"location":"tutorials/qiime2/qiime2/#topic","text":"Genomics Transcriptomics Proteomics Metabolomics Statistics and visualisation Structural Modelling Basic skills","title":"Topic"},{"location":"tutorials/qiime2/qiime2/#skill-level","text":"Beginner Intermediate Advanced This workshop is designed for participants with command-line knowledge. You will need to be able to ssh into a remote machine, navigate the directory structure and scp files from a remote computer to your local computer.","title":"Skill level"},{"location":"tutorials/qiime2/qiime2/#description","text":"What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities? Data: Illumina MiSeq v3 paired-end (2 \u00d7 300 bp) reads (FASTQ). Tools: QIIME 2 Pipeline: Section 1: Importing, cleaning and quality control of the data Section 2: Taxonomic Analysis Section 3: Building a phylogenetic tree Section 4: Basic visualisations and statistics Section 5: Exporting data for further analysis in R","title":"Description"},{"location":"tutorials/qiime2/qiime2/#learning-objectives","text":"At the end of this introductory workshop, you will: Take raw data from a sequencing facility and end with publication quality graphics and statistics Answer the question What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities?","title":"Learning Objectives"},{"location":"tutorials/qiime2/qiime2/#requirements-and-preparation","text":"Important Attendees are required to use their own laptop computers. At least one week before the workshop, if required, participants should install the software below. This should provide sufficient time for participants to liaise with their own IT support should they encounter any IT problems.","title":"Requirements and preparation"},{"location":"tutorials/qiime2/qiime2/#required-software","text":"Mac Users: No additional software needs to be installed for this workshop. Windows Users: 1. A terminal emulator such as PuTTY (free and open-source) will need to be downloaded. 2. Software for file transfers between a local computer and remote server such as WinSCP or FileZilla .","title":"Required Software"},{"location":"tutorials/qiime2/qiime2/#required-data","text":"No additional data needs to be downloaded for this workshop. It is located in the directory raw_data located in the home directory of the Nectar instance that you will log in to. If you wish to analyse the data independently at a later stage, it can be downloaded from here . This zipped folder contains both the FASTQs and associated metadata file.","title":"Required Data"},{"location":"tutorials/qiime2/qiime2/#mode-of-delivery","text":"This workshop will be run on a Nectar Instance. An \u201cInstance\u201d is Nectar terminology for a virtual machine running on the Nectar Cloud OpenStack infrastructure. An \u201cInstance\u201d runs on a \u201ccompute node\u201d; i.e. a physical computer populated with processor chips, memory chips and so on. You will be given an individual IP address and password to log on to using the SSH client tool on your computer (Terminal on Mac or PuTTY on Windows). 1 ssh ubuntu@ip-address Should you wish to do this tutorial at a later stage independently, it is possible to apply for your own instance directly through a Nectar allocation . There are also many helpful Nectar Research Cloud tutorials .","title":"Mode of Delivery"},{"location":"tutorials/qiime2/qiime2/#author-information","text":"Written by: Ashley Dungan and Gayle Philip School of Biosciences, University of Melbourne; Melbourne Bioinformatics Created/Reviewed: May 2021","title":"Author Information"},{"location":"tutorials/qiime2/qiime2/#background","text":"What is the influence of genotype (intrinsic) and environment (extrinsic) on anemone-associated bacterial communities?","title":"Background"},{"location":"tutorials/qiime2/qiime2/#the-players","text":"Exaiptasia diaphana - a shallow-water, marine anemone that is often used in research as a model organism for corals. In this experiment, two genotypes (AIMS1 and AIMS4) of E. diaphana were grown in each of two different environments: sterile seawater OR unfiltered control seawater The anemone-associated bacterial communities or microbiome - these bacteria live on, or within E. diaphana , and likely consist of a combination of commensals, transients, and long-term stable members, and combined with their host, form a mutually beneficial, stable symbiosis.","title":"The Players"},{"location":"tutorials/qiime2/qiime2/#the-study","text":"The anemone microbiome contributes to the overall health of this complex system and can evolve in tandem with the anemone host. In this data set we are looking at the impact of intrinsic and extrinsic factors on anemone microbiome composition. After three weeks in either sterile or control seawater (environment), anemones were homogenized and DNA was extracted. There are 23 samples in this data set - 5 from each anemone treatment combination (2 genotypes x 2 environments) and 3 DNA extraction blanks as controls. This data is a subset from a larger experiment . Dungan AM, van Oppen MJH, and Blackall LL (2021) Short-Term Exposure to Sterile Seawater Reduces Bacterial Community Diversity in the Sea Anemone, Exaiptasia diaphana . Front. Mar. Sci. 7:599314. doi:10.3389/fmars.2020.599314 [Full Text] .","title":"The Study"},{"location":"tutorials/qiime2/qiime2/#qiime-2-analysis-platform","text":"Quantitative Insights Into Microbial Ecology 2 ( QIIME 2\u2122 ) is a next-generation microbiome bioinformatics platform that is extensible, free, open source, and community developed. It allows researchers to: Automatically track analyses with decentralised data provenance Interactively explore data with beautiful visualisations Easily share results without QIIME 2 installed Plugin-based system \u2014 researchers can add in tools as they wish","title":"QIIME 2 Analysis platform"},{"location":"tutorials/qiime2/qiime2/#viewing-qiime2-visualisations","text":"As this workshop is being run on a remote Nectar Instance, you will need to download the visual files ( *.qzv ) to your local computer and view them in QIIME 2 View (q2view). Attention We will be doing this step multiple times throughout this workshop to view visualisation files as they are generated. The syntax to do this depends on whether you are running the copying command on your local computer, or on the remote computer (Nectar cloud). When running the command from your local computer, the syntax for copying a file from Nectar is: 1 scp ubuntu@your_IP_address:FILENAME /PATH/TO/TARGET/FOLDER/ Running the command on the remote computer, the syntax for copying a file to your local computer is: 1 scp FILENAME username@your_IP_address:/PATH/TO/TARGET/FOLDER/ Alternatively, if you have QIIME2 installed and are running it on your own computer , you can use qiime tools view to view the results from the command line. qiime tools view opens a browser window with your visualization loaded in it. When you are done, you can close the browser window and press ctrl - c on the keyboard to terminate the command. 1 qiime tools view filename . qzv","title":"Viewing QIIME2 visualisations"},{"location":"tutorials/qiime2/qiime2/#section-1-importing-cleaning-and-quality-control-of-the-data","text":"","title":"Section 1: Importing, cleaning and quality control of the data"},{"location":"tutorials/qiime2/qiime2/#import-data","text":"These samples were sequenced on a single Illumina MiSeq run using v3 (2 \u00d7 300 bp) reagents at the Walter and Eliza Hall Institute (WEHI), Melbourne, Australia. Data from WEHI came as paired-end, demultiplexed, unzipped *.fastq files with adapters still attached. Following the QIIME2 importing tutorial , this is the Casava One Eight format. The files have been renamed to satisfy the Casava format as SampleID_FWDXX-REVXX_L001_R[1 or 2]_001.fastq e.g. CTRLA_Fwd04-Rev25_L001_R1_001.fastq.gz. The files were then zipped (.gzip). Here, the data files (two per sample i.e. forward and reverse reads R1 and R2 respectively) will be imported and exported as a single QIIME 2 artefact file. These samples are already demultiplexed (i.e. sequences from each sample have been written to separate files), so a metadata file is not initially required. Note To check the input syntax for any QIIME2 command, enter the command, followed by --help e.g. qiime tools import -- help Start by making a new directory analysis to store all the output files from this tutorial. In addition, we will create a subdirectory called seqs to store the exported sequences. 1 2 cd mkdir -p analysis/seqs Run the command to import the raw data located in the directory raw_data and export it to a single QIIME 2 artefact file, combined . qza . 1 2 3 4 5 qiime tools import \\ -- type 'SampleData[PairedEndSequencesWithQuality]' \\ -- input - path raw_data \\ -- input - format CasavaOneEightSingleLanePerSampleDirFmt \\ -- output - path analysis / seqs / combined . qza","title":"Import data"},{"location":"tutorials/qiime2/qiime2/#remove-primers","text":"Important Remember to ask you sequencing facility if the raw data you get has the primers attached - they may have already been removed. These sequences still have the primers attached - they need to be removed (using cutadapt ) before denoising. 1 2 3 4 5 6 7 qiime cutadapt trim - paired \\ -- i - demultiplexed - sequences analysis / seqs / combined . qza \\ -- p - front - f AGGATTAGATACCCTGGTA \\ -- p - front - r CRRCACGAGCTGACGAC \\ -- p - error - rate 0.20 \\ -- output - dir analysis / seqs_trimmed \\ -- verbose Attention The primers specified (784f and 1492r for the bacterial 16S rRNA gene) correspond to this specific experiment - they will likely not work for your own data analyses. Attention The error rate parameter, -- p - error - rate , will likely need to be adjusted for your own sample data to get 100% (or close to it) of reads trimmed.","title":"Remove primers"},{"location":"tutorials/qiime2/qiime2/#create-and-interpret-sequence-quality-data","text":"Create a viewable summary file so the data quality can be checked. Viewing the quality plots generated here helps determine trim settings. Things to look for: Where does the median quality drop below 30? Do any of the samples have only a few sequences e.g. <1000? If so, you may want to omit them from the analysis later on in R. Create a subdirectory in analysis called visualisations to store all files that we will visualise in one place. 1 mkdir analysis/visualisations 1 2 3 qiime demux summarize \\ -- i - data analysis / seqs_trimmed / trimmed_sequences . qza \\ -- o - visualization analysis / visualisations / trimmed_sequences . qzv Copy analysis / visualisations / trimmed_sequences . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: Read quality and demux output Alternatively, if you have QIIME2 locally installed and are running it on your own computer , you can view the visualisation from the command-line: 1 qiime tools view analysis / visualisations / trimmed_sequences . qzv","title":"Create and interpret sequence quality data"},{"location":"tutorials/qiime2/qiime2/#denoising-the-data","text":"Trimmed sequences are now quality assessed using the dada2 plugin within QIIME2. dada2 denoises data by modelling and correcting Illumina-sequenced amplicon errors, and infers exact amplicon sequence variants ( ASVs ), resolving differences of as little as 1 nucleotide. Its workflow consists of filtering, de-replication, reference\u2010free chimera detection, and paired\u2010end reads merging, resulting in a feature or ASV table. Note This step may long time to run (i.e. days), depending on files sizes and computational power. Remember to adjust p - trunc - len - f and p - trunc - len - r values according to your own data. Question: Based on your assessment of the quality plots from the trimmed_sequences.qzv file generated in the previous step, what values would you select for p - trunc - len - f and p - trunc - len - r in the command below? Answer p - trunc - len - f 212 and p - trunc - len - r 167 The specified output directory must not pre-exist. 1 2 3 4 5 6 7 qiime dada2 denoise - paired \\ -- i - demultiplexed - seqs analysis / seqs_trimmed / trimmed_sequences . qza \\ -- p - trunc - len - f xx \\ -- p - trunc - len - r xx \\ -- p - n - threads 0 \\ -- output - dir analysis / dada2out \\ -- verbose","title":"Denoising the data"},{"location":"tutorials/qiime2/qiime2/#generate-summary-files","text":"A metadata file is required which provides the key to gaining biological insight from your data. The file metadata.tsv is provided in the home directory of your Nectar instance. This spreadsheet has already been verified using the plugin for Google Sheets, keemei . Things to look for: How many features ( ASVs ) were generated? Are the communities high or low diversity? Do BLAST searches of the representative sequences make sense? Are the features what you would expect e.g. marine or terrestrial? Have a large number (e.g. >50%) of sequences been lost during denoising/filtering? If so, the settings might be too stringent. 1 2 3 4 5 qiime feature - table summarize \\ -- i - table analysis / dada2out / table . qza \\ -- m - sample - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_table . qzv \\ -- verbose Copy analysis / visualisations / 16 s_table . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: ASVs summary 1 2 3 4 qiime feature - table tabulate - seqs \\ -- i - data analysis / dada2out / representative_sequences . qza \\ -- o - visualization analysis / visualisations / 16 s_representative_seqs . qzv \\ -- verbose Copy analysis / visualisations / 16 s_rep_seqs . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Representative Sequences 1 2 3 4 qiime metadata tabulate \\ -- m - input - file analysis / dada2out / denoising_stats . qza \\ -- o - visualization analysis / visualisations / 16 s_denoising_stats . qzv \\ -- verbose Copy analysis / visualisations / 16 s_denoising_stats . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: dada2 output","title":"Generate summary files"},{"location":"tutorials/qiime2/qiime2/#section-2-taxonomic-analysis","text":"","title":"Section 2: Taxonomic Analysis"},{"location":"tutorials/qiime2/qiime2/#assign-taxonomy","text":"Here we will classify each identical read or Amplicon Sequence Variant (ASV) to the highest resolution based on a database. Common databases for bacteria datasets are Greengenes , SILVA , Ribosomal Database Project , or Genome Taxonomy Database . See Porter and Hajibabaei, 2020 for a review of different classifiers for metabarcoding research. The classifier chosen is dependent upon: Previously published data in a field The target region of interest The number of reference sequences for your organism in the database and how recently that database was updated. A classifier has already been trained for you for the V5V6 region of the bacterial 16S rRNA gene using the SILVA database. The next step will take a while to run. The output directory cannot previously exist . n_jobs = 1 This runs the script using all available cores Note The classifier used here is only appropriate for the specific 16S rRNA region that this data represents. You will need to train your own - no worries, QIIME2 has a tutorial for that. 1 2 3 4 5 6 qiime feature - classifier classify - sklearn \\ -- i - classifier silva_132_16s_v5v6_classifier . qza \\ -- i - reads analysis / dada2out / representative_sequences . qza \\ -- p - n - jobs 1 \\ -- output - dir analysis / taxonomy \\ -- verbose Warning This step often runs out of memory on full datasets. Some options are to change the number of cores you are using (adjust --p-n-jobs ) or add --p-reads-per-batch 10000 and try again. The QIIME 2 forum has many threads regarding this issue so always check there was well.","title":"Assign taxonomy"},{"location":"tutorials/qiime2/qiime2/#generate-a-viewable-summary-file-of-the-taxonomic-assignments","text":"1 2 3 4 qiime metadata tabulate \\ -- m - input - file analysis / taxonomy / classification . qza \\ -- o - visualization analysis / visualisations / taxonomy . qzv \\ -- verbose Copy analysis / visualisations / taxonomy . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Taxonomy","title":"Generate a viewable summary file of the taxonomic assignments."},{"location":"tutorials/qiime2/qiime2/#filtering","text":"Filter out reads classified as mitochondria and chloroplast. Unassigned ASVs are retained. Generate a viewable summary file of the new table to see the effect of filtering. According to QIIME developer Nicholas Bokulich, low abundance filtering (i.e. removing ASVs containing very few sequences) is not necessary under the ASV model. 1 2 3 4 5 6 qiime taxa filter - table \\ -- i - table analysis / dada2out / table . qza \\ -- i - taxonomy analysis / taxonomy / classification . qza \\ -- p - exclude Mitochondria , Chloroplast \\ -- o - filtered - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- verbose 1 2 3 4 5 qiime feature - table summarize \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- m - sample - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_table_filtered . qzv \\ -- verbose Copy analysis / visualisations / 16 s_table_filtered . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: 16s_table_filtered","title":"Filtering"},{"location":"tutorials/qiime2/qiime2/#section-3-build-a-phylogenetic-tree","text":"The next step does the following: Perform an alignment on the representative sequences. Mask highly variable regions of the alignment. Generate a phylogenetic tree. Apply mid-point rooting to the tree. A phylogenetic tree is necessary for any analyses that incorporates information on the relative relatedness of community members, by incorporating phylogenetic distances between observed organisms in the computation. This would include any beta-diversity analyses and visualisations from a weighted or unweighted Unifrac distance matrix. 1 mkdir analysis/tree Use one thread only (which is the default action) so that identical results can be produced if rerun. 1 2 3 4 5 6 7 8 qiime phylogeny align - to - tree - mafft - fasttree \\ -- i - sequences analysis / dada2out / representative_sequences . qza \\ -- o - alignment analysis / tree / aligned_16s_representative_seqs . qza \\ -- o - masked - alignment analysis / tree / masked_aligned_16s_representative_seqs . qza \\ -- o - tree analysis / tree / 16 s_unrooted_tree . qza \\ -- o - rooted - tree analysis / tree / 16 s_rooted_tree . qza \\ -- p - n - threads 1 \\ -- verbose","title":"Section 3: Build a phylogenetic tree"},{"location":"tutorials/qiime2/qiime2/#section-4-basic-visualisations-and-statistics","text":"","title":"Section 4: Basic visualisations and statistics"},{"location":"tutorials/qiime2/qiime2/#rarefaction-curves","text":"Generate rarefaction curves to determine whether the samples have been sequenced deeply enough to capture all the community members. The max depth setting will depend on the number of sequences in your samples. Things to look for: Do the curves for each sample plateau? If they don\u2019t, the samples haven\u2019t been sequenced deeply enough to capture the full diversity of the bacterial communities, which is shown on the y-axis. At what sequencing depth (x-axis) do your curves plateau? This value will be important for downstream analyses, particularly for alpha diversity analyses. Make a directory for downstream analysis results. 1 mkdir analysis/downstream Note The value that you provide for \u2013p-max-depth should be determined by reviewing the \u201cFrequency per sample\u201d information presented in the 16s_table_filtered.qzv file that was created above. In general, choosing a value that is somewhere around the median frequency seems to work well, but you may want to increase that value if the lines in the resulting rarefaction plot don\u2019t appear to be leveling out, or decrease that value if you seem to be losing many of your samples due to low total frequencies closer to the minimum sampling depth than the maximum sampling depth. 1 2 3 4 5 6 7 qiime diversity alpha - rarefaction \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- i - phylogeny analysis / tree / 16 s_rooted_tree . qza \\ -- p - max - depth 9062 \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / 16 s_alpha_rarefaction . qzv \\ -- verbose Copy analysis / visualisations / 16 s_alpha_rarefaction . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Rarefaction","title":"Rarefaction curves"},{"location":"tutorials/qiime2/qiime2/#asv-relative-abundance-bar-charts","text":"Create bar charts to compare the relative abundance of ASVs across samples. 1 2 3 4 5 6 qiime taxa barplot \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- i - taxonomy analysis / taxonomy / classification . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / barchart . qzv \\ -- verbose Copy analysis / visualisations / barchart . qzv to your local computer and view in QIIME 2 View (q2view). Try selecting different taxonomic levels and metadata-based sample sorting. Visualisations: Taxonomy Barplots","title":"ASV relative abundance bar charts"},{"location":"tutorials/qiime2/qiime2/#alpha-and-beta-diversity-analysis","text":"The following is taken directly from the Moving Pictures tutorial and adapted for this data set. QIIME 2\u2019s diversity analyses are available through the q2 - diversity plugin, which supports computing alpha- and beta- diversity metrics, applying related statistical tests, and generating interactive visualisations. We\u2019ll first apply the core-metrics-phylogenetic method, which rarefies a FeatureTable[Frequency] to a user-specified depth, computes several alpha- and beta- diversity metrics, and generates principle coordinates analysis (PCoA) plots using Emperor for each of the beta diversity metrics. The metrics computed by default are: Alpha diversity (operate on a single sample (i.e. within sample diversity)). Shannon\u2019s diversity index (a quantitative measure of community richness) Observed OTUs (a qualitative measure of community richness) Faith\u2019s Phylogenetic Diversity (a qualitative measure of community richness that incorporates phylogenetic relationships between the features) Evenness (or Pielou\u2019s Evenness; a measure of community evenness) Beta diversity (operate on a pair of samples (i.e. between sample diversity)). Jaccard distance (a qualitative measure of community dissimilarity) Bray-Curtis distance (a quantitative measure of community dissimilarity) unweighted UniFrac distance (a qualitative measure of community dissimilarity that incorporates phylogenetic relationships between the features) weighted UniFrac distance (a quantitative measure of community dissimilarity that incorporates phylogenetic relationships between the features) An important parameter that needs to be provided to this script is --p-sampling-depth , which is the even sampling (i.e. rarefaction) depth that was determined above. As most diversity metrics are sensitive to different sampling depths across different samples, this script will randomly subsample the counts from each sample to the value provided for this parameter. For example, if --p-sampling-depth 500 is provided, this step will subsample the counts in each sample without replacement, so that each sample in the resulting table has a total count of 500. If the total count for any sample(s) are smaller than this value, those samples will be excluded from the diversity analysis. Choosing this value is tricky. We recommend making your choice by reviewing the information presented in the 16s_table_filtered.qzv file that was created above. Choose a value that is as high as possible (so more sequences per sample are retained), while excluding as few samples as possible. 1 2 3 4 5 6 qiime diversity core - metrics - phylogenetic \\ -- i - phylogeny analysis / tree / 16 s_rooted_tree . qza \\ -- i - table analysis / taxonomy / 16 s_table_filtered . qza \\ -- p - sampling - depth 5588 \\ -- m - metadata - file metadata . tsv \\ -- output - dir analysis / diversity_metrics Copy the .qzv files created from the above command into the visualisations subdirectory. 1 cp analysis/diversity_metrics/*.qzv analysis/visualisations To view the differences between sample composition using unweighted UniFrac in ordination space, copy analysis / visualisations / weighted_unifrac_emperor . qzv to your local computer and view in QIIME 2 View (q2view). Visualisations: Weighted UniFrac Emperor Ordination On q2view, select the \u201cColour\u201d tab and the heading \u201cEnvironment\u201d in the dropdown menu and then by \u201cGenotype\u201d in the \u201cShape\u201d tab. Next, we\u2019ll test for associations between categorical metadata columns and alpha diversity data. We\u2019ll do that here for the Faith Phylogenetic Diversity (a measure of community richness) and evenness metrics. 1 2 3 4 qiime diversity alpha - group - significance \\ -- i - alpha - diversity analysis / diversity_metrics / faith_pd_vector . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / faith - pd - group - significance . qzv Copy analysis / visualisations / faith - pd - group - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Faith Phylogenetic Diversity output 1 2 3 4 qiime diversity alpha - group - significance \\ -- i - alpha - diversity analysis / diversity_metrics / evenness_vector . qza \\ -- m - metadata - file metadata . tsv \\ -- o - visualization analysis / visualisations / evenness - group - significance . qzv Copy analysis / visualisations / evenness - group - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Evenness output Finally, we\u2019ll analyse sample composition in the context of categorical metadata using a permutational multivariate analysis of variance (PERMANOVA, first described in Anderson (2001)) test using the beta-group-significance command. The following commands will test whether distances between samples within a group, such as samples from the same genotype, are more similar to each other then they are to samples from the other groups. If you call this command with the --p-pairwise parameter, as we\u2019ll do here, it will also perform pairwise tests that will allow you to determine which specific pairs of groups (e.g., AIMS1 and AIMS4) differ from one another, if any. This command can be slow to run, especially when passing --p-pairwise , since it is based on permutation tests. So, unlike the previous commands, we\u2019ll run beta-group-significance on specific columns of metadata that we\u2019re interested in exploring, rather than all metadata columns to which it is applicable. Here we\u2019ll apply this to our unweighted UniFrac distances, using two sample metadata columns, as follows. 1 2 3 4 5 6 qiime diversity beta - group - significance \\ -- i - distance - matrix analysis / diversity_metrics / unweighted_unifrac_distance_matrix . qza \\ -- m - metadata - file metadata . tsv \\ -- m - metadata - column Genotype \\ -- o - visualization analysis / visualisations / unweighted - unifrac - genotype - significance . qzv \\ -- p - pairwise Copy analysis / visualisations / unweighted - unifrac - genotype - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Genotype significance output 1 2 3 4 5 6 qiime diversity beta - group - significance \\ -- i - distance - matrix analysis / diversity_metrics / unweighted_unifrac_distance_matrix . qza \\ -- m - metadata - file metadata . tsv \\ -- m - metadata - column Environment \\ -- o - visualization analysis / visualisations / unweighted - unifrac - environment - significance . qzv \\ -- p - pairwise Copy analysis / visualisations / unweighted - unifrac - environment - significance . qzv to your local computer and view in QIIME 2 View (q2view). Visualisation: Environmental significance output","title":"Alpha and beta diversity analysis"},{"location":"tutorials/qiime2/qiime2/#section-5-exporting-data-for-further-analysis-in-r","text":"You need to export your ASV table, taxonomy table, and tree file for analyses in R. Many file formats can be accepted. Export unrooted tree as .nwk format as required for the R package phyloseq . 1 2 3 qiime tools export \\ -- input - path analysis / tree / 16 s_unrooted_tree . qza \\ -- output - path analysis / export Create a BIOM table with taxonomy annotations. A FeatureTable[Frequency] artefact will be exported as a BIOM v2.1.0 formatted file. 1 2 3 qiime tools export \\ -- input - path analysis / taxonomy / 16 s_table_filtered . qza \\ -- output - path analysis / export Then export BIOM to TSV 1 2 3 4 biom convert \\ - i analysis / export / feature - table . biom \\ - o analysis / export / feature - table . tsv \\ -- to - tsv Export Taxonomy as TSV 1 2 3 qiime tools export \\ -- input - path analysis / taxonomy / classification . qza \\ -- output - path analysis / export Delete the header lines of the .tsv files 1 2 sed '1d' analysis/export/taxonomy.tsv > analysis/export/taxonomy_noHeader.tsv sed '1,2d' analysis/export/feature-table.tsv > analysis/export/feature-table_noHeader.tsv Some packages require your data to be in a consistent order, i.e. the order of your ASVs in the taxonomy table rows to be the same order of ASVs in the columns of your ASV table. It\u2019s recommended to clean up your taxonomy file. You can have blank spots where the level of classification was not completely resolved.","title":"Section 5: Exporting data for further analysis in R"},{"location":"tutorials/rna_seq_dge_advanced/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Advanced Tutorial \u00b6 Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview \u00b6 In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial. Background [15 min] \u00b6 Where does the data in this tutorial come from? \u00b6 The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total\u2013 two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8. Section 1: Preparation [15 min] \u00b6 1. Register as a new user in Galaxy if you don\u2019t already have an account \u00b6 Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. \u00b6 If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Yeast RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed. Convert the GTF to a GFF file \u00b6 This is needed for downstream analysis. In the tools panel, search for \u201cGTF\u201d and click on \u201cGTF-to-GFF converter\u201d. Select the GTF file and click \u201cExecute\u201d. Section 2: Alignment [30 mins] \u00b6 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Map/align the reads with Tophat to the S. cerevisiae reference \u00b6 In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is. 2. Rename the output files \u00b6 You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. \u2018Tophat on data 2 and data 1: accepted_hits\u2019 to \u2018batch1-accepted_hits.bam\u2019) by using the pen icon next to the file. 3. Visualise the aligned reads with JBrowse \u00b6 In the tool panel search bar, search for \u201cJBrowse\u201d and click on it. For \u201cReference genome to display\u201d: Use a built-in genome \u201cSelect a reference genome\u201d: Yeast: sacCer2 \u201cGenetic Code\u201d: 1. The Standard Code Set up a track for the mapped reads: Insert Track Group Insert Annotation Track Track Type: Bam Pileups Select two bam files, one from each condition, e.g. batch1_acceptedhits.bam and chem1_acceptedhits.bam Set up a track for the annotations: Insert Track Group Insert Annotation Track Select the GFF file (that we converted from the GTF file earlier on) Execute When the file is ready, click the eye icon. Select chr1 in the drop down box. Tick all the track names in the left hand side. Zoom in and out with the plus and minus buttons. Go to position 87500 to see a splice junction. Section 3. Cuffdiff [40 min] \u00b6 The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes. 1. Run Cuffdiff to identify differentially expressed genes and transcripts \u00b6 In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Note: This step may take a while, depending on how busy the server is. 2. Explore the Cuffdiff output files \u00b6 There should be 11 output files from Cuffdiff. These files should all begin with something like \u201cCuffdiff on data 43, data 38, and others\u201d. We\u2019ll mostly be interested in the file ending with \u2018gene differential expression testing\u2019 which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \u201cFilter on data x\u201d to \u201cCuffdiff_Significant_DE_Genes\u201d. Section 4. Count reads in features [30 min] \u00b6 HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu search bar, search for \u201ccount matrix\u201d. Click on SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes. Section 5: edgeR [30 min] \u00b6 edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models. 1. Generate a list of differentially expressed genes using edgeR \u00b6 In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute 2. Examine the outputs from the previous step \u00b6 Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user\u2019s guide at Bioconductor . 3. Extract the significant differentially expressed genes. \u00b6 Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDifferential_Counts_edgeR_topTable_edgeR.xls\u201d With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \u201cFilter on data x\u201d to \u201cedgeR_Significant_DE_Genes\u201d. Section 6. DESeq2 [30 min] \u00b6 DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity. 1. Generate a list of differentially expressed genes using DESeq2 \u00b6 In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2 2. Examine the outputs the previous step \u00b6 Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Filter out the significant differentially expressed genes. \u00b6 Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDifferential_Counts_DESeq2_topTable_DESeq2.xls\u201d With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \u201cFilter on data x\u201d to \u201cDESeq2_Significant_DE_Genes\u201d. You should see the first few differentially expressed genes are similar to the ones identified by EdgeR. Section 7: How much concordance is there between methods? \u00b6 We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like \u2018Cuffdiff_gene_list\u2019 Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like \u2018edgeR_gene_list\u2019 Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like \u2018DESeq2_gene_list\u2019 Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like \u2018Cuffdiff_edgeR_common_gene_list\u2019 Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like \u2018Cuffdiff_edgeR_DESeq2_common_gene_list\u2019 We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools. Section 8: Gene set enrichment analysis \u00b6 The biological question being asked in the original paper is essentially: \u201cWhat is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\u201d We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As\u2026 Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one. Optional extension: Degust \u00b6 Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ . 1. Load count data into Degust \u00b6 In Galaxy, download the count data \u201cbams to DGE count matrix_htseqsams2mx.xls\u201d generated in Section 4 using the disk icon . Go to http://degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data \u00b6 Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data. 3. Explore your data \u00b6 Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right. 4. Explore the demo data \u00b6 Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. References \u00b6 [1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"RNAseq differential expression tool comparision (Galaxy)"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#rna-seq-differential-gene-expression-advanced-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Advanced Tutorial"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#tutorial-overview","text":"In this tutorial we compare the performance of three statistically-based expression analysis tools: CuffDiff EdgeR DESeq2 This tutorial builds on top of the basic RNA-seq DGE tutorial . It is recommended to have some familiarity of RNA-seq before beginning this tutorial.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#background-15-min","text":"","title":"Background [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from the paper, A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK 113-7D (yeast) under two different metabolic conditions: glucose-excess (batch) or glucose-limited (chemostat). The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with accession SRS307298. There are 6 samples in total\u2013 two treatments with three biological replicates each. The data is paired-end. We have extracted chromosome I reads from the samples to make the tutorial a suitable length. This has implications, as discussed in section 8.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account","text":"Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Yeast RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the batch condition (glucose-excess). Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq These six files are three paired-end samples from the chem condition (glucose-limited). Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Then, upload this file of gene definitions. You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf You should now have these 13 files in your history: batch1_chrI_1.fastq batch1_chrI_2.fastq batch2_chrI_1.fastq batch2_chrI_2.fastq batch3_chrI_1.fastq batch3_chrI_2.fastq chem1_chrI_1.fastq chem1_chrI_2.fastq chem2_chrI_1.fastq chem2_chrI_2.fastq chem3_chrI_1.fastq chem3_chrI_2.fastq genes.gtf These files can be renamed by clicking the pen icon if you wish. Note: The reads are paired end; for example batch1_chrI_1.fastq and batch1_chrI_2.fastq are paired reads from one sequencing run. Low quality reads have already been trimmed.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#convert-the-gtf-to-a-gff-file","text":"This is needed for downstream analysis. In the tools panel, search for \u201cGTF\u201d and click on \u201cGTF-to-GFF converter\u201d. Select the GTF file and click \u201cExecute\u201d.","title":"Convert the GTF to a GFF file"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-2-alignment-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Alignment [30 mins]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-mapalign-the-reads-with-tophat-to-the-s-cerevisiae-reference","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Paired-end (as individual datasets) RNA-Seq FASTQ file, forward reads: (Click on the multiple datasets icon and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) batch1_chrI_1.fastq batch2_chrI_1.fastq batch3_chrI_1.fastq chem1_chrI_1.fastq chem2_chrI_1.fastq chem3_chrI_1.fastq RNA-Seq FASTQ file, reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) batch1_chrI_2.fastq batch2_chrI_2.fastq batch3_chrI_2.fastq chem1_chrI_2.fastq chem2_chrI_2.fastq chem3_chrI_2.fastq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: S. cerevisiae June 2008 (SGD/SacCer2) (sacCer2) Use defaults for the other fields Execute Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Map/align the reads with Tophat to the S. cerevisiae reference"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-rename-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 2 and data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 2 and data 1: splice junctions: This file lists all the places where TopHat had to split a read into two pieces to span an exon junction. Tophat on data 2 and data 1 deletions and Tophat on data 2 and data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 2 and data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. Rename the 6 accepted_hits files into a more meaningful name (e.g. \u2018Tophat on data 2 and data 1: accepted_hits\u2019 to \u2018batch1-accepted_hits.bam\u2019) by using the pen icon next to the file.","title":"2.  Rename the output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-visualise-the-aligned-reads-with-jbrowse","text":"In the tool panel search bar, search for \u201cJBrowse\u201d and click on it. For \u201cReference genome to display\u201d: Use a built-in genome \u201cSelect a reference genome\u201d: Yeast: sacCer2 \u201cGenetic Code\u201d: 1. The Standard Code Set up a track for the mapped reads: Insert Track Group Insert Annotation Track Track Type: Bam Pileups Select two bam files, one from each condition, e.g. batch1_acceptedhits.bam and chem1_acceptedhits.bam Set up a track for the annotations: Insert Track Group Insert Annotation Track Select the GFF file (that we converted from the GTF file earlier on) Execute When the file is ready, click the eye icon. Select chr1 in the drop down box. Tick all the track names in the left hand side. Zoom in and out with the plus and minus buttons. Go to position 87500 to see a splice junction.","title":"3.  Visualise the aligned reads with JBrowse"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-3-cuffdiff-40-min","text":"The aim in this section is to statistically test for differential expression using Cuffdiff and obtain a list of significant genes.","title":"Section 3. Cuffdiff [40 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: genes.gtf Condition: 1: Condition name batch Replicates: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam (Multiple datasets can be selected by holding down the shift key or the ctrl key (Windows) or the command key (OSX).) 2: Condition name chem Replicates: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Note: This step may take a while, depending on how busy the server is.","title":"1.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \u201cCuffdiff on data 43, data 38, and others\u201d. We\u2019ll mostly be interested in the file ending with \u2018gene differential expression testing\u2019 which contains the statistical results from testing the level of gene expression between the batch condition and chem condition. Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. There should be 53 differentially expressed genes in this list. We can rename this file by clicking on the pencil icon of the outputted file and change the name from \u201cFilter on data x\u201d to \u201cCuffdiff_Significant_DE_Genes\u201d.","title":"2.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-4-count-reads-in-features-30-min","text":"HTSeq-count creates a count matrix using the number of the reads from each bam file that map to the genomic features in the genes.gtf. For each feature (a gene for example) a count matrix shows how many reads were mapped to this feature. Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu search bar, search for \u201ccount matrix\u201d. Click on SAM/BAM to count matrix and set the parameters as follows: Gene model (GFF) file to count reads over from your current history: genes.gtf bam/sam file from your history: (Select all six bam files using the shift key.) batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed from looking at the counts. We now have a count matrix, with a count against each corresponding sample. We will use this matrix in later sections to calculate the differentially expressed genes.","title":"Section 4. Count reads in features [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-5-edger-30-min","text":"edgeR is an R package, that is used for analysing differential expression of RNA-Seq data and can either use exact statistical methods or generalised linear models.","title":"Section 5: edgeR  [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-edger","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_edgeR Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Run edgeR Use defaults for the other fields Execute","title":"1.  Generate a list of differentially expressed genes using edgeR"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-from-the-previous-step","text":"Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by clicking on the eye icon . This file is a list of genes sorted by p-value from using EdgeR to perform differential expression analysis. Examine the Differential_Counts_edgeR.html file. This file has some output logs and plots from running edgeR. If you are familiar with R, you can examine the R code used for analysis by scrolling to the bottom of the file, and clicking Differential_Counts.Rscript to download the Rscript file. If you are curious about the statistical methods edgeR uses, you can read the edgeR user\u2019s guide at Bioconductor .","title":"2.  Examine the outputs from the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-extract-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDifferential_Counts_edgeR_topTable_edgeR.xls\u201d With following condition: c6 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 55 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \u201cFilter on data x\u201d to \u201cedgeR_Significant_DE_Genes\u201d.","title":"3.  Extract the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-6-deseq2-30-min","text":"DESeq2 is an R package that uses a negative binomial statistical model to find differentially expressed genes. It can work without replicates (unlike edgeR) but the author strongly advises against this for reasons of statistical validity.","title":"Section 6. DESeq2 [30 min]"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-deseq2","text":"In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count and set the parameters as follows: Select an input matrix - rows are contigs, columns are counts for each sample: bams to DGE count matrix_htseqsams2mx.xls Title for job outputs: Differential_Counts_DESeq2 Treatment Name: Batch Select columns containing treatment: batch1-accepted_hits.bam batch2-accepted_hits.bam batch3-accepted_hits.bam Control Name: Chem Select columns containing control: chem1-accepted_hits.bam chem2-accepted_hits.bam chem3-accepted_hits.bam Run this model using edgeR: Do not run edgeR Run the same model with DESeq2 and compare findings: Run DESeq2","title":"1.  Generate a list of differentially expressed genes using DESeq2"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-the-previous-step","text":"Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file. This file is a list of genes sorted by p-value from using DESeq2 to perform differential expression analysis. Examine the Differential_Counts_DESeq2.html file. This file has some output logs and plots from running DESeq2. Take a look at the PCA plot. More info on PCA plots PCA plots are useful for exploratory data analysis. Samples which are more similar to each other are expected to cluster together. A count matrix often has thousands of dimensions (one for each feature) and our PCA plot generated in the previous step transforms the data so the most variability is represented in principal components 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively). Take note of the scales on the x-axis and the y-axis. The x-axis representing the first principal component accounts for 96% of the variance and ranges from approximately -6 to +6, while the y-axis ranges from approximately -1 to +1. For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition. Additionally, within conditions, the lower glucose (chem) condition shows more variability between replicates than the higher glucose (batch) condition. //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Examine the outputs the previous step"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-filter-out-the-significant-differentially-expressed-genes","text":"Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDifferential_Counts_DESeq2_topTable_DESeq2.xls\u201d With following condition: c7 <= 0.05 Execute This will keep the genes that have an adjusted p-value of less or equal to 0.05. There should be 53 genes in this file. Rename this file by clicking on the pencil icon of and change the name from \u201cFilter on data x\u201d to \u201cDESeq2_Significant_DE_Genes\u201d. You should see the first few differentially expressed genes are similar to the ones identified by EdgeR.","title":"3.  Filter out the significant differentially expressed genes."},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-7-how-much-concordance-is-there-between-methods","text":"We are interested in how similar the identified genes are between the different statistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a Venn diagram to visualise the amount of overlap. Generate a Venn diagram of the output of the 3 differential expression tools. Note that column index 2 (or c3) contains the gene name in the CuffDiff output. Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. In the Galaxy tool panel, under Statistics and Visualisation, select Graph/Display Data > proportional venn and set the parameters as follows: title: Common genes input file 1: Cuffdiff_Significant_DE_Genes column index: 2 as name: Cuffdiff input file 2: edgeR_Significant_DE_Genes column index file 2: 0 as name file 2: edgeR two or three: three input file 3: DESeq2_Significant_DE_Genes column index file 3: 0 as name file 3: DESeq2 Execute View the generated Venn diagram. Agreement between the tools is good: there are 49 differentially expressed genes that all three tools agree upon, and only a handful that are exclusive to each tool. Generate the common list of significantly expressed genes identified by the three mentioned tools by extracting the respective gene list columns and intersecting: Under Basic Tools in the Galaxy tool panel, select Text Manipulation > cut Cut columns: c3 Delimited by: Tab From: Cuffdiff_Significant_DE_Genes Execute Rename the output to something like \u2018Cuffdiff_gene_list\u2019 Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: edgeR_Significant_DE_Genes Execute Rename the output to something like \u2018edgeR_gene_list\u2019 Select Text Manipulation > cut Cut columns: c1 Delimited by: Tab From: DESeq2_Significant_DE_Genes Execute Rename the output to something like \u2018DESeq2_gene_list\u2019 Under Basic Tools in the Galaxy tool panel, select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_gene_list against: edgeR_gene_list Use defaults for the other fields Execute Rename the output to something like \u2018Cuffdiff_edgeR_common_gene_list\u2019 Select Join, Subtract and Group > Compare two Datasets Compare: Cuffdiff_edgeR_common_gene_list against: DESeq2_gene_list Use defaults for the other fields Execute Rename the output to something like \u2018Cuffdiff_edgeR_DESeq2_common_gene_list\u2019 We now have a list of 49 genes that have been identified as significantly differentially expressed by all three tools.","title":"Section 7: How much concordance is there between methods?"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-8-gene-set-enrichment-analysis","text":"The biological question being asked in the original paper is essentially: \u201cWhat is the global response of the yeast transcriptome in the shift from growth at glucose excess conditions (batch) to glucose-limited conditions (chemostat)?\u201d We can address this question by attempting to interpret our differentially expressed gene list at a higher level, perhaps by examining the categories of gene and protein networks that change in response to glucose. For example, we can input our list of differentially expressed genes to a Gene Ontology (GO) enrichment analysis tool such as GOrilla to find out the GO enriched terms. NOTE: Because of time-constraints in this tutorial, the analysis was confined to a single chromosome (chromosome I) and as a consequence we don\u2019t have sufficient information to look for groups of differentially expressed genes (simply because we don\u2019t have enough genes identified from the one chromosome to look for statistically convincing over-representation of any particular gene group). Download the list of genes here in a plain-text file to your local computer by right clicking on the link and selecting Save Link As\u2026 Note that there are ~2500 significantly differentially expressed genes identified in the full analysis. Also note that the genes are ranked in order of statistical significance. This is critical for the next step. Explore the data using gene set enrichment analysis (GSEA) using the online tool GOrilla Go to cbl-gorilla.cs.technion.ac.il Choose Organism: Saccharomyces cerevisiae Choose running mode: Single ranked list of genes Open the gene list you downloaded in the previous step in a text editor. Select the full list, then copy and paste the list into the text box. Choose an Ontology: Process Search Enriched GO terms Once the analysis has finished running, you will be redirected to a page depicting the GO enriched biological processes and its significance (indicated by colour), based on the genes you listed. Scroll down to view a table of GO terms and their significance scores. In the last column, you can toggle the [+] Show genes to see the list of associated genes. Experiment with different ontology categories (Function, Component) in GOrilla. At this stage you are interpreting the experiment in different ways, potentially discovering information that will lead you to further lab experiments. This is driven by your biological knowledge of the problem space. There are an unlimited number of methods for further interpretation of which GSEA is just one.","title":"Section 8: Gene set enrichment analysis"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#optional-extension-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at vicbioinformatics.com/degust/ .","title":"Optional extension: Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-load-count-data-into-degust","text":"In Galaxy, download the count data \u201cbams to DGE count matrix_htseqsams2mx.xls\u201d generated in Section 4 using the disk icon . Go to http://degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts.","title":"1. Load count data into Degust"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-configure-your-uploaded-data","text":"Give your visualisation a name. For the Info column, select Contig. Add two conditions: batch and chem. For each condition, select the three samples which correspond with the condition. Click Save changes and view your data.","title":"2. Configure your uploaded data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-explore-your-data","text":"Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right.","title":"3. Explore your data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#4-explore-the-demo-data","text":"Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"4. Explore the demo data"},{"location":"tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#references","text":"[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10","title":"References"},{"location":"tutorials/rna_seq_dge_basic/","text":"PR reviewers and advice: Jessica Chung, Clare Sloggett Current slides (Jessica\u2019s slides from March 2018): - Overview: https://docs.google.com/presentation/d/1HovpEc5xzB_plxlIpWJAoTpb4309ujfiqOcLKGCDWYw - Workshop: https://docs.google.com/presentation/d/1YmJl8ks7tCg9UOYcjOg1rzr97Qrid5HsS4FqCqxgqq4 Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/","text":"Background \u00b6 Introduction to RNA-seq \u00b6 RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites The Galaxy workflow platform \u00b6 What is Galaxy? \u00b6 Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here Figure 1: The Galaxy interface \u00b6 Tools on the left, data in the middle, analysis workflow on the right. Differential gene expression analysis using Tophat and Cufflinks \u00b6 Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome. The Alternate protocol \u00b6 The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes. Figure 2: General workflow for testing expression differences between two experimental conditions \u00b6 Tophat \u00b6 Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cuffdiff \u00b6 The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences The full Tuxedo Protocol \u00b6 Figure 3: Full Tuxedo protocol workflow \u00b6 Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots. Protocols recommendations \u00b6 Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult. Limitations of the protocols \u00b6 Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques. References \u00b6 Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"Introduction to bulk RNAseq analysis"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#introduction-to-rna-seq","text":"RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample. Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples. In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites","title":"Introduction to RNA-seq"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-galaxy-workflow-platform","text":"","title":"The Galaxy workflow platform"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#what-is-galaxy","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here","title":"What is Galaxy?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-1-the-galaxy-interface","text":"Tools on the left, data in the middle, analysis workflow on the right.","title":"Figure 1: The Galaxy interface"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#differential-gene-expression-analysis-using-tophat-and-cufflinks","text":"Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012): The Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes The Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed Assembling a transcriptome is advised if no well characterised transcriptome exists, but as D. melanogaster is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler. If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.","title":"Differential gene expression analysis using Tophat and Cufflinks"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-alternate-protocol","text":"The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.","title":"The Alternate protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-2-general-workflow-for-testing-expression-differences-between-two-experimental-conditions","text":"","title":"Figure 2: General workflow for testing expression differences between two experimental conditions"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#tophat","text":"Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file)","title":"Tophat"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#cuffdiff","text":"The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences","title":"Cuffdiff"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-full-tuxedo-protocol","text":"","title":"The full Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-3-full-tuxedo-protocol-workflow","text":"Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments. TopHat input: Fasta or Fastq files TopHat output: BAM file (Compressed SAM file) Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones Cufflinks input: Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat Genome annotation: GTF file Cufflinks output: assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular) transcript_expression (tabular): table of expression levels for each transcript gene_expression (tabular): table of total expression levels for each gene. Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy. Cuffcompare input: Assembled_transcripts for each sample Reference Annotation Cuffcompare output: combined_transcripts.gtf Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes. Cuffdiff input: Reference transcriptome as GTF file BAM files of mapped reads from Tophat for all samples Cuffdiff output: Gene and transcript expression levels as tables of normalised read counts Differential analysis testing on: Genes Transcripts Transcription Start Site (TSS) groups Splicing: files reporting on splicing Promoter: differentially spliced genes via promoter switching CDS: CoDing Sequences CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.","title":"Figure 3: Full Tuxedo protocol workflow"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#protocols-recommendations","text":"Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended. Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads. Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads. Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.","title":"Protocols recommendations"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#limitations-of-the-protocols","text":"Both Tophat and Cufflinks require a reference genome. The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.","title":"Limitations of the protocols"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_background/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578. James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov. Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) , [^1]: The published protocol has been designed for running on the command line in Linux. This tutorial has been adapted to use on the web-based Galaxy platform. [^2]: Note: Cuffmerge is a meta-assembler. It treats the assemblies created by Cufflinks the same way Cufflinks treats reads from Tophat. That is, it produces a parsimonious transcript assembly of the assemblies. The difference between Cuffmerge and Cuffcompare is that if we have two transfrags A and B, Cuffcompare only combines the two where either A or B is \u2018contained\u2019 in the other transfrag or in other words one of them is redundant; whereas Cuffmerge assembles them if they overlap with each other and agree on splicing. [^3]: Since CummRbund is currently not installed on Galaxy, the underlying steps are not included in this tutorial; instead we use IGV (Robinson et al 2011) and Trackster for visualizing the output which are accessible from Galaxy.","title":"References"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 2px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.extra { color: #444444; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq - Differential Gene Expression \u00b6 Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie Tutorial Overview \u00b6 In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. Here, we\u2019ll be using a subset of the data from a published experiment by Hateley et. al. in 2016. In practice, full-sized datasets would be much larger and take longer to run. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner, HISAT2 visualise RNA-seq alignment data with IGV or JBrowse use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data Learning Objectives \u00b6 At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Requirements \u00b6 Participants with no previous Galaxy experience are strongly recommended to attend the \u201cIntroduction to Galaxy\u201d workshop first. Attendees are required to bring their own laptop computers. The data \u00b6 The sequencing data you will be working with is from Drosophila melanogaster pupae from the study, Transcriptomic response of Drosophila melanogaster pupae developed in hypergravity . The experiment has two conditions, g3 where pupae underwent development in three times Earth\u2019s gravity (i.e. 3 g ), and g1 , the control, where pupae developed in the standard gravitational acceleration felt on the surface of Earth (i.e. 1 g ). There are three samples in each condition and the sequencing data is paired-end so you will have two files for each of the six samples. Your aim will be to find differentially expressed genes in g1 vs g3 . Section 1: Preparation \u00b6 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Data for RNA-Seq tutorial - Hypergravity . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the g1 group. Keep the type as \u201cAuto-detect\u201d when uploading these files. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R2.fastq.gz These six files are three paired-end samples from the g3 group. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R2.fastq.gz Then, upload this file of gene definitions. Keep the type as \u201cAuto-detect\u201d when this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the 3 samples that developed in 1 g : g1_01_R1.fastq.gz g1_01_R2.fastq.gz g1_02_R1.fastq.gz g1_02_R2.fastq.gz g1_03_R1.fastq.gz g1_03_R2.fastq.gz 6 files containing paired-ended reads for the 3 samples that developed in 3 g : g3_01_R1.fastq.gz g3_01_R2.fastq.gz g3_02_R1.fastq.gz g3_02_R2.fastq.gz g3_03_R1.fastq.gz g3_03_R2.fastq.gz And 1 gene annotation file for chromosome 4 of the Drosophila genome: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: \u201c.fastq.gz\u201d. If you are not familiar with the FASTQ format, click here for an overview . The \u201c.gz\u201d extension indicates these files have been compressed by gzip . FASTQ files are typically stored as compressed files to save disk space as they are usually gigabytes in size. Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: The reads are paired-end, i.e. g1_01_R1.fastq.gz and g1_01_R2.fastq.gz are paired reads from one sequencing run. If you\u2019re unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here . Section 2: Alignment with HISAT2 \u00b6 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT2 to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *_R1.fastq.gz. This should be correspond to every second dataset (e.g. 1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) g1_01_R1.fastq.gz g1_02_R1.fastq.gz g1_03_R1.fastq.gz g3_01_R1.fastq.gz g3_02_R1.fastq.gz g3_03_R1.fastq.gz Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *_R2.fastq.gz.) g1_01_R2.fastq.gz g1_02_R2.fastq.gz g1_03_R2.fastq.gz g3_01_R2.fastq.gz g3_02_R2.fastq.gz g3_03_R2.fastq.gz Use defaults for the other fields Execute Your tool interface panel will look similar to this: Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. \u2018HISAT on data 2 and data 1\u2019 to \u2018g1_01.bam\u2019) by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view when you click on the eye icon. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \u201cinfo\u201d button (view details) of a dataset, say g1_01.bam, and find the \u201cTool Standard Error\u201d row under \u201cJob Information\u201d in the table. Click the \u201cstderr\u201d link to view the alignment summary output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 50000 reads ; of these : 50000 ( 100 . 00 % ) were paired ; of these : 321 ( 0 . 64 % ) aligned concordantly 0 times 45766 ( 91 . 53 % ) aligned concordantly exactly 1 time 3913 ( 7 . 83 % ) aligned concordantly > 1 times ---- 321 pairs aligned concordantly 0 times ; of these : 0 ( 0 . 00 % ) aligned discordantly 1 time ---- 321 pairs aligned 0 times concordantly or discordantly ; of these : 642 mates make up the pairs ; of these : 529 ( 82 . 40 % ) aligned 0 times 77 ( 11 . 99 % ) aligned exactly 1 time 36 ( 5 . 61 % ) aligned > 1 times 99 . 47 % overall alignment rate Here we see we have a very high alignment rate, which is expected since the reads in this dataset have been pre-selected to align to chromosome 4. Section 3: Visualise the aligned reads \u00b6 The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse and IGV. JBrowse and IGV are both interactive tools that can visualise BAM files. You can pick either one to use in this section. JBrowse is run on Galaxy which means you can view your BAM file in your browser, but it takes a while to run the job (~30 mins). IGV is a separate application you\u2019ll need to download to your computer and run locally. Viewing in JBrowse \u00b6 Before using JBrowse, you\u2019ll need to convert your GTF file to a GFF file. In the left tool panel menu, select the \u201cGTF-to-GFF converter\u201d tool, then provide your GTF file and click \u201cexecute\u201d. GTF and GFF are similar representations of the same information, but JBrowse requires the annotation information to be in GFF format. To visualise the alignment data: search for \u201cJBrowse\u201d in the tool panel search bar for \u201cReference genome to display\u201d: Use a built-in genome for \u201cSelect a reference genome\u201d: Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: BAM pileups for \u201cBAM Track Data\u201d: select the multiple datasets icon, then select a bam from each condition, e.g., g1_01.bam and g3_01.bam (your files may be named differently) for \u201cAutogenerate SNP Track\u201d: Yes Set up a track for the annotated genome: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: GFF/GFF3/BED/GBK Features for \u201cGFF/GFF3/BED Track Data\u201d: select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \u201cAvailable Tracks\u201d on the left are ticked. Zoom in and out with the plus and minus buttons. Select \u201cChr4\u201d. Viewing in IGV \u00b6 An alternative to JBrowse is IGV. If you don\u2019t already have IGV installed on your computer, download and install it now. You will also need Java installed to run IGV. To visualise the alignment data: Open the IGV application (this may take a few seconds). Once opened, use the top left drop-down menu bar to select the correct Drosophila genome (dm3). You may need to select the \u201cMore\u2026\u201d option, and select \u201cD. melanogaster (dm3)\u201d. In Galaxy, click on one of the BAM files, for example \u2018g1_01.bam\u2019, to expand the available options. Click on \u201cdisplay with IGV local \u201d and the BAM file should be loaded into IGV. Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) Open another BAM file from the other condition (e.g. \u2018g3_01.bam\u2019) by clicking on the dataset in Galaxy and clicking on \u201cdisplay with IGV local \u201d. Section 4. Quantification \u00b6 HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select \u2018Multiple datasets\u2019, then select all six bam files using the shift key.) g1_01.bam g1_02.bam g1_03.bam g3_01.bam g3_02.bam g3_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \u201c(no feature)\u201d) contains the stats for the reads that weren\u2019t able to be uniquely aligned to a gene. We don\u2019t need the \u201c(no feature)\u201d files so we can remove then with the delete \u201cX\u201d button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as g1_01, g1_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) g1_01 g1_02 g1_03 g3_01 g3_02 g3_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between g1 samples and g3 samples. Section 5. Degust \u00b6 Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \u201cgene_id\u201d. Add two conditions: g1 and g3. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 0.5 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. 5. Explore the full dataset The FASTQ files we started with is only a small proportion of the full dataset. If you wish, you can download the full count matrix here , upload it to Degust, and explore the results. Section 5. DESeq2 \u00b6 In this section we\u2019ll use the \u201cDESeq2\u201d tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: gravity 1: Factor level: Specify a factor level: g1 (Select the three g1 htseq-count files.) g1_01 g1_02 g1_03 2: Factor level: Specify a factor level: g3 (Select the three g3 htseq-count files.) g3_01 g3_02 g3_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDESeq2 result file on data \u2026\u201d With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there? Section 6. The importance of replicates \u00b6 Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene\u2019s counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. CG1674 is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like CG1674 was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifying a gene as differentially expressed when it is, in reality, not. Optional extension \u00b6 Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called \u2018batch\u2019 and \u2018chem\u2019, and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"RNAseq analysis using HISAT2 (Galaxy)"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#rna-seq-differential-gene-expression","text":"Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie","title":"RNA-Seq - Differential Gene Expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. Here, we\u2019ll be using a subset of the data from a published experiment by Hateley et. al. in 2016. In practice, full-sized datasets would be much larger and take longer to run. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner, HISAT2 visualise RNA-seq alignment data with IGV or JBrowse use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#learning-objectives","text":"At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#requirements","text":"Participants with no previous Galaxy experience are strongly recommended to attend the \u201cIntroduction to Galaxy\u201d workshop first. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#the-data","text":"The sequencing data you will be working with is from Drosophila melanogaster pupae from the study, Transcriptomic response of Drosophila melanogaster pupae developed in hypergravity . The experiment has two conditions, g3 where pupae underwent development in three times Earth\u2019s gravity (i.e. 3 g ), and g1 , the control, where pupae developed in the standard gravitational acceleration felt on the surface of Earth (i.e. 1 g ). There are three samples in each condition and the sequencing data is paired-end so you will have two files for each of the six samples. Your aim will be to find differentially expressed genes in g1 vs g3 .","title":"The data"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-1-preparation","text":"1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Data for RNA-Seq tutorial - Hypergravity . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the g1 group. Keep the type as \u201cAuto-detect\u201d when uploading these files. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g1_03_R2.fastq.gz These six files are three paired-end samples from the g3 group. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_01_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_02_R2.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R1.fastq.gz https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/g3_03_R2.fastq.gz Then, upload this file of gene definitions. Keep the type as \u201cAuto-detect\u201d when this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_hypergravity/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the 3 samples that developed in 1 g : g1_01_R1.fastq.gz g1_01_R2.fastq.gz g1_02_R1.fastq.gz g1_02_R2.fastq.gz g1_03_R1.fastq.gz g1_03_R2.fastq.gz 6 files containing paired-ended reads for the 3 samples that developed in 3 g : g3_01_R1.fastq.gz g3_01_R2.fastq.gz g3_02_R1.fastq.gz g3_02_R2.fastq.gz g3_03_R1.fastq.gz g3_03_R2.fastq.gz And 1 gene annotation file for chromosome 4 of the Drosophila genome: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: \u201c.fastq.gz\u201d. If you are not familiar with the FASTQ format, click here for an overview . The \u201c.gz\u201d extension indicates these files have been compressed by gzip . FASTQ files are typically stored as compressed files to save disk space as they are usually gigabytes in size. Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: The reads are paired-end, i.e. g1_01_R1.fastq.gz and g1_01_R2.fastq.gz are paired reads from one sequencing run. If you\u2019re unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here .","title":"Section 1: Preparation"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-2-alignment-with-hisat2","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT2 to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *_R1.fastq.gz. This should be correspond to every second dataset (e.g. 1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) g1_01_R1.fastq.gz g1_02_R1.fastq.gz g1_03_R1.fastq.gz g3_01_R1.fastq.gz g3_02_R1.fastq.gz g3_03_R1.fastq.gz Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *_R2.fastq.gz.) g1_01_R2.fastq.gz g1_02_R2.fastq.gz g1_03_R2.fastq.gz g3_01_R2.fastq.gz g3_02_R2.fastq.gz g3_03_R2.fastq.gz Use defaults for the other fields Execute Your tool interface panel will look similar to this: Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. \u2018HISAT on data 2 and data 1\u2019 to \u2018g1_01.bam\u2019) by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view when you click on the eye icon. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \u201cinfo\u201d button (view details) of a dataset, say g1_01.bam, and find the \u201cTool Standard Error\u201d row under \u201cJob Information\u201d in the table. Click the \u201cstderr\u201d link to view the alignment summary output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 50000 reads ; of these : 50000 ( 100 . 00 % ) were paired ; of these : 321 ( 0 . 64 % ) aligned concordantly 0 times 45766 ( 91 . 53 % ) aligned concordantly exactly 1 time 3913 ( 7 . 83 % ) aligned concordantly > 1 times ---- 321 pairs aligned concordantly 0 times ; of these : 0 ( 0 . 00 % ) aligned discordantly 1 time ---- 321 pairs aligned 0 times concordantly or discordantly ; of these : 642 mates make up the pairs ; of these : 529 ( 82 . 40 % ) aligned 0 times 77 ( 11 . 99 % ) aligned exactly 1 time 36 ( 5 . 61 % ) aligned > 1 times 99 . 47 % overall alignment rate Here we see we have a very high alignment rate, which is expected since the reads in this dataset have been pre-selected to align to chromosome 4.","title":"Section 2: Alignment with HISAT2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-3-visualise-the-aligned-reads","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse and IGV. JBrowse and IGV are both interactive tools that can visualise BAM files. You can pick either one to use in this section. JBrowse is run on Galaxy which means you can view your BAM file in your browser, but it takes a while to run the job (~30 mins). IGV is a separate application you\u2019ll need to download to your computer and run locally.","title":"Section 3: Visualise the aligned reads"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#viewing-in-jbrowse","text":"Before using JBrowse, you\u2019ll need to convert your GTF file to a GFF file. In the left tool panel menu, select the \u201cGTF-to-GFF converter\u201d tool, then provide your GTF file and click \u201cexecute\u201d. GTF and GFF are similar representations of the same information, but JBrowse requires the annotation information to be in GFF format. To visualise the alignment data: search for \u201cJBrowse\u201d in the tool panel search bar for \u201cReference genome to display\u201d: Use a built-in genome for \u201cSelect a reference genome\u201d: Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: BAM pileups for \u201cBAM Track Data\u201d: select the multiple datasets icon, then select a bam from each condition, e.g., g1_01.bam and g3_01.bam (your files may be named differently) for \u201cAutogenerate SNP Track\u201d: Yes Set up a track for the annotated genome: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: GFF/GFF3/BED/GBK Features for \u201cGFF/GFF3/BED Track Data\u201d: select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \u201cAvailable Tracks\u201d on the left are ticked. Zoom in and out with the plus and minus buttons. Select \u201cChr4\u201d.","title":"Viewing in JBrowse"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#viewing-in-igv","text":"An alternative to JBrowse is IGV. If you don\u2019t already have IGV installed on your computer, download and install it now. You will also need Java installed to run IGV. To visualise the alignment data: Open the IGV application (this may take a few seconds). Once opened, use the top left drop-down menu bar to select the correct Drosophila genome (dm3). You may need to select the \u201cMore\u2026\u201d option, and select \u201cD. melanogaster (dm3)\u201d. In Galaxy, click on one of the BAM files, for example \u2018g1_01.bam\u2019, to expand the available options. Click on \u201cdisplay with IGV local \u201d and the BAM file should be loaded into IGV. Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) Open another BAM file from the other condition (e.g. \u2018g3_01.bam\u2019) by clicking on the dataset in Galaxy and clicking on \u201cdisplay with IGV local \u201d.","title":"Viewing in IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-4-quantification","text":"HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select \u2018Multiple datasets\u2019, then select all six bam files using the shift key.) g1_01.bam g1_02.bam g1_03.bam g3_01.bam g3_02.bam g3_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \u201c(no feature)\u201d) contains the stats for the reads that weren\u2019t able to be uniquely aligned to a gene. We don\u2019t need the \u201c(no feature)\u201d files so we can remove then with the delete \u201cX\u201d button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as g1_01, g1_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) g1_01 g1_02 g1_03 g3_01 g3_02 g3_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between g1 samples and g3 samples.","title":"Section 4. Quantification"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \u201cgene_id\u201d. Add two conditions: g1 and g3. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 0.5 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. 5. Explore the full dataset The FASTQ files we started with is only a small proportion of the full dataset. If you wish, you can download the full count matrix here , upload it to Degust, and explore the results.","title":"Section 5. Degust"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-deseq2","text":"In this section we\u2019ll use the \u201cDESeq2\u201d tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: gravity 1: Factor level: Specify a factor level: g1 (Select the three g1 htseq-count files.) g1_01 g1_02 g1_03 2: Factor level: Specify a factor level: g3 (Select the three g3 htseq-count files.) g3_01 g3_02 g3_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDESeq2 result file on data \u2026\u201d With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there?","title":"Section 5. DESeq2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-6-the-importance-of-replicates","text":"Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene\u2019s counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. CG1674 is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like CG1674 was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifying a gene as differentially expressed when it is, in reality, not.","title":"Section 6. The importance of replicates"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#optional-extension","text":"Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called \u2018batch\u2019 and \u2018chem\u2019, and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Optional extension"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 2px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.extra { color: #444444; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq - Differential Gene Expression \u00b6 Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie Tutorial Overview \u00b6 In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives \u00b6 At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Requirements \u00b6 Participants with no previous Galaxy experience are strongly recommended to attend the \u201cIntroduction to Galaxy\u201d workshop first. Attendees are required to bring their own laptop computers. The data \u00b6 The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditions, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO. Section 1: Preparation \u00b6 1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Fly RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you\u2019re unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here . Convert the GTF to a GFF file \u00b6 This is needed for downstream analysis. In the tools panel, search for \u201cGTF\u201d and click on \u201cGTF-to-GFF converter\u201d. Select the GTF file and click \u201cExecute\u201d. Section 2: Alignment with HISAT2 \u00b6 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Use defaults for the other fields Execute Your tool interface panel will look similar to this (although the options may be in a different order): Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. \u2018HISAT on data 2 and data 1\u2019 to \u2018WT_01.bam\u2019) by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view, when you click on the eye icon. In section 3, we\u2019ll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \u201cinfo\u201d button (view details) of a dataset, say WT_01.bam, and find the \u201cTool Standard Error\u201d row under \u201cJob Information\u201d in the table. Click the \u201cstderr\u201d link to view the alignment summary output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16046 reads ; of these : 16046 ( 100 . 00 % ) were paired ; of these : 104 ( 0 . 65 % ) aligned concordantly 0 times 13558 ( 84 . 49 % ) aligned concordantly exactly 1 time 2384 ( 14 . 86 % ) aligned concordantly > 1 times ---- 104 pairs aligned concordantly 0 times ; of these : 1 ( 0 . 96 % ) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly ; of these : 206 mates make up the pairs ; of these : 106 ( 51 . 46 % ) aligned 0 times 91 ( 44 . 17 % ) aligned exactly 1 time 9 ( 4 . 37 % ) aligned > 1 times 99 . 67 % overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination. Section 3: Visualise the aligned reads \u00b6 The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse. Viewing in JBrowse \u00b6 To visualise the alignment data: search for \u201cJBrowse\u201d in the tool panel search bar for \u201cReference genome to display\u201d: Use a built-in genome for \u201cSelect a reference genome\u201d: Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: BAM pileups for \u201cBAM Track Data\u201d: select the multiple datasets icon, then select a bam from each condition, e.g., WT_01.bam and KO_01.bam (your files may be named differently) for \u201cAutogenerate SNP Track\u201d: Yes Set up a track for the annotated genome: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: GFF/GFF3/BED/GBK Features for \u201cGFF/GFF3/BED Track Data\u201d: select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \u201cAvailable Tracks\u201d on the left are ticked. Zoom in and out with the plus and minus buttons. Select \u201cChr4\u201d. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). If you can\u2019t find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The \u2018Sox102F\u2019 gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. Viewing in IGV \u00b6 To visualise the alignment data: Click on one of the BAM files, for example \u2018WT_01.bam\u2019. Click on Display with IGV \u2018webcurrent\u2019 (or \u2018local\u2019 if you have IGV installed on your computer. You will need to open IGV before you click on \u2018local\u2019). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select \u2018KO_02.bam\u2019 and click on \u2018display with IGV local\u2019. This time we are using the \u2018local\u2019 link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can\u2019t find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The \u2018Sox102F\u2019 gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. Section 4. Quantification \u00b6 HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select \u2018Multiple datasets\u2019, then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \u201c(no feature)\u201d) contains the stats for the reads that weren\u2019t able to be uniquely aligned to a gene. We don\u2019t need the \u201c(no feature)\u201d files so we can remove then with the delete \u201cX\u201d button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples. Section 5. Degust \u00b6 Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \u201cgene_id\u201d. Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right. Section 5. DESeq2 \u00b6 In this section we\u2019ll use the \u201cDESeq2\u201d tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDESeq2 result file on data \u2026\u201d With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there? Section 6. The importance of replicates \u00b6 Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene\u2019s counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. Optional extension \u00b6 Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called \u2018batch\u2019 and \u2018chem\u2019, and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Rna seq basic tutorial 2018"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#rna-seq-differential-gene-expression","text":"Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie","title":"RNA-Seq - Differential Gene Expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a simulated dataset from the common fruit fly, Drosophila melanogaster . The tutorial is designed to introduce the tools, datatypes and workflows of an RNA-seq DGE analysis. In practice, real datasets would be much larger and contain sequencing and alignment errors that make analysis more difficult. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with an aligner (HISAT2) visualise RNA-seq alignment data with IGV use a number of different methods to find differentially expressed genes understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that we might do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#learning-objectives","text":"At the end of this tutorial you will be able to: understand the basic workflow of alignment, quantification, and testing, for RNA-seq differential expression analysis process raw RNA sequence data into a list of differentially expressed genes understand the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#requirements","text":"Participants with no previous Galaxy experience are strongly recommended to attend the \u201cIntroduction to Galaxy\u201d workshop first. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#the-data","text":"The sequencing data you will be working with is simulated from Drosophila melanogaster. The experiment has two conditions, WT (wildtype) and KO (knockout), and three samples in each condition. The sequencing data is paired-end, so there are two files for each of the six samples. Your aim will be to find differentially expressed genes in WT vs KO.","title":"The data"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-1-preparation","text":"1. Register as a new user in Galaxy if you don\u2019t already have an account Open a browser and go to a Galaxy server. For example, you could use Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. If you are using Galaxy Australia, go to Shared Data > Data Libraries in the top toolbar, and select Galaxy Australia Training Material: RNA-Seq: Fly RNA-Seq . Select (tick) all of the files and click To History , and choose as Datasets , then Import . Alternatively, if you are using your own personal Galaxy server or a different Galaxy server, you can import the data like this: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area. These six files are three paired-end samples from the WT flies. Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq These six files are three paired-end samples from the KO flies. Make sure the type is specified as \u2018fastqsanger\u2019 when uploading. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq Then, upload this file of gene definitions. You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf You should now have 13 files in your history. Note: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 3. View and have an understanding of the files involved in RNA-seq analysis. You should now have the following files in your Galaxy history: 6 files containing paired-ended reads for the WT samples: WT_01_R1.fastq WT_01_R2.fastq WT_02_R1.fastq WT_02_R2.fastq WT_03_R1.fastq WT_03_R2.fastq 6 files containing paired-ended reads for the KO samples: KO_01_R1.fastq KO_01_R2.fastq KO_02_R1.fastq KO_02_R2.fastq KO_03_R1.fastq KO_03_R2.fastq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 12 sequencing files are in FASTQ format and have the file extension: .fastq. If you are not familiar with the FASTQ format, click here for an overview . Each condition has three samples, and each sample has two files (an R1 file containing forward reads and an R2 file containing reverse reads). Click on the eye icon to the top right of any FASTQ file to view the first part of the file. Note: Since the reads in this dataset are synthetic, they do not have real quality scores. Note: The reads are paired-end, i.e. WT_01_R1.fastq and WT_01_R2.fastq are paired reads from one sequencing run. If you\u2019re unfamiliar with paired-end sequencing, you can read about it here . The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file describes where the genes are located in the D. melanogaster reference genome, filtered for genes on chromosome 4. Each feature is defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. We will examine this file more closely later in Section 3 of this tutorial. More information on the GTF format can be found here .","title":"Section 1: Preparation"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#convert-the-gtf-to-a-gff-file","text":"This is needed for downstream analysis. In the tools panel, search for \u201cGTF\u201d and click on \u201cGTF-to-GFF converter\u201d. Select the GTF file and click \u201cExecute\u201d.","title":"Convert the GTF to a GFF file"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-2-alignment-with-hisat2","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. We will use HISAT to perform our alignment. HISAT2 is a fast, splice-aware, alignment program that is a successor to TopHat2. More information on HISAT2 can be found here . 1. Align the RNA-seq reads to a reference genome. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2 and set the parameters as follows: Source for the reference genome to align against: Use built-in genome Select a reference genome: Fruit Fly (Drosophila melanogaster): dm3 Single end or paired reads? Paired end Forward reads: (Click on the multiple datasets icon (which looks like two pieces of paper in a stack) and select all six of the forward FASTQ files ending in *1.fastq. This should be correspond to every second file (1,3,5,7,9,11). This can be done by holding down the ctrl key (Windows) or the command key (OSX) to select multiple files.) WT_01_R1.fastq WT_02_R1.fastq WT_03_R1.fastq KO_01_R1.fastq KO_02_R1.fastq KO_03_R1.fastq Reverse reads: (Click on the multiple datasets icon and select all six of the reverse FASTQ files ending in *2.fastq.) WT_01_R2.fastq WT_02_R2.fastq WT_03_R2.fastq KO_01_R2.fastq KO_02_R2.fastq KO_03_R2.fastq Use defaults for the other fields Execute Your tool interface panel will look similar to this (although the options may be in a different order): Note: This may take a few minutes, depending on how busy the server is. 2. Examine the alignment stats HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6 files into a more meaningful name (e.g. \u2018HISAT on data 2 and data 1\u2019 to \u2018WT_01.bam\u2019) by using the pen icon next to the file. These files are BAM files (short for Binary Alignment Map ) and like the name suggests, is a binary file. Galaxy automatically converts these to a plain-text equivalent (SAM) file to view, when you click on the eye icon. In section 3, we\u2019ll use a genome viewer to view our alignments. HISAT2 also outputs some information to stderr which we can preview by clicking on the dataset name. To view the raw file, click the \u201cinfo\u201d button (view details) of a dataset, say WT_01.bam, and find the \u201cTool Standard Error\u201d row under \u201cJob Information\u201d in the table. Click the \u201cstderr\u201d link to view the alignment summary output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16046 reads ; of these : 16046 ( 100 . 00 % ) were paired ; of these : 104 ( 0 . 65 % ) aligned concordantly 0 times 13558 ( 84 . 49 % ) aligned concordantly exactly 1 time 2384 ( 14 . 86 % ) aligned concordantly > 1 times ---- 104 pairs aligned concordantly 0 times ; of these : 1 ( 0 . 96 % ) aligned discordantly 1 time ---- 103 pairs aligned 0 times concordantly or discordantly ; of these : 206 mates make up the pairs ; of these : 106 ( 51 . 46 % ) aligned 0 times 91 ( 44 . 17 % ) aligned exactly 1 time 9 ( 4 . 37 % ) aligned > 1 times 99 . 67 % overall alignment rate Here we see we have a very high alignment rate, which is expected since the data we have is simulated and has no contamination.","title":"Section 2: Alignment with HISAT2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-3-visualise-the-aligned-reads","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with interactive visualisation tools such as JBrowse.","title":"Section 3: Visualise the aligned reads"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#viewing-in-jbrowse","text":"To visualise the alignment data: search for \u201cJBrowse\u201d in the tool panel search bar for \u201cReference genome to display\u201d: Use a built-in genome for \u201cSelect a reference genome\u201d: Fruit Fly dm3 leave other settings as default, except: Set up a track for mapped RNA-seq reads: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: BAM pileups for \u201cBAM Track Data\u201d: select the multiple datasets icon, then select a bam from each condition, e.g., WT_01.bam and KO_01.bam (your files may be named differently) for \u201cAutogenerate SNP Track\u201d: Yes Set up a track for the annotated genome: click \u201cInsert Track Group\u201d and then \u201cInsert Annotation Track\u201d for \u201cTrack Type\u201d: GFF/GFF3/BED/GBK Features for \u201cGFF/GFF3/BED Track Data\u201d: select the reference genome in GFF format, e.g. the converted GTF file (ensembl_dm3.chr4.gtf) - your files may be named differently Execute JBrowse will create a single file with this visualization (this may take a while). When it is ready, click the eye icon to display in the centre panel. Make sure all \u201cAvailable Tracks\u201d on the left are ticked. Zoom in and out with the plus and minus buttons. Select \u201cChr4\u201d. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). If you can\u2019t find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The \u2018Sox102F\u2019 gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"Viewing in JBrowse"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#viewing-in-igv","text":"To visualise the alignment data: Click on one of the BAM files, for example \u2018WT_01.bam\u2019. Click on Display with IGV \u2018webcurrent\u2019 (or \u2018local\u2019 if you have IGV installed on your computer. You will need to open IGV before you click on \u2018local\u2019). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (WT) and condition 2 (KO). Select \u2018KO_02.bam\u2019 and click on \u2018display with IGV local\u2019. This time we are using the \u2018local\u2019 link, as we already have an IGV window up and running locally from the last step. Once the file has loaded, try to find some genes that look differentially expressed. If you can\u2019t find any, try changing the location to chr4:816349-830862 using the field on the top toolbar. The \u2018Sox102F\u2019 gene in this area looks like it has many more reads mapped in WT than in KO. Hover over the coverage track to view the read depth of the area. But, of course, it may be that there are many more reads in the library for WT than KO. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"Viewing in IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-4-quantification","text":"HTSeq-count counts the number of the reads from each bam file that map to the genomic features in the provided annotation file. For each feature (a gene for example) we will obtain a numerical value associated with the expression of that feature in our sample (i.e. the number of reads that were aligned to that gene). 1. Examine the GTF file Click on the eye icon to display the ensembl_dm3.chr4.gtf file in Galaxy. This GTF file is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. 2. Run HTSeq-count Use HTSeq-count to count the number of reads for each feature. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > htseq-count and set the parameters as follows: Aligned SAM/BAM File: (Select \u2018Multiple datasets\u2019, then select all six bam files using the shift key.) WT_01.bam WT_02.bam WT_03.bam KO_01.bam KO_02.bam KO_03.bam GFF File: ensembl_dm3.chr4.gtf Stranded: No ID Attribute: gene_name Use defaults for the other fields Execute In the previous step, each input BAM file outputted two files. The first file contains the counts for each of our genes. The second file (ending with \u201c(no feature)\u201d) contains the stats for the reads that weren\u2019t able to be uniquely aligned to a gene. We don\u2019t need the \u201c(no feature)\u201d files so we can remove then with the delete \u201cX\u201d button on the top right. Rename the remaining six files from htseq-count to meaningful names, such as WT_01, WT_02, etc. 3. Generate a count matrix Generate a combined count matrix by combining our six files. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Generate count matrix and set the parameters as follows: Count files from your history: (Select all six count files using the shift key.) WT_01 WT_02 WT_03 KO_01 KO_02 KO_03 Use defaults for the other fields Execute Examine the outputted matrix by using the eye icon . Each column corresponds to a sample and each row corresponds to a gene. By sight, see if you can find a gene you think is differentially expressed just by looking at the counts. We now have a count matrix which we will now use to find differentially expressed genes between WT samples and KO samples.","title":"Section 4. Quantification"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-5-degust","text":"Degust is an interactive visualiser for analysing RNA-seq data. It runs as a web service and can be found at degust.erc.monash.edu/ . 1. Load count data into Degust In Galaxy, download the count matrix you generated in the last section using the disk icon . Go to degust.erc.monash.edu/ and click on \u201cUpload your counts file\u201d. Click \u201cChoose file\u201d and upload the recently downloaded Galaxy tabular file containing your RNA-seq counts. 2. Configure your uploaded data Give your visualisation a name. For the Info column, select \u201cgene_id\u201d. Add two conditions: WT and KO. For each condition, select the three samples which correspond with the condition. Set min gene CPM to 1 in at least 3 samples. Click Save changes and view your data. Read through the Degust tour of features. Explore the parallel coordinates plot, MA plot, MDS plot, heatmap and gene list. Each is fully interactive and influences other portions on the display depending on what is selected. On the right side of the page is an options module which can set thresholds to filter genes using statistical significance or absolute-fold-change. On the left side is a dropdown box you can specify the method (Voom/Limma or edgeR) used to perform differential expression analysis on the data. You can also view the R code by clicking \u201cShow R code\u201d under the options module on the right. 4. Explore the demo data Degust also provides an example dataset with 4 conditions and more genes. You can play with the demo dataset by clicking on the \u201cTry the demo\u201d button on the Degust homepage. The demo dataset includes a column with an EC number for each gene. This means genes can be displayed on Kegg pathways using the module on the right.","title":"Section 5. Degust"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-5-deseq2","text":"In this section we\u2019ll use the \u201cDESeq2\u201d tool in Galaxy to do our differential gene analysis. This tool uses the separate HTSeq files we generated in section 4. Similar to Voom/Limma or edgeR that was used in Degust to statistically test our data, DESeq2 will: statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. Use DESeq2 to find differentially expressed features from the count data. In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > DESeq2 and set the parameters as follows: 1: Factor Specify a factor name: condition 1: Factor level: Specify a factor level: WT (Select the three WT htseq-count files.) WT_01 WT_02 WT_03 2: Factor level: Specify a factor level: KO (Select the three KO htseq-count files.) KO_01 KO_02 KO_03 Use defaults for the other fields Execute Have a look at the outputs of DESeq2. We will now filter significant (adjusted p-value < 0.05) genes from the DESeq2 result file. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cDESeq2 result file on data \u2026\u201d With following condition: c7 < 0.05 Execute How many differentially expressed genes with adjusted p-value < 0.05 are there?","title":"Section 5. DESeq2"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#section-6-the-importance-of-replicates","text":"Repeat the previous differential expression analysis with two samples in each group instead of three. How do you expect your results to differ when using fewer samples? Filter genes with adjusted-p-value < 0.05. How many genes are significant? Run DESeq2 again, using only one sample from each group. How many genes are now significant? Can you find genes that were identified as differentially expressed when using three samples in each condition that were not identified as differentially expressed when using two samples? What do you expect these gene\u2019s counts or logFC values to look like compared to genes that remained statistically significance? Have a look at the counts or the logFC values of these genes. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. Myoglianin is an example of a gene that showed up as differentially expressed when we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion. If we say that genes like Myoglianin was truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not.","title":"Section 6. The importance of replicates"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial_2018/#optional-extension","text":"Have a go at doing another differential expression analysis with the following Saccharomyces cerevisiae data from chromosome I. This time, the two conditions are called \u2018batch\u2019 and \u2018chem\u2019, and like before, there are three samples per condition. Batch sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq Chem sequence data: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq Gene annotation: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf","title":"Optional extension"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/","text":"body{ line-height: 2; font-size: 16px; } ol li{padding: 4px;} ul li{padding: 0px;} h4 {margin: 30px 0px 15px 0px;} div.code { font-family: \"Courier New\"; border: 1px solid; border-color: #999999; background-color: #eeeeee; padding: 5px 10px; margin: 10px; border-radius: 5px; overflow: auto; } div.question { color: #666666; background-color: #e1eaf9; padding: 15px 25px; margin: 20px; font-size: 15px; border-radius: 20px; } div.question h4 { font-style: italic; margin: 10px 0px !important; } RNA-Seq Differential Gene Expression: Basic Tutorial \u00b6 Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung Tutorial Overview \u00b6 In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data. Learning Objectives \u00b6 At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes Background \u00b6 Where does the data in this tutorial come from? \u00b6 The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \u201cDifferential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\u201d (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions. The Tuxedo Protocol \u00b6 The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here . Section 1: Preparation [15 min] \u00b6 1. Register as a new user in Galaxy if you don\u2019t already have an account ( what is Galaxy? ) \u00b6 Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login . 2. Import the RNA-seq data for the workshop. \u00b6 If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \u201cImport History\u201d on the top right and \u201cstart using this history\u201d to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as \u2018fastqsanger\u2019 and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file. 3. View and have an understanding of the files involved in RNA-seq analysis. \u00b6 You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019 Section 2: Align reads with Tophat [30 mins] \u00b6 In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here . 1. Align the RNA-seq short reads to a reference genome. \u00b6 In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is. 2. Examine the output files \u00b6 You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history. 3. Visualise the aligned reads with IGV \u00b6 The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) \u2013 an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example \u2018Tophat on data 1: accepted_hits\u2019. Click on Display with IGV \u2018webcurrent\u2019 (or \u2018local\u2019 if you have IGV installed on your computer. You will need to open IGV before you click on \u2018local\u2019). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as \u2018TopHat on data 1: splice junctions\u2019. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select \u2018TopHat on data 4: accepted_hits\u2019 (this is the accepted hits alignment file from first replicate of condition C2) and click on \u2018display with IGV local\u2019. This time we are using the \u2018local\u2019 link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section. 4. [Optional] Visualise the aligned reads in Trackster \u00b6 We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking \u2018Analyze Data\u2019 on the top Galaxy toolbar. Section 3: Test differential expression with Cuffdiff [45 min] \u00b6 The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here . 1. Examine the reference transcriptome \u00b6 Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites. 2. Run Cuffdiff to identify differentially expressed genes and transcripts \u00b6 In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]> 3. Explore the Cuffdiff output files \u00b6 There should be 11 output files from Cuffdiff. These files should all begin with something like \u201cCuffdiff on data 37, data 32, and others\u201d. FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \u201cCuffdiff on data x, data x, and others: gene FPKM tracking\u201d by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \u201cCuffdiff on data x, data x, and others: gene differential expression testing\u201d by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \u201cFilter on data x\u201d to \u201cSignificant_DE_Genes\u201d. Examine the sorted list of differentially expressed genes. Click on the eye icon next to \u201cSignificant_DE_Genes\u201d to view the data. How many genes are in the Significant_DE_Genes file? What are their names? Two genes have been identified as differentially expressed between conditions C1 and C2: Ank located at chr4:137014-150378, and CG2177 located at chr4:331557-334534 Both genes have q-values of 0.00175. \u201cCG2177\u201d located at chr4:331557-334534 was the gene that we intuitively (with IGV) saw to be differentially expressed in the previous section, in the broader region of chr4:325197-341887. Section 4. Repeat without replicates [20 min] \u00b6 In this section, we will run Cuffdiff with fewer replicates. Stop and think: Why do we need replicates for an RNA-seq differential gene expression experiment? What do you expect to happen if we only use one sample from each condition for our analysis? Repeat the differential gene expression testing from section 2, but this time only use one replicate from each condition group (C1 and C2). From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute Rename the output file to something meaningful like \u201cSignificant_DE_Genes_C1_R1_vs_C2_R1\u201d Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R1. You should get no differentially expressed genes at statistical significance of 0.05. The \u201cAnk\u201d gene and the \u201cCG1277\u201d, which were found to be significantly differentially expressed in our first analysis, are not identified as differentially expressed when we only use one sample for each condition. Repeat this no-replicates analysis, but this time specify a different set of samples. From the Galaxy tool panel, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 5: accepted_hits Library normalization method: classic-fpkm Dispersion estimation method: blind Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute Rename the output file to something meaningful like \u201cSignificant_DE_Genes_C1_R1_vs_C2_R2\u201d Click on the eye icon of Significant_DE_Genes_C1_R1_vs_C2_R2. We now see \u201cCG2177\u201d appear again in the list as significantly differentially expressed, but not \u201cAnk\u201d. How can we interpret the difference in results from using different replicates? There is a larger absolute difference in CG1277 expression between samples 1 (C1_R1) and 5 (C2_R2) than samples 1 (C1_R1) and 4 (C2_R1), hence Cuffdiff identifies CG1277 as differentially expressed between C1_R1 and C2_R2, but not between C1_R1 and C2_R1. On the other hand, differences in level of expression of Ank is much smaller between samples, so we need to see it consistently across multiple replicates for Cuffdiff to be confident it actually exists. One replicate is not enough. The identification of differentially expressed genes is based on the size of the difference in expression and the variance observed across multiple replicates. This demonstrates how important it is to have biological replicates in differential gene expression experiments. If we say that genes Ank and CG2177 are truly differentially expressed, we can call these instances where the true differentially expressed genes are not identified as false negatives. Generally, increasing replicates decreases the number of false negatives. It is also more likely to see more false positives when using an insufficient number of replicates. False positives can be defined as identifiying a gene as differentially expressed when it is, in reality, not. [Optional step] Repeat this analysis, specifying groups of two replicates each. What do you get? How many replicates do we need to identify Ank as differentially expressed? Section 5. Optional Extension [20 min] \u00b6 Extension on the Tuxedo Protocol \u00b6 The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn\u2019t have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with \u2018assembled transcripts\u2019 produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute Rename the output file to something meaningful like \u201cSignificant_DE_Genes_using_Cufflinks_Assembly\u201d Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177). Transcript-level differential expression \u00b6 One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \u201cunit of aggregation\u201d to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation. References \u00b6 Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"RNAseq analysis using Tuxedo (Galaxy)"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#rna-seq-differential-gene-expression-basic-tutorial","text":"Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung","title":"RNA-Seq Differential Gene Expression: Basic Tutorial"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#tutorial-overview","text":"In this tutorial we cover the concepts of RNA-seq differential gene expression (DGE) analysis using a small synthetic dataset from the model organism, Drosophila melanogaster. The tutorial is designed to introduce the tools, datatypes and workflow of an RNA-seq DGE analysis. In practice, real datasets would be much larger and would contain sequencing and alignment errors that make analysis more difficult. Our input data for this tutorial will be raw RNA-seq reads from two experimental conditions and we will output a list of differentially expressed genes identified to be statistically significant. In this tutorial we will: introduce the types of files typically used in RNA-seq analysis align RNA-seq reads with Tophat visualise RNA-seq alignment data with IGV find differentially expressed genes with Cuffdiff understand the importance of replicates for differential expression analysis This tutorial does not cover the following steps that you would do in a real RNA-seq DGE analysis: QC (quality control) of the raw sequence data Trimming the reads for quality and for adaptor sequences QC of the RNA-seq alignment data These steps have been omitted because the data we use in this tutorial is synthetic and has no quality issues, unlike real data.","title":"Tutorial Overview"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#learning-objectives","text":"At the end of this tutorial you should: Be familiar with the Tuxedo Protocol workflow for RNA-seq differential expression analysis Be able to process raw RNA sequence data into a list of differentially expressed genes Be aware of how the relationship between the number of biological replicates in an experiment and the statistical power available to detect differentially expressed genes","title":"Learning Objectives"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#background","text":"","title":"Background"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#where-does-the-data-in-this-tutorial-come-from","text":"The data for this tutorial is from an RNA-seq experiment looking for differentially expressed genes in D. melanogaster (fruit fly) between two experimental conditions. The experiment and analysis protocol we will follow is derived from a paper in Nature Protocols by the research group responsible for one of the most widely used set of RNA-seq analysis tools: \u201cDifferential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks\u201d (Trapnell et al 2012). The sequence datasets are single-end Illumina synthetic short reads, filtered to only include chromosome 4 to facilitate faster mapping (which would otherwise take hours). We\u2019ll use data from three biological replicates from each of the two experimental conditions.","title":"Where does the data in this tutorial come from?"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#the-tuxedo-protocol","text":"The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first mapped with TopHat and a transcriptome is then assembled using Cufflinks. Cuffdiff then quantifies the expression in each condition, and tests for differential expression. In this tutorial we use a simpler protocol as the D. melanogaster transcriptome is already very well characterised. More information about the Tuxedo protocol can be found here .","title":"The Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-1-preparation-15-min","text":"","title":"Section 1: Preparation [15 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account-what-is-galaxy","text":"Open a browser and go to a Galaxy server. This can either be your personal GVL server you started previously , the public Galaxy Tutorial server or the public Galaxy Australia . Recommended browsers include Firefox and Chrome. Internet Explorer is not supported. Register as a new user by clicking User > Register on the top dark-grey bar. Alternatively, if you already have an account, login by clicking User > Login .","title":"1.  Register as a new user in Galaxy if you don\u2019t already have an account (what is Galaxy?)"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-import-the-rna-seq-data-for-the-workshop","text":"If you are using the public Galaxy Tutorial server or Galaxy Melbourne server, you can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories on the top toolbar, and selecting the history called RNA-Seq_Basic_Sec_1 . Then click on \u201cImport History\u201d on the top right and \u201cstart using this history\u201d to switch to the newly imported history. Alternatively, if you are using your own personal Galaxy server, you can import the data by: In the tool panel located on the left, under Basic Tools select Get Data > Upload File . Click on the Paste/Fetch data button on the bottom section of the pop-up window. Upload the sequence data by pasting the following links into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq Select the type as \u2018fastqsanger\u2019 and press start to upload the files to Galaxy. Upload the annotated gene list reference by pasting the following link into the text input area: https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf You don\u2019t need to specify the type for this file as Galaxy will auto-detect the file as a GTF file.","title":"2.  Import the RNA-seq data for the workshop."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-view-and-have-an-understanding-of-the-files-involved-in-rna-seq-analysis","text":"You should now have the following files in your Galaxy history: 6 files containing single-ended reads: C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq And 1 gene annotation file: ensembl_dm3.chr4.gtf These files can be renamed by clicking the pen icon if you wish. These 6 sequencing files are in FASTQ format and have the file extension: .fq. If you are not familiar with the FASTQ format, click here for an overview . Click on the eye icon to the top right of each FASTQ file to view the first part of the file. The first 3 files are from the first condition (C1) and has 3 replicates labelled R1, R2, and R3. The next 3 FASTQ files are from the second condition (C2) and has 3 replicates labelled R1, R2, and R3. In this tutorial, we aim to find genes which are differentially expressed between condition C1 and condition C2. The gene annotation file is in GTF format. This file describes where the genes are located in the Drosophila reference genome. We will examine this file more closely later in Section 3 of this tutorial. NOTE: Since the reads in this dataset are synthetic, they do not have real quality scores. NOTE: If you log out of Galaxy and log back at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History\u2019","title":"3.  View and have an understanding of the files involved in RNA-seq analysis."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-2-align-reads-with-tophat-30-mins","text":"In this section we map the reads in our FASTQ files to a reference genome. As these reads originate from mRNA, we expect some of them will cross exon/intron boundaries when we align them to the reference genome. Tophat is a splice-aware mapper for RNA-seq reads that is based on Bowtie. It uses the mapping results from Bowtie to identify splice junctions between exons. More information on Tophat can be found here .","title":"Section 2: Align reads with Tophat [30 mins]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-align-the-rna-seq-short-reads-to-a-reference-genome","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat and set the parameters as follows: Is this single-end or paired-end data? Single-end RNA-Seq FASTQ file: (Click on the multiple datasets icon and select all six of the FASTQ files. This can be done by holding down the shift key to select a range of files, or holding down the ctrl key (Windows) or command key (OSX) and clicking to select multiple files.) C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq Use a built in reference genome or own from your history: Use built-in genome Select a reference genome: D. melanogaster Apr. 2006 (BDGP R5/dm3) (dm3) Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- (function(w,d,u){if(!w.<span><span class=\"MathJax_Preview\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.</span><script type=\"math/tex\">){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w. =w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document) //-->]]> //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink0\").click(function(e){ e.preventDefault(); $(\"#showable0\").toggleClass(\"showable-hidden\"); }); }); //-->]]> Note: This may take a few minutes, depending on how busy the server is.","title":"1.  Align the RNA-seq short reads to a reference genome."},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-examine-the-output-files","text":"You should have 5 output files for each of the FASTQ input files: Tophat on data 1: accepted_hits: This is a BAM file containing sequence alignment data of the reads. This file contains the location of where the reads mapped to in the reference genome. We will examine this file more closely in the next step. Tophat on data 1: splice junctions: This file lists all the places where Tophat had to split a read into two pieces to span an exon junction. Tophat on data 1: deletions and Tophat on data 1: insertions: These files list small insertions or deletions found in the reads. Since we are working with synthetic reads we can ignore Tophat for Illumina data 1:insertions Tophat for Illumina data 1:deletions for now. Tophat on data 1: align_summary: This file gives some mapping statistics including the number of reads mapped and the mapping rate. You should have a total of 30 Tophat output files in your history.","title":"2.  Examine the output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-visualise-the-aligned-reads-with-igv","text":"The purpose of this step is to : visualise the quantitative, exon-based nature of RNA-seq data visualise the expression differences between samples represented by the quantity of reads, and become familiar with the Integrative Genomics Viewer (IGV) \u2013 an interactive visualisation tool by the Broad Institute. To visualise the alignment data: Click on one of the Tophat accepted hits files, for example \u2018Tophat on data 1: accepted_hits\u2019. Click on Display with IGV \u2018webcurrent\u2019 (or \u2018local\u2019 if you have IGV installed on your computer. You will need to open IGV before you click on \u2018local\u2019). This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV) Once IGV opens, it will show you the accepted_hits BAM file. (Note: this may take a bit of time as the data is downloaded to IGV) Select chr4 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. You should see the characteristic distribution of RNA-seq reads across the exons of the genes, with some gaps at intron/exon boundaries. The number of reads aligned to a particular gene is proportional to the abundance of the RNA derived from that gene in the sequenced sample. (Note that IGV already has a list of known genes of most major organisms including Drosophila, which is why you can see the genes in the bottom panel of IGV.) View one of the splice function files such as \u2018TopHat on data 1: splice junctions\u2019. You will need to save this file to your local disk using the disk icon under the details of the file. Then open the saved .bed file directly in IGV using the File > Load From File option from IGV. This is because IGV doesn\u2019t automatically stream BED files from Galaxy. The junctions file is loaded at the bottom of the IGV window and splicing events are represented as coloured arcs. The height and thickness of the arcs are proportional to the read depth. View differentially expressed genes by viewing two alignment files simultaneously. The aim of this tutorial is to statistically test differential expression, but first it\u2019s useful to reassure ourselves that the data looks right at this stage by comparing the aligned reads for condition 1 (C1) and condition 2 (C2). Select \u2018TopHat on data 4: accepted_hits\u2019 (this is the accepted hits alignment file from first replicate of condition C2) and click on \u2018display with IGV local\u2019. This time we are using the \u2018local\u2019 link, as we already have an IGV window up and running locally from the last step. One the file has loaded, change the location to chr4:325197-341887 using the field on the top toolbar. The middle gene in this area clearly looks like it has many more reads mapped in condition 2 than condition 1, whereas for the surrounding genes the reads look about the same. The middle gene looks like it is differentially expressed. But, of course, it may be that there are many more reads in the readsets for C1 and C2, and the other genes are underexpressed in condition 2. So we need to statistically normalise the read counts before we can say anything definitive, which we will do in the next section.","title":"3.  Visualise the aligned reads with IGV"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#4-optional-visualise-the-aligned-reads-in-trackster","text":"We can also use the inbuilt Galaxy genome browser, Trackster, to visualise alignments. Trackster has fewer features than IGV, but sometimes it may be more convenient to use as it only requires the browser. On the top bar of Galaxy, select Visualization > New Track Browser . Name your new visualization and select D. melanogaster (dm3) as the reference genome build. Click the Add Datasets to Visualization button and select Tophat on data 1: accepted_hits and Tophat on data 4: accepted_hits by using the checkboxes on the left. Select chr4 from the dropdown box. You can zoom in and out using the buttons on the top toolbar. You can also add more tracks using the Add Tracks icon located on the top right. Next to the drop down list, click on the chromosomal position number display and specify the location chr4:325197-341887 . Before starting the next section, leave the Trackster interface and return to the analysis view of Galaxy by clicking \u2018Analyze Data\u2019 on the top Galaxy toolbar.","title":"4.  [Optional] Visualise the aligned reads in Trackster"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-3-test-differential-expression-with-cuffdiff-45-min","text":"The aim in this section is to: generate tables of normalised read counts per gene per condition based on the annotated reference transcriptome, statistically test for expression differences in normalised read counts for each gene, taking into account the variance observed between samples, for each gene, calculate the p-value of the gene being differentially expressed\u2013 this is the probability of seeing the data or something more extreme given the null hypothesis (that the gene is not differentially expressed between the two conditions), for each gene, estimate the fold change in expression between the two conditions. All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff is part of the Cufflinks software suite which takes the aligned reads from Tophat and generates normalised read counts and a list of differentially expressed genes based on a reference transcriptome - in this case, the curated Ensembl list of D. melanogaster genes from chromosome 4 that we supply as a GTF (Gene Transfer Format) file. A more detailed explanation of Cufflinks DGE testing can be found here .","title":"Section 3: Test differential expression with Cuffdiff [45 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-examine-the-reference-transcriptome","text":"Click on the eye icon to display the ensembl_dm3.chr4.gtf reference transcriptome file in Galaxy. The reference transcriptome is essentially a list of chromosomal features which together define genes. Each feature is in turn defined by a chromosomal start and end point, feature type (CDS, gene, exon etc), and parent gene and transcript. Importantly, a gene may have many features, but one feature will belong to only one gene. More information on the GTF format can be found here . The ensembl_dm3.chr4.gtf file contains ~4900 features which together define the 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff uses the reference transcriptome to aggregate read counts per gene, transcript, transcription start site and coding sequence (CDS). For this tutorial, we\u2019ll only consider differential gene testing, but it is also possible to test for differential expression of transcripts or transcription start sites.","title":"1.  Examine the reference transcriptome"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts","text":"In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: ensembl_dm3.chr4.gtf Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits (Multiple datasets can be selected by holding down the shift key or the ctrl key for Windows or the command key for OSX.) 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Show screenshot //<![CDATA[<!-- $(document).ready(function(){ $(\"#showablelink1\").click(function(e){ e.preventDefault(); $(\"#showable1\").toggleClass(\"showable-hidden\"); }); }); //-->]]>","title":"2.  Run Cuffdiff to identify differentially expressed genes and transcripts"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-explore-the-cuffdiff-output-files","text":"There should be 11 output files from Cuffdiff. These files should all begin with something like \u201cCuffdiff on data 37, data 32, and others\u201d. FPKM tracking files: transcript FPKM tracking gene FPKM tracking TSS groups FPKM tracking CDS FPKM tracking These 4 files contain the FPKM (a unit of normalised expression taking into account the transcript length for each transcript and the library size of the sample) for each of the two conditions. Differential expression testing files: gene differential expression testing transcript differential expression testing TSS groups differential expression testing CDS FPKM differential expression testing CDS overloading diffential expression testing promoters differential expression testing splicing differential expression testing These 7 files contain the statistical results from testing the level of expression between condition C1 and condition C2. Examine the tables of normalised gene counts View the Cuffdiff file \u201cCuffdiff on data x, data x, and others: gene FPKM tracking\u201d by clicking on the eye icon . The file consists of one row for each gene from the reference transcriptome, with columns containing the normalised read counts for each of the two conditions. Note: Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates to a gene. Multiple transcription start sites are aggregated under a single tracking_id. A gene encompasses a chromosomal locus which covers all the features that make up that gene (exons, introns, 5\u2019 UTR, etc). Inspect the gene differential expression testing file View the Cuffdiff file \u201cCuffdiff on data x, data x, and others: gene differential expression testing\u201d by clicking on the eye icon . The columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10), p_value (c12), q_value (c13) and significant (c14). Filter based on column 14 (\u2018significant\u2019) - a binary assessment of q_value > 0.05, where q_value is p_value adjusted for multiple testing. Under Basic Tools, click on Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute This will keep only those entries that Cuffdiff has marked as significantly differentially expressed. We can rename this file (screenshot) by clicking on the pencil icon of the outputted file and change the name from \u201cFilter on data x\u201d to \u201cSignificant_DE_Genes\u201d. Examine the sorted list of differentially expressed genes. Click on the eye icon next to \u201cSignificant_DE_Genes\u201d to view the data.","title":"3.  Explore the Cuffdiff output files"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-4-repeat-without-replicates-20-min","text":"In this section, we will run Cuffdiff with fewer replicates.","title":"Section 4. Repeat without replicates [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-5-optional-extension-20-min","text":"","title":"Section 5. Optional Extension [20 min]"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#extension-on-the-tuxedo-protocol","text":"The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge, and CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by identifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow you to identify new genes and transcripts, and then analyse them for differential expression. This is critical for organisms in which the transcriptome is not well characterised. CummeRbund helps visualise the data produced from the Cuffdiff using the R statistical programming language. Read more on the full Tuxedo protocol here . If the organism we were working on did not have a well characterized reference transcriptome, we would run Cufflinks and Cuffmerge to create a transcriptome. Suppose we didn\u2019t have our Drosophila GTF file containing the location of known genes. We can use Cufflinks to assemble transcripts from the alignment data to create GTF files. From the Galaxy tool panel, select NGS: RNA Analysis > Cufflinks and set the parameters as follows: SAM or BAM file of aligned RNA-Seq reads: Click on the multiple datasets icon and select all 6 BAM files from Tophat Max Intron Length: 50000 Use defaults for the other fields Execute Next, we want to merge the assemblies outputted by Cufflinks by selecting NGS: RNA Analysis > Cuffmerge and setting the parameters as follows: GTF file(s) produced by Cufflinks: Select the 6 GTF files ending with \u2018assembled transcripts\u2019 produced by Cufflinks. Use the ctrl key or command key to select multiple files. Use defaults for the other fields Execute Note: In cases where you have a reference GTF, but also want to identify novel transcripts with Cufflinks, you would add the reference GTF to the cuffmerge inputs with the Additional GTF Inputs (Lists) parameter. View the Cuffmerge GTF file by clicking the eye icon Run Cuffdiff using the new GTF file In the Galaxy tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff and set the parameters as follows: Transcripts: Cuffmerge on data x, data x, and others: merged transcripts Condition: 1: Condition name: C1 Replicates: Tophat on data 1: accepted_hits Tophat on data 2: accepted_hits Tophat on data 3: accepted_hits 2: Condition name: C2 Replicates: Tophat on data 4: accepted_hits Tophat on data 5: accepted_hits Tophat on data 6: accepted_hits Use defaults for the other fields Execute Filter the recently generated gene set for significantly differentially expressed genes by going to Filter and Sort > Filter : Filter: \u201cCuffdiff on data....: gene differential expression testing\u201d With following condition: c14==\u2019yes\u2019 Execute Rename the output file to something meaningful like \u201cSignificant_DE_Genes_using_Cufflinks_Assembly\u201d Viewing the significant genes, we see that there are two genes that are identified as differentially expressed by Cuffdiff using the GTF file produced from Cufflinks and Cuffmerge. The locations of these two genes correspond to the previous result from section 3 (genes Ank and CG2177).","title":"Extension on the Tuxedo Protocol"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#transcript-level-differential-expression","text":"One can think of a scenario in an experiment aiming to investigate the differences between two experimental conditions, where a gene had the same number of read counts in the two conditions but these read counts were derived from different transcripts; this gene would not be identified in a differential gene expression test, but would be in a differential transcript expression test. The choice of what \u201cunit of aggregation\u201d to use in differential expression testing is one that should be made by the biological investigator, and will affect the bioinformatics analysis done (and probably the data generation too). Take a look at the different differential expression files produced by Cuffdiff from section 3 which use different units of aggregation.","title":"Transcript-level differential expression"},{"location":"tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#references","text":"Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. Nature Protocols [serial online]. March 1, 2012;7(3):562-578.","title":"References"},{"location":"tutorials/rna_seq_dge_in_r/rna_seq_r/","text":"RNA-seq analysis in R \u00b6 We recommend this course produced by COMBINE: RNA-seq analysis in R The tutorial introduces the analysis of RNA-seq count data using R. This includes reading the data into R, quality control and preprocessing, and performing differential expression analysis and gene set testing, with a focus on the limma-voom analysis workflow. Additional RNA-seq analysis links \u00b6 Van Den Berge K. et al, 2018. RNA sequencing data: hitchhiker\u2019s guide to expression analysis. PeerJ Preprints 6:e27283v2 Love MI. et al, 2016. RNA-Seq workflow: gene-level exploratory analysis and differential expression. F1000Research, 4:1070","title":"RNAseq differential expression in R"},{"location":"tutorials/rna_seq_dge_in_r/rna_seq_r/#rna-seq-analysis-in-r","text":"We recommend this course produced by COMBINE: RNA-seq analysis in R The tutorial introduces the analysis of RNA-seq count data using R. This includes reading the data into R, quality control and preprocessing, and performing differential expression analysis and gene set testing, with a focus on the limma-voom analysis workflow.","title":"RNA-seq analysis in R"},{"location":"tutorials/rna_seq_dge_in_r/rna_seq_r/#additional-rna-seq-analysis-links","text":"Van Den Berge K. et al, 2018. RNA sequencing data: hitchhiker\u2019s guide to expression analysis. PeerJ Preprints 6:e27283v2 Love MI. et al, 2016. RNA-Seq workflow: gene-level exploratory analysis and differential expression. F1000Research, 4:1070","title":"Additional RNA-seq analysis links"},{"location":"tutorials/rna_seq_exp_design/","text":"PR reviewers and advice: Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/","text":"RNA-Seq Experimental Design \u00b6 What is RNA-seq? \u00b6 RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes. Why is a good experimental design vital? \u00b6 An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you\u2019re planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage. Terminology \u00b6 Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth. The importance of replicates to estimate variance \u00b6 When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can\u2019t be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis. How many replicates and how many reads do I need? \u00b6 Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions. Sequencing options to consider \u00b6 How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3\u2019 end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility. Summary \u00b6 A good experimental design is vital for a successful experiment. If you\u2019re planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"RNAseq Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#rna-seq-experimental-design","text":"","title":"RNA-Seq Experimental Design"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#what-is-rna-seq","text":"RNA-seq is a method of measuring gene expression using shotgun sequencing. The process involves reverse transcribing RNA into cDNA, then sequencing fragments on a high-throughput platform such as Illumina to obtain a large number of short reads. For each sample, the reads are then aligned to a genome, and the number of reads aligned to each gene or feature is recorded. A typical RNA-seq experiment aims to find differentially expressed genes between two conditions (e.g. up and down-regulated genes in knock-out mice compared to wild-type mice). RNA-seq can also be used to discover new transcripts, splice variants, and fusion genes.","title":"What is RNA-seq?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#why-is-a-good-experimental-design-vital","text":"An RNA-seq experiment produces high dimensional data. This means we get a huge number of observations for a small number of samples. For example, the expression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3 wild-type). A frequently used approach to analyse RNA-seq data is to fit each gene to a linear model where for each of the 20,000 genes, parameters need to be estimated using a small number of observations. To complicate matters, each measurement of gene expression is comprised of a mix of biological signal and unwanted noise. Thus, in order to perform a robust statistical analysis, the methodology must be carefully designed. Before you begin any RNA-seq experiment, some questions you should ask yourself are: Why do you expect to find differentially expressed genes in the particular tissue? What types of genes do you expect to find differentially expressed? What are the sources of variability from your samples? Where do you expect most of your variation to come from? A coherent experimental design is the groundwork of a successful experiment. You should invest time and thought in designing a robust experiment as failing to think this step through can lead to unusable data and wasted time, money, and effort. It is also useful to think about the statistical methods you will use to analyse the data. If you\u2019re planning to bring a data analyst or bioinformatician onboard for data analysis, you should include him or her in the experimental design stage.","title":"Why is a good experimental design vital?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#terminology","text":"Before progressing, it may be useful to define some terms which are commonly used in RNA-seq. Variability: A measure of how much the data is spread around. Variance is mathematically defined as the average of the squared difference between observations and the expected value. Simply put, a larger variance means it is harder to identify differentially expressed genes. Feature: A defined genomic region. Usually a gene in RNA-seq, but can also refer to any region such as an exon or an isoform. In RNA-seq, an estimate of abundance is obtained for each feature. Biological replicates: Samples that have been obtained from biologically separate samples. This can mean different individual organisms (e.g. tissue samples from different mice), different samplings of the same tumour, or different population of cells grown separately from each other but originating from the same cell-line. For example, the samples obtained from three different knock-out mice could be considered biological replicates in a knock-out versus wild-type experiment. A biological replicate combines both technical and biological variability as it is also an independent case of all the technical steps. Technical replicates: Samples in which the starting biological sample is the same, but the replicates are processed separately. For example, if a biological sample is divided and two different library preps are processed and sequenced, those two samples would be considered technical replicates. Covariate: The term 'covariate' is often used interchangeably with 'factor' or 'variable' in RNA-seq. The term refers to a property of the sample which may have some influence on gene expression and should be represented in the RNA-seq model. Covariates in RNA-seq are often categorical (e.g. treatment condition, sex, batch), but continuous factors are also possible (e.g. time points, age). A linear model will contain terms to represent the relationships between covariates and each sample. Each possible value a factor can take is called a level (e.g. 'male' and 'female' are two levels in the factor 'sex'). Factors can either be directly of interest to the experiment (e.g. treatment condition) or not of interest (also known as nuisance variables) (e.g. sex, batch). The purpose of covariates is to explain the variance seen in samples. Confounding variable: A confounding variable is a nuisance variable that is associated with the factor of interest. Possible confounding factors should be controlled for so they don't interfere with analysis. For example, if all knock-out mice samples were harvested in the morning and all wild-type mice samples were harvested in the afternoon, the time of sample collection would be a confounding factor as the effects from sample collection time and from the knock-out cannot be separated. Statistical power: The ability to identify differentially expressed genes when there really is a difference. This is partly dependent on variance and therefore is affected by the number of replicates available and sequencing depth.","title":"Terminology"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#the-importance-of-replicates-to-estimate-variance","text":"When performing a differential gene expression analysis, we look at the expression values of each gene and try to determine if the expression is significantly different in the different conditions (e.g. knock-out and wild-type). The ability to distinguish whether a gene is differentially expressed is partly determined by the estimates of variability obtained by using multiple observations in each condition. Variability is present in two forms: technical variability and biological variability. Combined biological and technical variability is measured using biological replicates. Biological variability is the main source of variability and is due to natural variation in the population and within cells. This includes different individuals having different levels of a particular gene and the stochastic nature of expression levels in different cells. Technical variability is measured using technical replicates. Technical variability is often very small compared to biological variability. Usually the question is whether an observed difference is greater than the total variability (i.e. significant). As combined variability is measured by biological replicates technical replicates are only important if you need to know the degree of biological variability or technical variability. An example of wanting technical variability would be method development. The main source of technical variation comes from RNA processing and from library prep. Variability from sequencing in different flow cells or different lanes is usually minimal. Generally, creating technical replicates from multiple library preps is unnecessary for RNA-seq experiments. The amount of variance between your biological replicates will affect the outcome of your analysis. Ideally, you aim to have minimal variability between samples so you only measure the effect of the condition of interest. Too much variability between samples can drown out the signal of truly differentially expressed genes. Controlling for possible confounding factors between conditions is also important to prevent falsely attributing differential expression to the condition of interest. Strategies to minimise variation between samples and to control confounding variables include: choosing organisms from the same litter, choosing organisms of the same sex if possible, using a constant sample collection time, having the same laboratory technician perform each library prep, randomising samples to prevent a confounding batch effect if all samples can\u2019t be processed at one time. If variation between samples can not be removed it should be balanced between conditions of interest as much as possible, and carefully recorded to allow its effect to be measured and potentially removed during analysis.","title":"The importance of replicates to estimate variance"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#how-many-replicates-and-how-many-reads-do-i-need","text":"Two very common question asked are: how many biological replicates do I need, and what sequencing depth is needed for each sample in order to have enough statistical power for my RNA-seq experiment? These questions cannot be precisely answered without a pilot study. A small amount of data (minimum of two biological replicates for each condition with at least 10M reads) can estimate the amount of biological variation, which determines how many biological replicates are required. Performing a pilot study is highly recommended to estimate statistical power and identify possible problems before investing more time and money into the project. Scotty is a web-based tool that uses data generated from a pilot study to optimize a design for statistical power. With a limited budget, one must balance sequence coverage and number of biological replicates. Scotty also has a cost estimate feature which returns the most powerful design within budget constraints. As a general rule, the number of biological replicates should never be below 3. For a basic RNA-seq differential expression experiment, 10M to 20M reads per sample is usually enough. If similar data exists it can be helpful to check the read counts for key genes of interest to estimate the required depth. Biological variability is usually the largest effect limiting the power of RNA-seq analysis. The most improvement in an experiment will usually be achieved by increasing the biological replication to improve estimation of the biological variation. It is often possible to design experiments where the analysis is done incrementally such that a pilot study is added to with an additional block of samples or a pool of libraries is sequenced to additional depth. In these cases care must be taken to balance the design in a manner that each stage is a valid experiment in its own right. This can allow a focused question to be answered in the first stage, with an ability to either address issues or progress to a second stage with additional questions.","title":"How many replicates and how many reads do I need?"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#sequencing-options-to-consider","text":"How much total RNA is needed: Many sequencing centres such as AGRF recommend at least 250ng of total RNA for RNA sequencing. It is possible to go as low as 100ng of total RNA, but results are not guaranteed. The quality of RNA is also important when making libraries. A RNA Integrity Number (RIN) is a number from 1 (poor) to 10 (good) and can indicate how much degradation there is in the sample. A poor score can lead to over representation at the 3\u2019 end of the transcript and low yield. Samples with low RIN scores (below 8) are not recommended for sequencing. Care should also be taken to ensure RIN is consistent between conditions to avoid confounding this technical effect with the biological question. Choosing an enrichment method: Ribosomal RNA makes up >95% of total cellular RNA, so a preparation for RNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA. Poly-A enrichment is recommended for most standard RNA-seq experiments, but will not provide information about microRNAs and other non-coding RNA species. In general, ribo-depleted RNA-seq data will contain more noise, however, the protocol is recommended if you have poor or variable quality of RNA as the 3\u2019 bias of poly-A enrichment will be more pronounced with increased RNA degradation. The amount of RNA needed for each method differs. For Poly-A enrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of 200ng is recommended. Choosing read type: For basic differential expression analysis RNA-seq experiments, single-end sequencing is recommended to obtain gene transcript counts. In more advanced experiments, paired-ends are useful for determining transcript structure and discovering splice variants. Choosing strandedness: With a non-directional (unstranded) protocol, there is no way to identify whether a read originated from the coding strand or its reverse complement. Non-directional protocols allow mapping of a read to a genomic location, but not the direction in which the RNA was transcribed. They are therefore used to count transcripts for known genes, and are recommended for basic RNA-seq experiments. Directional protocols (stranded) preserve strand information and are useful for novel transcript discovery. Multiplexing: Multiplexing is an approach to sequence multiple samples in the same sequencing lane. By sequencing all samples in the same lane, multiplexing can also minimise bias from lane effects. Spike-in controls: RNA-seq spike-in controls are a set of synthetic RNAs of known concentration which act as negative or positive controls. These controls have been used for normalisation and quality control, but recent work has shown that the amount of technical variability in their use dramatically reduces their utility.","title":"Sequencing options to consider"},{"location":"tutorials/rna_seq_exp_design/rna_seq_experimental_design/#summary","text":"A good experimental design is vital for a successful experiment. If you\u2019re planning to work with a data analyst or bioinformatician, include them in the design stage. Aim to minimise variability by identifying possible sources of variance in your samples. Biological replicates are important. The number of biological replicates you should have should never be below 3. Technical replicates are often unnecessary. Pilot studies are highly recommended for identifying how many replicates and how many reads you should have for enough statistical power in your experiment. For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded sequencing at depths of 10M to 20M is probably what you want.","title":"Summary"},{"location":"tutorials/singlecell/","text":"10X single-cell RNA-seq analysis in R \u00b6 Overview \u00b6 In this workshop, you will be learning how to analyse 10X Chromium single-cell RNA-seq profiles using R. This will include reading the count data into R, quality control, normalisation, dimensionality reduction, cell clustering and finding marker genes. The main part of the workflow uses the package. You will learn how to generate common plots for visualising single-cell data, such as t-SNE plots and heatmaps. This workshop is aimed at biologists interested in learning how to explore single-cell RNA-seq data generated by the 10X platform. This workshop is presented by Dr Yunshun Chen from Walter Eliza Hall Institute of Medical Research (WEHI). Dr Chen is a statistical bioinformatician and a senior post-doc at WEHI Bioinformatics Division. He is one of the authors and the main maintainer of the edgeR package. He has extensive experience in RNA-seq gene expression and single-cell RNA-seq analyses. Requirements \u00b6 The course is aimed at advanced PhD students, postdoctoral researchers and principal investigators. Some basic R knowledge is assumed \u2013 this is not an introduction to R course. If you are not familiar with the R statistical programming language it is compulsory that you work through an introductory R course before you attend this workshop. This workshop will cover single-cell RNA-seq analysis and assumes you have some familiarity with the more common analysis of bulk RNA-seq data. If you have no experience in analysing bulk RNA-seq data, we strongly recommend you also attend our RNA-seq Differential Gene Expression analysis in R workshop. Participants must bring a laptop with a Mac, Linux, or Windows operating system (not a tablet, Chromebook, etc.) that they have administrative privileges on. Bringing your laptop charger is advised. R and a few specific software packages should be installed in advance. The 10X data to be analysed, the gene annotation file and the gene signature RData file should also be downloaded prior to the workshop to allow us time to troubleshoot any problems you may have. Downloads: Download R from https://cran.r-project.org/ . The lastest version is recommended (version 3.6.0 for Windows and Mac OS X 10.11 and higher). Install R packages by opening R and copying the following commands into your R console 1 2 3 4 5 if ( ! requireNamespace ( \u201c BiocManager \u201d, quietly = TRUE )) install . packages ( \u201c BiocManager \u201d ) BiocManager :: install ( c ( \u201c scran \u201d, \u201c monocle \u201d, \u201c vcd \u201d )) Download the 10X single-cell RNA-seq data from GEO ( https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2510617 ). The three data files required for the workshop are: GSM2510617_P7-barcodes.tsv.gz, GSM2510617_P7-genes.tsv.gz and GSM2510617_P7-matrix.mtx.gz, of which the download links are available at the bottom of the web page. Unzip the three data files and store them in the working directory (or a sub-folder under the working directory). Download the mouse gene annotation file from http://bioinf.wehi.edu.au/edgeR/Mus_musculus.gene_info.gz and save it under the working directory. Note there is no need to unzip the file. Download gene signatures from WEHI and save it under the working directory. Check downloads are correct by looking for any error messages when running the commands: 1 2 3 4 5 6 7 8 library ( edgeR ) library ( scater ) library ( scran ) library ( monocle ) library ( vcd ) read . delim ( \u201c P7 - genes . tsv \u201d , nrow = 5 ) read . delim ( \u201c Mus_musculus . gene_info . gz \u201d , nrow = 5 ) load ( \u201c BulkSignatures . RData \u201d ) Important: If you have any trouble installing the software or packages, please contact us prior to the workshop. Workshop syllabus \u00b6 This workshop will follow this analysis guide . Additional single-cell analysis links \u00b6 An overview and comparison of protocols and analysis methods: Tian, L. et al, Biorxiv, scRNA-seq mixology: towards better benchmarking of single cell RNA-seq protocols and analysis methods Single cell RNA-seq data analysis: A course by CSC, Finland: Single cell data analysis A course by the Hemberg Lab: scRNA-seq analysis List of software packages for single-cell data analysis: Davis, S. et al: Zenodo link ; Github link","title":"10X single-cell RNA-seq analysis in R"},{"location":"tutorials/singlecell/#10x-single-cell-rna-seq-analysis-in-r","text":"","title":"10X single-cell RNA-seq analysis in R"},{"location":"tutorials/singlecell/#overview","text":"In this workshop, you will be learning how to analyse 10X Chromium single-cell RNA-seq profiles using R. This will include reading the count data into R, quality control, normalisation, dimensionality reduction, cell clustering and finding marker genes. The main part of the workflow uses the package. You will learn how to generate common plots for visualising single-cell data, such as t-SNE plots and heatmaps. This workshop is aimed at biologists interested in learning how to explore single-cell RNA-seq data generated by the 10X platform. This workshop is presented by Dr Yunshun Chen from Walter Eliza Hall Institute of Medical Research (WEHI). Dr Chen is a statistical bioinformatician and a senior post-doc at WEHI Bioinformatics Division. He is one of the authors and the main maintainer of the edgeR package. He has extensive experience in RNA-seq gene expression and single-cell RNA-seq analyses.","title":"Overview"},{"location":"tutorials/singlecell/#requirements","text":"The course is aimed at advanced PhD students, postdoctoral researchers and principal investigators. Some basic R knowledge is assumed \u2013 this is not an introduction to R course. If you are not familiar with the R statistical programming language it is compulsory that you work through an introductory R course before you attend this workshop. This workshop will cover single-cell RNA-seq analysis and assumes you have some familiarity with the more common analysis of bulk RNA-seq data. If you have no experience in analysing bulk RNA-seq data, we strongly recommend you also attend our RNA-seq Differential Gene Expression analysis in R workshop. Participants must bring a laptop with a Mac, Linux, or Windows operating system (not a tablet, Chromebook, etc.) that they have administrative privileges on. Bringing your laptop charger is advised. R and a few specific software packages should be installed in advance. The 10X data to be analysed, the gene annotation file and the gene signature RData file should also be downloaded prior to the workshop to allow us time to troubleshoot any problems you may have. Downloads: Download R from https://cran.r-project.org/ . The lastest version is recommended (version 3.6.0 for Windows and Mac OS X 10.11 and higher). Install R packages by opening R and copying the following commands into your R console 1 2 3 4 5 if ( ! requireNamespace ( \u201c BiocManager \u201d, quietly = TRUE )) install . packages ( \u201c BiocManager \u201d ) BiocManager :: install ( c ( \u201c scran \u201d, \u201c monocle \u201d, \u201c vcd \u201d )) Download the 10X single-cell RNA-seq data from GEO ( https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2510617 ). The three data files required for the workshop are: GSM2510617_P7-barcodes.tsv.gz, GSM2510617_P7-genes.tsv.gz and GSM2510617_P7-matrix.mtx.gz, of which the download links are available at the bottom of the web page. Unzip the three data files and store them in the working directory (or a sub-folder under the working directory). Download the mouse gene annotation file from http://bioinf.wehi.edu.au/edgeR/Mus_musculus.gene_info.gz and save it under the working directory. Note there is no need to unzip the file. Download gene signatures from WEHI and save it under the working directory. Check downloads are correct by looking for any error messages when running the commands: 1 2 3 4 5 6 7 8 library ( edgeR ) library ( scater ) library ( scran ) library ( monocle ) library ( vcd ) read . delim ( \u201c P7 - genes . tsv \u201d , nrow = 5 ) read . delim ( \u201c Mus_musculus . gene_info . gz \u201d , nrow = 5 ) load ( \u201c BulkSignatures . RData \u201d ) Important: If you have any trouble installing the software or packages, please contact us prior to the workshop.","title":"Requirements"},{"location":"tutorials/singlecell/#workshop-syllabus","text":"This workshop will follow this analysis guide .","title":"Workshop syllabus"},{"location":"tutorials/singlecell/#additional-single-cell-analysis-links","text":"An overview and comparison of protocols and analysis methods: Tian, L. et al, Biorxiv, scRNA-seq mixology: towards better benchmarking of single cell RNA-seq protocols and analysis methods Single cell RNA-seq data analysis: A course by CSC, Finland: Single cell data analysis A course by the Hemberg Lab: scRNA-seq analysis List of software packages for single-cell data analysis: Davis, S. et al: Zenodo link ; Github link","title":"Additional single-cell analysis links"},{"location":"tutorials/unix/","text":"PR reviewers and advice: Andrew Robinson, Peter Georgeson, Chol-hee Jung, Ben Moran Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/unix/intro/","text":"Why use unix \u00b6 Powerful : Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don\u2019t typically use a Graphical User Interface which can free up much resources for actual computing. Big data : Unix programs are designed to handle large data sets Flexible : Small programs that can be arranged in many ways to solve your problems Automation : Scripting allows you to do many tasks in one step and repeat steps many times Pipelines : Unix programs are designed to be \u2018chained\u2019 together to form long multi-step pipelines Science Software : Lots of Scientific software is designed to run in a Unix environment User interface \u00b6 The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. Command prompt \u00b6 The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: Time : the time (when the last command finished) Username : the username that you are logged in as Hostname : the name of the computer that your are connected to Current working directory : the current position within the file system that your are working. More to follow Prompt : this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a \u2018 ' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the ' ' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the ' \u2018 sign. Command line \u00b6 Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more \u2018white-space\u2019 characters (i.e. space, tab): Command : this is the name of the program (command) that you want to run Flag : these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. Long flag : same as flag except they are generally two dashes (\u2013) followed by a word (or two) Option : set the value of a configurable option. They are a flag (or long flag) followed by a value Anonymous options : these are one or more options that are specified in the required order Quoted value : if you need to specify a space (or tab) in an option then you will need to use double (\u201c) or single (\u2018) quotes on each side of the value. File system \u00b6 The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called \u2018root\u2019; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. Absolute file names \u00b6 Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. Absolute file name : /settings Absolute file name : /home/user1/file01.txt Absolute file name : /home/user2 Note : the final slash is not needed (but generally doesn\u2019t hurt if it is present). Current working directory \u00b6 The current working directory is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. Relative file names \u00b6 Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to /home you can leave this part from the beginning of the filename. Relative file name : user1/muscle.fq (Note: the absence of the leading slash) Special file names : There are a few further short cuts for typing relative file names: ~ (Tilde): is a short cut to your home directory . (dot): is a short cut for the current directory .. (2x dot): means the parent (or one directory up) from current directory \u2026 (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot\u2019s e.g. ../.. Note : the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to /home/user2 the relative path to muscle.fq is different. Relative file name : ../user1/muscle.fq","title":"Intro"},{"location":"tutorials/unix/intro/#why-use-unix","text":"Powerful : Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don\u2019t typically use a Graphical User Interface which can free up much resources for actual computing. Big data : Unix programs are designed to handle large data sets Flexible : Small programs that can be arranged in many ways to solve your problems Automation : Scripting allows you to do many tasks in one step and repeat steps many times Pipelines : Unix programs are designed to be \u2018chained\u2019 together to form long multi-step pipelines Science Software : Lots of Scientific software is designed to run in a Unix environment","title":"Why use unix"},{"location":"tutorials/unix/intro/#user-interface","text":"The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment.","title":"User interface"},{"location":"tutorials/unix/intro/#command-prompt","text":"The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: Time : the time (when the last command finished) Username : the username that you are logged in as Hostname : the name of the computer that your are connected to Current working directory : the current position within the file system that your are working. More to follow Prompt : this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a \u2018 ' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the ' ' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the ' \u2018 sign.","title":"Command prompt"},{"location":"tutorials/unix/intro/#command-line","text":"Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more \u2018white-space\u2019 characters (i.e. space, tab): Command : this is the name of the program (command) that you want to run Flag : these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. Long flag : same as flag except they are generally two dashes (\u2013) followed by a word (or two) Option : set the value of a configurable option. They are a flag (or long flag) followed by a value Anonymous options : these are one or more options that are specified in the required order Quoted value : if you need to specify a space (or tab) in an option then you will need to use double (\u201c) or single (\u2018) quotes on each side of the value.","title":"Command line"},{"location":"tutorials/unix/intro/#file-system","text":"The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called \u2018root\u2019; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on.","title":"File system"},{"location":"tutorials/unix/intro/#absolute-file-names","text":"Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. Absolute file name : /settings Absolute file name : /home/user1/file01.txt Absolute file name : /home/user2 Note : the final slash is not needed (but generally doesn\u2019t hurt if it is present).","title":"Absolute file names"},{"location":"tutorials/unix/intro/#current-working-directory","text":"The current working directory is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt.","title":"Current working directory"},{"location":"tutorials/unix/intro/#relative-file-names","text":"Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to /home you can leave this part from the beginning of the filename. Relative file name : user1/muscle.fq (Note: the absence of the leading slash) Special file names : There are a few further short cuts for typing relative file names: ~ (Tilde): is a short cut to your home directory . (dot): is a short cut for the current directory .. (2x dot): means the parent (or one directory up) from current directory \u2026 (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot\u2019s e.g. ../.. Note : the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to /home/user2 the relative path to muscle.fq is different. Relative file name : ../user1/muscle.fq","title":"Relative file names"},{"location":"tutorials/unix/robinson-unix-link/","text":"Introduction to Unix \u00b6 Please see the link here .","title":"Introduction to Unix"},{"location":"tutorials/unix/robinson-unix-link/#introduction-to-unix","text":"Please see the link here .","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/","text":"em {font-style: normal; font-family: courier new;} Introduction to Unix \u00b6 A hands-on workshop covering the basics of the Unix/Linux command line interface. Overview \u00b6 Knowledge of the Unix operating system is fundamental to being productive on HPC systems. This workshop will introduce you to the fundamental Unix concepts by way of a series of hands-on exercises. The workshop is facilitated by experienced Unix users who will be able to guide you through the exercises and offer assistance where needed. Learning Objectives \u00b6 At the end of the course, you will be able to: Log into a Unix machine remotely Organise your files into directories Change file permissions to improve security and safety Create and edit files with a text editor Copy files between directories Use command line programs to manipulate files Automate your workflow using shell scripts Requirements \u00b6 The workshop is intended for beginners with no prior experience in Unix. Attendees are required to bring their own laptop computers. Introduction \u00b6 Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the Unix concepts. The slides are available if you would like. Additionally the following reference material is available for later use. Reference Material ### Why use unix * **Powerful**: Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. * **Big data**: Unix programs are designed to handle large data sets * **Flexible**: Small programs that can be arranged in many ways to solve your problems * **Automation**: Scripting allows you to do many tasks in one step and repeat steps many times * **Pipelines**: Unix programs are designed to be 'chained' together to form long multi-step pipelines * **Science Software**: Lots of Scientific software is designed to run in a Unix environment ### User interface The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. ### Command prompt The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: * **Time**: the time (when the last command finished) * **Username**: the username that you are logged in as * **Hostname**: the name of the computer that your are connected to * **Current working directory**: the current position within the file system that your are working. More to follow * **Prompt**: this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign. ### Command line Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): * **Command**: this is the name of the program (command) that you want to run * **Flag**: these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. * **Long flag**: same as flag except they are generally two dashes (--) followed by a word (or two) * **Option**: set the value of a configurable option. They are a flag (or long flag) followed by a value * **Anonymous options**: these are one or more options that are specified in the required order * **Quoted value**: if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value. ### File system The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. #### Absolute file names Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. **Absolute file name**: */settings* **Absolute file name**: */home/user1/file01.txt* **Absolute file name**: */home/user2* **Note**: the final slash is not needed (but generally doesn't hurt if it is present). #### Current working directory The *current working directory* is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. #### Relative file names Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to */home* you can leave this part from the beginning of the filename. **Relative file name**: *user1/muscle.fq* (Note: the absence of the leading slash) **Special file names**: There are a few further short cuts for typing relative file names: * *~* (Tilde): is a short cut to your home directory * *.* (dot): is a short cut for the current directory * *..* (2x dot): means the parent (or one directory up) from current directory * *...* (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. *../..* **Note**: the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to */home/user2* the relative path to muscle.fq is different. **Relative file name**: *../user1/muscle.fq* Topic 1: Remote log in \u00b6 In this topic we will learn how to connect to a Unix computer via a program called ssh and run a few basic commands. Connecting to a Unix computer \u00b6 To begin this workshop you will need to connect to an HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC\u2019s tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: 1 2 3 The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. barcoo is a high performance computer for Melbourne Bioinformatics users. Logging in connects your local computer (e.g. laptop) to barcoo, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on barcoo for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of barcoo, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into barcoo over ssh. Exercises \u00b6 1.1) When you\u2019ve logged into the Unix server, run the following commands and see what they do: \u00b6 who whoami date cal hostname /vlsci/TRAINING/shared/Intro_to_Unix/hi Answer * **who**: displays a list of the users who are currently using this Unix computer. * **whoami**: displays your username (i.e. they person currently logged in). * **date**: displays the current date and time. * **cal**: displays a calendar on the terminal. It can be configured to display more than just the current month. * **hostname**: displays the name of the computer we are logged in to. * **/vlsci/TRAINING/shared/Intro_to_Unix/hi**: displays the text \"Hello World\" Topic 2: Exploring your home directory \u00b6 In this topic we will learn how to \u201clook\u201d at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer. 2.1) Use the ls command to list the files in your home directory. How many files are there? \u00b6 Hint Literally, type *ls* and press the *ENTER* key. Answer 1 2 $ ls exp01 file01 muscle.fq When running the *ls* command with no options it will list files in your current working directory. The place where you start when you first login is your *HOME* directory. **Answer**: 3 (exp01, file01 and muscle.fq) The above answer is not quite correct. There are a number of hidden files in your home directory as well. 2.2) What flag might you use to display all files with the ls command? How many files are really there? \u00b6 Hint Take the *all* quite literally. Additional Hint Type *ls --all* and press the *ENTER* key. Answer **Answer 1**: *--all* (or *-a*) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. 1 2 3 4 $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely *.* and *..* which are not real files but instead, shortcuts. *.* is a shortcut for the current directory and *..* a shortcut for the directory above the current one. **Answer 2**: 10 files (don't count *.* and *..*) 2.3) What is the full path name of your home directory? \u00b6 Hint Remember your *Current Working Directory* starts in your *home* directory. Additional Hint Try a shortened version of *print working directory* Answer You can find out the full path name of the current working directory with the *pwd* command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX **Answer**: */vlsci/TRAINING/trainXX* where *XX* is replaced by some 2 digit sequence. **Alternate method**: You can also find out the name of your home directory by printing the value of the *$HOME* shell variable: 1 echo $HOME 2.4) Run ls using the long flag ( -l ), how did the output change? \u00b6 Hint Run *ls -l* Answer **Answer**: it changed the output to place 1 file/directory per line. It also added some extra information about each. 1 2 3 4 5 $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11 :28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11 :28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11 :28 muscle.fq **Details**: 1 2 3 4 5 drwxr-x--- 2 training01 training 2048 Jun 14 11 :28 exp01 \\- -------/ ^ \\- -------/ \\- -----/ \\- -/ \\- ---------/ \\- --/ permission | username group size date name /---^--- \\ linkcount Where: * **permissions**: 4 parts, file type, user perms, group perms and other perms * *filetype*: 1 character, *d* = directory and *-* regular file * *user* permissions: 3 characters, *r* = read, *w* = write, *x* = execute and *-* no permission * *group* permissions: same as user except for users within the owner group * *other* permissions: same as user except for users that are not in either user *or* *group* * **username**: the user who *owns* this file/directory * **group**: the group name who *owns* this file/directory * **size**: the number of bytes this file/directory takes to store on disk * **date**: the date and time when this file/directory was *last edited* * **name**: name of the file * **linkcount**: technical detail which represents the number of links this file has in the file system (safe to ignore) 2.5) What type of file is exp01 and muscle.fq ? \u00b6 Hint Check the output from the *ls -l*. Answer **Answer**: * *exp01*: Directory (given the 'd' as the first letter of its permissions) * *muscle.fq*: Regular File (given the '-') 2.6) Who has permission to read , write and execute your home directory? \u00b6 Hint You can also give *ls* a filename as the first option. Additional Hint *ls -l* will show you the contents of the *CWD*; how might you see the contents of the *parent* directory? (remember the slides) Answer If you pass the *-l* flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. **Method 1**: given we know the *CWD* is our home directory. 1 2 3 4 $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... The *..* refers to the parent directory. **Method 2**: using $HOME. This works no matter what our *CWD* is set to. You could list the permissions of all files and directories in the parent directory of your home: 1 2 3 4 $ ls -l $HOME /.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... In this case we use the shell variable to refer to our home directory. **Method 3**: using *~* (tilde) shortcut You may also refer to your home directory using the *~* (tilde) character: 1 2 3 4 $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like *trainXX*. Where *XX* is replaced by a two digit string. **Altername**: using the *-a* flag and looking at the *.* (dot) special file. 1 2 3 4 $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 . ... **Answer**: *drwxr-x---* * **You**: read (see filenames), write (add, delete files), execute (change your CWD to this directory). * **Training users**: read, execute * **Everyone else**: No access **Discussion on Permissions**: The permission string is *\"drwxr-x---\"*. The *d* means it is a directory. The *rwx* means that the owner of the directory (your user account) can *read*, *write* and *execute* the directory. Execute permissions on a directory means that you can *cd* into the directory. The *r-x* means that anyone in the same user group as *training* can read or execute the directory. The *---* means that nobody else (other users on the system) can do anything with the directory. man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q . 2.7) Use the man command to find out what the -h flag does for ls \u00b6 Hint Give *ls* as an option to *man* command. Additional Hint *man ls* Answer Use the following command to view the *man* page for *ls*: 1 $ man ls **Answer**: You should discover that the *-h* option prints file sizes in human readable format 1 2 -h, --human-readable with -l, print sizes in human readable format ( e.g., 1K 234M 2G ) 2.8) Use the -h , how did the output change of muscle.fq ? \u00b6 Hint Don't forget the *-l* option too. Additional Hint Run *ls -lh* Answer 1 2 3 $ ls -lh ... -rw-r----- 1 training01 training 2 .5K Jun 14 11 :28 muscle.fq **Answer**: it changed the output so the *filesize* of *muscle.fq* is now *2.5K* instead of *2461* Topic 3: Exploring the file system \u00b6 In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file 3.1) Print the value of your current working directory. \u00b6 Answer The *pwd* command prints the value of your current working directory. 1 2 $ pwd /home/training01 3.2) List the contents of the root directory, called \u2018 / \u2018 (forward slash). \u00b6 Hint *ls* expects one or more anonymous options which are the files/directories to list. Answer 1 2 3 4 5 6 $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that *ls* can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory. 3.3) Use the cd command to change your working directory to the root directory. Did your prompt change? \u00b6 Hint *cd* expects a single option which is the directory to change to Answer The *cd* command changes the value of your current working directory. To change to the root directory use the following command: 1 $ cd / **Answer**: Yes, it now says the CWD is */* instead of *~*. Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen. 3.4) List the contents of the CWD and verify it matches the list in 3.2 \u00b6 Hint *ls* Answer Assuming you have changed to the root directory then this can be achieved with *ls*, or *ls -a* (for all files) or *ls -la* for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: 1 2 3 4 5 6 $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys **Answer**: Yes, we got the same output as exercise 3.2 3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system? \u00b6 Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory Additional Hint *$HOME*, *~*, */vlsci/TRAINING/trainXX* are all methods to name your home directory. Yet there is a simpler method; the answer is buried in *man cd* however *cd* doesn't have its own manpage so you will need to search for it. Answer Use the *cd* command to change your working directory to your home directory. There are a number of ways to refer to your home directory: 1 cd $HOME is equivalent to: 1 cd ~ The simplest way to change your current working directory to your home directory is to run the *cd* command with no arguments: **Answer**: the simplest for is cd with NO options. 1 cd This is a special-case behaviour which is built into *cd* for convenience. 3.6) Change your working directory to the following directory: \u00b6 /vlsci/TRAINING/shared/Intro_to_Unix Answer **Answer**: *cd /vlsci/TRAINING/shared/Intro_to_Unix* 3.7) List the contents of that directory. How many files does it contain? \u00b6 Hint *ls* Answer You can do this with *ls* 1 2 $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy **Answer**: 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy) 3.8) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/sleepy ? \u00b6 Hint Take the word *file* quite literally. Additional Hint *file sleepy* Answer Use the *file* command to get extra information about the contents of a file: Assuming your current working directory is */vlsci/TRAINING/shared/Intro_to_Unix* 1 2 $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: 1 2 $ file /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Bourne-Again shell script text executable **Answer**: Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The *file* command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: 1 man file 3.9) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/hi ? Hint Take the word *file* quite literally. Answer Use the file command again. If you are in the same directory as *hi* then: 1 2 3 $ file hi ELF 64 -bit LSB executable, x86-64, version 1 ( SYSV ) , dynamically linked ( uses shared libs ) , for GNU/Linux 2 .6.9, not stripped **Answer**: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called *hi* contains a binary executable program (raw instructions that the computer can execute directly). 3.10) What are the file permissions of the following file and what do they mean? \u00b6 /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Hint Remember the *ls* command, and don't forget the *-l* flag Answer You can find the permissions of *sleepy* using the *ls* command with the *-l* flag. If you are in the same directory as *sleepy* then: 1 2 $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16 :36 sleepy **Answer**: The Answer is dependent on the computer you are connected too however will follow something like above. We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and write-able only to arobinson. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the *ln* command. 3.11) Change your working directory back to your home directory ready for the next topic. \u00b6 Hint *cd* Answer You should know how to do this with the cd command: 1 cd Topic 4: Working with files and directories \u00b6 In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections. 4.1) In your home directory make a sub-directory called test. \u00b6 Hint You are trying to *make a directory*, which of the above commands looks like a shortened version of this? Additional Hint *mkdir* Answer Make sure you are in your home directory first. If not *cd* to your home directory. Use the *mkdir* command to make new directories: 1 $ mkdir test Use the *ls* command to check that the new directory was created. 1 2 $ ls exp01 file01 muscle.fq test 4.2) Copy all the files from the following directory into the newly created test directory: \u00b6 /vlsci/TRAINING/shared/Intro_to_Unix Hint You are trying to *copy*, which of the above commands looks like a shortened version of this? Additional Hint 1 2 3 4 5 6 7 $ man cp ... SYNOPSIS cp [ OPTION ] ... [ -T ] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE ( s ) to DIRECTORY. which means *cp* expects zero or more flags, a SOURCE file followed by a DEST file or directory Answer Use the *cp* command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to *cp*. You only need to perform one of these ways, but we show multiple ones for your reference. **Answer 1**: From your home directory: 1 $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test **Answer 2**: Change to the test directory and then copy (assuming you started in your home directory): 1 2 $ cd test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* . In the example above the '*.*' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. **Answer 3**: Change to the \\end{UNIX_TRAINING_FILES_PATH} directory and then copy: 1 2 cd /vlsci/TRAINING/shared/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. Note : This exercise assumes that the copy command from the previous exercise was successful. 4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to. \u00b6 Hint Remember *ls* can show you the file size (with one of its flags) Additional Hint *ls -l* Answer Use *ls -l* to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: 1 2 3 $ ls -l /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. **Answer**: They should each be *1033773* bytes **Alternate**: Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the *-h* option for ls: 1 2 $ ls -lh /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as *1010K*. Larger files are reported in megabytes, gigabytes etcetera. Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test 4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to. \u00b6 Hint What is the opposite of *same*? Additional Hint *diff*erence Answer Use the *diff* command to compare the contents of two files. 1 $ diff /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the *diff* command will NOT produce any output) **Answer**: Yes, they are the same since no output was given. 4.5) How many lines, words and characters are in expectations.txt? \u00b6 Hint Initialisms are key Additional Hint *w*ord *c*ount Answer Use the *wc* (for \"word count\") to count the number of characters, lines and words in a file: 1 2 $ wc expectations.txt 20415 187465 1033773 expectations.txt **Answer**: There are *20415* lines, *187465* words and *1033773* characters in expectations.txt. To get just the line, word or character count: 1 2 3 4 5 6 $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt 4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano . \u00b6 Hint *nano FILENAME* Once *nano* is open it displays some command hints along the bottom of the screen. Additional Hint *^O* means hold the *Control* (or CTRL) key while pressing the *o*. Despite what it displays, you need to type the lower-case letter that follows the *^* character. WriteOut is another name for Save. Answer Take some time to play around with the *nano* text editor. *Nano* is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as *vim* and *emacs*, however they take a substantial amount of time to learn. 4.7) Did the changes you made to ~/test/expectations.txt have any effect on /vlsci/TRAINING/shared/Intro_to_Unix ? \u00b6 How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 Additional Hint Use *diff* Answer Use *diff* to check that the two files are different after you have made the change to the copy of *expectations.txt* in your *~/test* directory. 1 2 diff ~/test/expectations.txt \\ /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt You could also use *ls* to check that the files have different sizes. 4.8) In your test subdirectory, rename expectations.txt to foo.txt . \u00b6 Hint Another way to think of it is *moving* it from *expectations.txt* to *foo.txt* Additional Hint *mv* Use *man mv* if you need to work out how to use it. Answer Use the *mv* command to rename the file: 1 2 3 $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy 4.9) Rename foo.txt back to expectations.txt. \u00b6 Answer Use the *mv* command to rename the file: 1 2 3 $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use *ls* to check that the file is in fact renamed. 4.10) Remove the file expectations.txt from your test directory. \u00b6 Hint We are trying to *remove* a file, check the commands at the top of this topic. Additional Hint *rm* Answer Use the *rm* command to remove files (carefully): 1 2 3 $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy 4.11) Remove the entire test directory and all the files within it. \u00b6 Hint We are trying to *remove a directory*. Additional Hint You could use *rmdir* but there is an easier way using just *rm* and a flag. Answer You could use the *rm* command to remove each file individually, and then use the *rmdir* command to remove the directory. Note that *rmdir* will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the *-r* (for recursive) flag to *rm* to remove all the files and the directory in one go: **Logical Answer**: 1 2 3 cd ~ rm test/* rmdir test **Easier Answer**: 1 2 cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies. 4.12) Recreate the test directory in your home directory and copy all the files from /vlsci/TRAINING/shared/Intro_to_Unix back into the test directory. \u00b6 Hint See exercises 4.1 and 4.2 Answer Repeat exercises 4.1 and 4.2. 1 2 3 $ cd ~ $ mkdir test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test 4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c \u00b6 Hint Use *man* if you can't guess how it might work. Answer 1 2 3 4 5 6 7 $ cd ~/test $ cat hello.c #include <stdio.h> int main ( void ) { printf ( \"Hello World\\n\" ) ; return 0 ; } *hello.c* contains the source code of a C program. The compiled executable version of this code is in the file called *hi*, which you can run like so: 1 2 $ ./hi Hello World 4.14) Use the head command to view the first 20 lines of the file sample_1.fastq \u00b6 Hint Remember your *best* friend! Additional Hint Use *man* to find out what option you need to add to display a given number of *lines*. Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa `` aaa ` aaaa_^a ``` ]][ Z [ DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa ` a ``` ^aaaaa ` _ ] aaa ` aaa__a_X ] `` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa ` aaaaaaa^ba_ ]] aaa^aaaaa_^ ][ aa 4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq \u00b6 Hint It's very much like *head*. Answer 1 2 3 4 5 6 7 8 9 tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab ] abbaa ` babaaabbb ` bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 ` aaaaa `` aaa ` a `` a ` ^a ` a ` a_ [ a_a ` a ` aa ` __ 4.16) Use the grep command to find out all the lines in moby.txt that contain the word \u201cAhab\u201d \u00b6 Hint One might say we are 'looking for the *pattern* \"Ahab\"' Additional Hint 1 2 3 4 5 $ man grep ... SYNOPSIS grep [ OPTIONS ] PATTERN [ FILE... ] ... Answer 1 2 3 4 5 $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the *wc -l* command: 1 2 $ grep Ahab moby.txt | wc -l 491 which shows that there are *491* lines in *moby.txt* that contain the word Ahab. 4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \u201cthe\u201d with a case insensitive search (it should count \u201cthe\u201d \u201cThe\u201d \u201cTHE\u201d \u201ctHe\u201d etcetera). \u00b6 Hint One might say we are *ignoring case*. Additional Hint 1 2 3 4 5 $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. ( -i is specified by POSIX. ) ... Answer Use the *-i* flag to *grep* to make it perform case insensitive search: 1 2 3 4 5 6 $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [ Project Gutenberg Editor ' s Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to *wc -l* to count the number of lines: 1 2 $ grep -i the expectations.txt | wc -l 8165 4.18) Use the gzip command to compress the file sample_1.fastq . Use gunzip to decompress it back to the original contents. \u00b6 Hint Use the above commands along with *man* and *ls* to see what happens to the file. Answer Check the file size of sample_1.fastq before compressing it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20 :03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20 :03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20 :03 sample_1.fastq You will see that when it was compressed it is *26997595* bytes in size, making it about *0.3* times the size of the original file. **Note**: in the above section the lines starting with *#* are comments so don't need to be copied but if you do then they wont do anything. Topic 5: Pipes, output redirection and shell scripts \u00b6 In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \u201ctip of the iceberg\u201d. Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut 5.1) How many reads are contained in the file sample_1.fastq ? \u00b6 Hint Examine some of the file to work out how many lines each *read* takes up. Additional Hint Count the number of lines Answer We can answer this question by counting the number of lines in the file and dividing by 4: 1 2 $ wc -l sample_1.fastq 3000000 **Answer**: There are *3000000* lines in the file representing *750000* reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called *bc*: 1 2 $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. *bc* is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera). 5.2) How many reads in sample_1.fastq contain the sequence GATTACA ? \u00b6 Hint Check out exercise 4.16 Answer Use *grep* to find all the lines that contain *GATTACA* and \"pipe\" the output to *wc -l* to count them: 1 2 $ grep GATTACA sample_1.fastq | wc -l 1119 **Answer**: *1119* If you are unsure about the possibility of upper and lower case characters then consider using the *-i* (ignore case option for grep). 5.3) On what line numbers do the sequences containing GATTACA occur? \u00b6 Hint We are looking for the *line numbers*. Additional Hint Check out the manpage for *grep* and/or *nl* Answer You can use the *-n* flag to grep to make it prefix each line with a line number: **Answer 1**: 1 2 3 4 5 $ grep -n GATTACA sample_1.fastq 5078 :AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 :AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 :ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Answer 2**: Or you can use the *nl* command to number each line of sample_1.fastq and then search for *GATTACA* in the numbered lines: 1 2 3 4 5 $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Just the line numbers**: If you just want to see the line numbers then you can \"pipe\" the output of the above command into *cut -f 1*: 1 2 3 4 5 $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... *cut* will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the *-f 1* option) 1 2 3 4 5 $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... 5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning. \u00b6 Hint Check answer to 5.3. Answer 1 2 3 4 5 6 7 8 9 10 $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with *control-c* (hold the *control* key down and simultaneously press the \"*c*\" character). 5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl . \u00b6 Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and \u2018 / \u2018 to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, *> FILENAME* is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" Answer 1 $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \"*>*\" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the *head* command like so: 1 2 3 4 5 6 7 8 9 10 $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT ... The *less* command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the '*/*' followed by the search term. You can quit by pressing \"*q*\". Note that the *less* command is used by default to display man pages. 1 $ less sample_1.fastq.nl 5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq into TSV format: \u00b6 1 $ cat sample_1.fastq | paste - - - - > sample_1.tsv Answer The *'-'* (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all will support it. The *paste* command is useful for merging multiple files together line-by-line, such that the *Nth* line from each file is joined together into one line in the output, separated by default with a *tab* character. In the above example we give paste 4 copies of the contents of *sample_1.fastq*, which causes it to join consecutive groups of 4 lines from the file into one line of output. 5.7) Do you expect the output of the following command to produce the same output as above? and why? \u00b6 1 $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) Hint Use *less* to examine it. Answer **Answer**: No, in the second instance we get 4 copies of each line. **Why**: In the first command *paste* will use the input file (standard input) 4 times since the *cat* command will only give one copy of the file to *paste*, where as, in the second command *paste* will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point. 5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file. \u00b6 Hint Remember the *wc* command. Answer We can count the number of lines in *sample_1.tsv* using *wc*: 1 $ wc -l sample_1.tsv The output should be *750000* as expected (1/4 of the number of lines in sample_1.fastq). To view the first *20* lines of *sample_1.tsv* use the *head* command: 1 $ head -20 sample_1.tsv 5.9) Use the cut command to print out the second column of sample_1.tsv . Redirect the output to a file called sample_1.dna.txt . \u00b6 Hint See exercise 5.3 (for cut) and 5.5 (redirection) Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). 1 cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using *head* or *less*. 5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt . Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As. \u00b6 Hint Use *man* (sort) and see exercise 5.5 (redirection) Answer 1 $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running *head* on the output file reveals that there are duplicate DNA sequences in the input FASTQ file. 5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt , redirect the result to sample_1.dna.uniq.txt . Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt . \u00b6 Hint I am pretty sure you have already used *man* (or just guessed how to use *uniq*). You're also a gun at redirection now. Answer 1 $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: 1 2 3 4 $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of *sample_1.dna.uniq.txt* to check that the duplicate DNA sequences have been removed. 5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt ? \u00b6 Hint Checkout the *uniq* manpage Additional Hint Look at the man page for uniq. Answer Use the *-d* flag to *uniq* to print out only the duplicated lines from the file: 1 $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt 5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq. \u00b6 Hint That is, *piping* most of the commands you used above instead of redirecting to file Additional Hint i.e. 6 commands (*cat*, *paste*, *cut*, *sort*, *uniq*, *wc*) Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: **Answer**: 1 2 $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have *56079* lines. 5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq. \u00b6 Hint Check out the *sleepy* file (with *cat* or *nano*); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out *chmod* command). Answer Put the answer to *5.13* into a file called *sample_1_dups.sh* (or whatever you want). Use *nano* to create the file. **Answer**: the contents of the file will look like this: 1 2 3 #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: 1 $ chmod +x sample_1_dups.sh You can run the script like so: 1 $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13. 5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter. \u00b6 Hint Shell scripts can refer to command line arguments by their position using special variables called *$0*, *$1*, *$2* and so on. Additional Hint *$0* refers to the name of the script as it was called on the command line. *$1* refers to the first command line argument, and so on. Answer Copy the shell script from *5.14* into a new file: 1 $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: 1 2 3 #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: 1 $ ./fastq_dups.sh sample_1.fastq In the above example the script takes *sample_1.fastq* as input and prints the number of duplicated sequences as output. **A better Answer**: Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: 1 2 3 4 5 6 7 #!/bin/bash if [ $# -eq 1 ] ; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The '*if ...; then*' line means: do the following line(s) ONLY if the *...* (called condition) bit is true. The '*else*' line means: otherwise do the following line(s) instead. Note: it is optional. The '*fi*' line means: this marks the end of the current *if* or *else* section. The '*[ $# -eq 1 ]*' part is the condition: * *$#*: is a special shell variable that indicates how many command line arguments were given. * *-eq*: checks if the numbers on either side of it are equal. * *1*: is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you. 5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file. \u00b6 Answer We can add a loop to our script to accept multiple input FASTQ files: 1 2 3 4 5 #!/bin/bash for file in $@ ; do dups = $( cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l ) echo \" $file $dups \" done There's a lot going on in this script. The *$@* is a sequence of all command line arguments. The '*for ...; do*' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called *file*. The *$(...)* allow us to capture the output of another command (in-place of the *...*). In this case we capture the output of the pipeline and save it to the variable called *dups*. If you had multiple FASTQ files available you could run the script like so: 1 ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: 1 2 3 sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY Finished \u00b6 Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don\u2019t forget to complete the training survey and give it back to the workshop facilitators.","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/#introduction-to-unix","text":"A hands-on workshop covering the basics of the Unix/Linux command line interface.","title":"Introduction to Unix"},{"location":"tutorials/unix/unix/#overview","text":"Knowledge of the Unix operating system is fundamental to being productive on HPC systems. This workshop will introduce you to the fundamental Unix concepts by way of a series of hands-on exercises. The workshop is facilitated by experienced Unix users who will be able to guide you through the exercises and offer assistance where needed.","title":"Overview"},{"location":"tutorials/unix/unix/#learning-objectives","text":"At the end of the course, you will be able to: Log into a Unix machine remotely Organise your files into directories Change file permissions to improve security and safety Create and edit files with a text editor Copy files between directories Use command line programs to manipulate files Automate your workflow using shell scripts","title":"Learning Objectives"},{"location":"tutorials/unix/unix/#requirements","text":"The workshop is intended for beginners with no prior experience in Unix. Attendees are required to bring their own laptop computers.","title":"Requirements"},{"location":"tutorials/unix/unix/#introduction","text":"Before we commence the hands-on part of this workshop we will first give a short 30 minute talk to introduce the Unix concepts. The slides are available if you would like. Additionally the following reference material is available for later use. Reference Material ### Why use unix * **Powerful**: Unix computers are typically very powerful in comparision to your desktop/laptop computers. Additionally they don't typically use a Graphical User Interface which can free up much resources for actual computing. * **Big data**: Unix programs are designed to handle large data sets * **Flexible**: Small programs that can be arranged in many ways to solve your problems * **Automation**: Scripting allows you to do many tasks in one step and repeat steps many times * **Pipelines**: Unix programs are designed to be 'chained' together to form long multi-step pipelines * **Science Software**: Lots of Scientific software is designed to run in a Unix environment ### User interface The Unix interface is a text-based command driven one; often known as a Command Line Interface (CLI). This means that you control it by issuing (i.e. typing) commands at the command prompt. Consequently, the Mouse does not perform any function in the unix environment. ### Command prompt The command prompt is the first thing you see when you connect to a Unix Computer. Its purpose is to receive the next command from you, the user. There are several parts that make up the command prompt: * **Time**: the time (when the last command finished) * **Username**: the username that you are logged in as * **Hostname**: the name of the computer that your are connected to * **Current working directory**: the current position within the file system that your are working. More to follow * **Prompt**: this is simply a sign to the user that the computer is ready to accept the next command From this point forward in this document, the command prompt will be simply represented as a '$' rather than the full command prompt as shown above. When copying and pasting commands you should NOT copy the '$' sign. ### Command line Below is an example command with various flags and options. There are a number of parts which may be included in a command; each is separated by one of more 'white-space' characters (i.e. space, tab): * **Command**: this is the name of the program (command) that you want to run * **Flag**: these turn on (or off) specific features in the program. They consist of a dash (-) followed by a single character. * **Long flag**: same as flag except they are generally two dashes (--) followed by a word (or two) * **Option**: set the value of a configurable option. They are a flag (or long flag) followed by a value * **Anonymous options**: these are one or more options that are specified in the required order * **Quoted value**: if you need to specify a space (or tab) in an option then you will need to use double (\") or single (') quotes on each side of the value. ### File system The file-system of a unix computer can be thought of as an up-side-down tree. The topmost directory has a special name called 'root'; it contains all files and directories that are on the computer system. It is represented by a single slash (/). The figure below shows an example file system with directories (black outline boxes) and files (grey dashed boxes). At the top level we have one file (settings) and one directory (home). Inside the home directory we have two directories (user1 and user2) and so on. #### Absolute file names Absolute file names are a single unique name for each file and directory within the computer. They start with the slash (/) character and follow all the parent directories above the file/directory. **Absolute file name**: */settings* **Absolute file name**: */home/user1/file01.txt* **Absolute file name**: */home/user2* **Note**: the final slash is not needed (but generally doesn't hurt if it is present). #### Current working directory The *current working directory* is the current location within the file system that you are currently using. When you first login to a unix computer it will begin with the current working directory set to your home directory, that is, a place that is unique to you and generally nobody else will have access to it. Remember from earlier that the current working directory is shown in the command prompt. #### Relative file names Relative file names are a short cut to writing file names that are shorter. The difference between an absolute file name is that relative file names do NOT begin with a slash. If your current working directory is set to */home* you can leave this part from the beginning of the filename. **Relative file name**: *user1/muscle.fq* (Note: the absence of the leading slash) **Special file names**: There are a few further short cuts for typing relative file names: * *~* (Tilde): is a short cut to your home directory * *.* (dot): is a short cut for the current directory * *..* (2x dot): means the parent (or one directory up) from current directory * *...* (3x dot): does not mean anything (a gotcha for new users). If you want 2 directories up then chain two double dot's e.g. *../..* **Note**: the special file names above can be used within absolute and relative file name and used multiple times. Now, if the current working directory is changed to */home/user2* the relative path to muscle.fq is different. **Relative file name**: *../user1/muscle.fq*","title":"Introduction"},{"location":"tutorials/unix/unix/#topic-1-remote-log-in","text":"In this topic we will learn how to connect to a Unix computer via a program called ssh and run a few basic commands.","title":"Topic 1: Remote log in"},{"location":"tutorials/unix/unix/#connecting-to-a-unix-computer","text":"To begin this workshop you will need to connect to an HPC. Today we will use barcoo . The computer called barcoo.vlsci.org.au is the one that coordinates all the HPC\u2019s tasks. Server details : host : barcoo.vlsci.org.au port : 22 username : (provided at workshop) password : (provided at workshop) Mac OS X / Linux Both Mac OS X and Linux come with a version of ssh (called OpenSSH) that can be used from the command line. To use OpenSSH you must first start a terminal program on your computer. On OS X the standard terminal is called Terminal, and it is installed by default. On Linux there are many popular terminal programs including: xterm, gnome-terminal, konsole (if you aren't sure, then xterm is a good default). When you've started the terminal you should see a command prompt. To log into *barcoo*, for example, type this command at the prompt and press return (where the word *username* is replaced with your *barcoo* username): *$ ssh username@barcoo.vlsci.org.au* The same procedure works for any other machine where you have an account except that if your Unix computer uses a port other than 22 you will need to specify the port by adding the option *-p PORT* with PORT substituted with the port number. You may be presented with a message along the lines of: 1 2 3 The authenticity of host 'barcoo.vlsci.org.au (131.172.36.150)' can't be established. ... Are you sure you want to continue connecting (yes/no)? Although you should never ignore a warning, this particular one is nothing to be concerned about; type **yes** and then **press enter**. If all goes well you will be asked to enter your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Windows On Microsoft Windows (Vista, 7, 8) we recommend that you use the PuTTY ssh client. PuTTY (putty.exe) can be downloaded from this web page: [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) Documentation for using PuTTY is here: [http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/docs.html) When you start PuTTY you should see a window which looks something like this: To connect to *barcoo* you should enter *barcoo.vlsci.org.au* into the box entitled \"Host Name (or IP address)\" and *22* in the port, then click on the Open button. All of the settings should remain the same as they were when PuTTY started (which should be the same as they are in the picture above). In some circumstances you will be presented with a window entitled PuTTY Security Alert. It will say something along the lines of *\"The server's host key is not cached in the registry\"*. This is nothing to worry about, and you should agree to continue (by clicking on Yes). You usually see this message the first time you try to connect to a particular remote computer. If all goes well, a terminal window will open, showing a prompt with the text *\"login as:\"*. An example terminal window is shown below. You should type your *barcoo* username and press enter. After entering your username you will be prompted for your password. Assuming you type the correct username and password the system should then display a welcome message, and then present you with a Unix prompt. If you get this far then you are ready to start entering Unix commands and thus begin using the remote computer. Note : for security reasons ssh will not display any characters when you enter your password. This can be confusing because it appears as if your typing is not recognised by the computer. Don\u2019t be alarmed; type your password in and press return at the end. barcoo is a high performance computer for Melbourne Bioinformatics users. Logging in connects your local computer (e.g. laptop) to barcoo, and allows you to type commands into the Unix prompt which are run on the HPC, and have the results displayed on your local screen. You will be allocated a training account on barcoo for the duration of the workshop. Your username and password will be supplied at the start of the workshop. Log out of barcoo, and log back in again (to make sure you can repeat the process). All the remaining parts assume that you are logged into barcoo over ssh.","title":"Connecting to a Unix computer"},{"location":"tutorials/unix/unix/#exercises","text":"","title":"Exercises"},{"location":"tutorials/unix/unix/#11-when-youve-logged-into-the-unix-server-run-the-following-commands-and-see-what-they-do","text":"who whoami date cal hostname /vlsci/TRAINING/shared/Intro_to_Unix/hi Answer * **who**: displays a list of the users who are currently using this Unix computer. * **whoami**: displays your username (i.e. they person currently logged in). * **date**: displays the current date and time. * **cal**: displays a calendar on the terminal. It can be configured to display more than just the current month. * **hostname**: displays the name of the computer we are logged in to. * **/vlsci/TRAINING/shared/Intro_to_Unix/hi**: displays the text \"Hello World\"","title":"1.1) When you've logged into the Unix server, run the following commands and see what they do:"},{"location":"tutorials/unix/unix/#topic-2-exploring-your-home-directory","text":"In this topic we will learn how to \u201clook\u201d at the filesystem and further expand our repertoire of Unix commands. Duration : 20 minutes. Relevant commands : ls , pwd , echo , man Your home directory contains your own private working space. Your current working directory is automatically set to your home directory when you log into a Unix computer.","title":"Topic 2: Exploring your home directory"},{"location":"tutorials/unix/unix/#21-use-the-ls-command-to-list-the-files-in-your-home-directory-how-many-files-are-there","text":"Hint Literally, type *ls* and press the *ENTER* key. Answer 1 2 $ ls exp01 file01 muscle.fq When running the *ls* command with no options it will list files in your current working directory. The place where you start when you first login is your *HOME* directory. **Answer**: 3 (exp01, file01 and muscle.fq) The above answer is not quite correct. There are a number of hidden files in your home directory as well.","title":"2.1) Use the ls command to list the files in your home directory.  How many files are there?"},{"location":"tutorials/unix/unix/#22-what-flag-might-you-use-to-display-all-files-with-the-ls-command-how-many-files-are-really-there","text":"Hint Take the *all* quite literally. Additional Hint Type *ls --all* and press the *ENTER* key. Answer **Answer 1**: *--all* (or *-a*) flag Now you should see several files in your home directory whose names all begin with a dot. All these files are created automatically for your user account. They are mostly configuration options for various programs including the shell. It is safe to ignore them for the moment. 1 2 3 4 $ ls --all . .bash_logout exp01 .lesshst .. .bash_profile file01 muscle.fq .bash_history .bashrc .kshrc .viminfo There are two trick files here; namely *.* and *..* which are not real files but instead, shortcuts. *.* is a shortcut for the current directory and *..* a shortcut for the directory above the current one. **Answer 2**: 10 files (don't count *.* and *..*)","title":"2.2) What flag might you use to display all files with the ls command?  How many files are really there?"},{"location":"tutorials/unix/unix/#23-what-is-the-full-path-name-of-your-home-directory","text":"Hint Remember your *Current Working Directory* starts in your *home* directory. Additional Hint Try a shortened version of *print working directory* Answer You can find out the full path name of the current working directory with the *pwd* command. Your home directory will look something like this: 1 2 $ pwd /home/trainingXX **Answer**: */vlsci/TRAINING/trainXX* where *XX* is replaced by some 2 digit sequence. **Alternate method**: You can also find out the name of your home directory by printing the value of the *$HOME* shell variable: 1 echo $HOME","title":"2.3) What is the full path name of your home directory?"},{"location":"tutorials/unix/unix/#24-run-ls-using-the-long-flag-l-how-did-the-output-change","text":"Hint Run *ls -l* Answer **Answer**: it changed the output to place 1 file/directory per line. It also added some extra information about each. 1 2 3 4 5 $ ls -l total 32 drwxr-x--- 2 training01 training 2048 Jun 14 11 :28 exp01 -rw-r----- 1 training01 training 97 Jun 14 11 :28 file01 -rw-r----- 1 training01 training 2461 Jun 14 11 :28 muscle.fq **Details**: 1 2 3 4 5 drwxr-x--- 2 training01 training 2048 Jun 14 11 :28 exp01 \\- -------/ ^ \\- -------/ \\- -----/ \\- -/ \\- ---------/ \\- --/ permission | username group size date name /---^--- \\ linkcount Where: * **permissions**: 4 parts, file type, user perms, group perms and other perms * *filetype*: 1 character, *d* = directory and *-* regular file * *user* permissions: 3 characters, *r* = read, *w* = write, *x* = execute and *-* no permission * *group* permissions: same as user except for users within the owner group * *other* permissions: same as user except for users that are not in either user *or* *group* * **username**: the user who *owns* this file/directory * **group**: the group name who *owns* this file/directory * **size**: the number of bytes this file/directory takes to store on disk * **date**: the date and time when this file/directory was *last edited* * **name**: name of the file * **linkcount**: technical detail which represents the number of links this file has in the file system (safe to ignore)","title":"2.4) Run ls using the long flag (-l), how did the output change?"},{"location":"tutorials/unix/unix/#25-what-type-of-file-is-exp01-and-musclefq","text":"Hint Check the output from the *ls -l*. Answer **Answer**: * *exp01*: Directory (given the 'd' as the first letter of its permissions) * *muscle.fq*: Regular File (given the '-')","title":"2.5) What type of file is exp01 and muscle.fq?"},{"location":"tutorials/unix/unix/#26-who-has-permission-to-read-write-and-execute-your-home-directory","text":"Hint You can also give *ls* a filename as the first option. Additional Hint *ls -l* will show you the contents of the *CWD*; how might you see the contents of the *parent* directory? (remember the slides) Answer If you pass the *-l* flag to ls it will display a \"long\" listing of file information including file permissions. There are various ways you could find out the permissions on your home directory. **Method 1**: given we know the *CWD* is our home directory. 1 2 3 4 $ ls -l .. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... The *..* refers to the parent directory. **Method 2**: using $HOME. This works no matter what our *CWD* is set to. You could list the permissions of all files and directories in the parent directory of your home: 1 2 3 4 $ ls -l $HOME /.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... In this case we use the shell variable to refer to our home directory. **Method 3**: using *~* (tilde) shortcut You may also refer to your home directory using the *~* (tilde) character: 1 2 3 4 $ ls -l ~/.. ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 trainingXY ... All 3 of the methods above mean the same thing. You will see a list of files and directories in the parent directory of your home directory. One of them will be the name of your home directory, something like *trainXX*. Where *XX* is replaced by a two digit string. **Altername**: using the *-a* flag and looking at the *.* (dot) special file. 1 2 3 4 $ ls -la ... drwxr-x--- 4 trainingXY training 512 Feb 9 14 :18 . ... **Answer**: *drwxr-x---* * **You**: read (see filenames), write (add, delete files), execute (change your CWD to this directory). * **Training users**: read, execute * **Everyone else**: No access **Discussion on Permissions**: The permission string is *\"drwxr-x---\"*. The *d* means it is a directory. The *rwx* means that the owner of the directory (your user account) can *read*, *write* and *execute* the directory. Execute permissions on a directory means that you can *cd* into the directory. The *r-x* means that anyone in the same user group as *training* can read or execute the directory. The *---* means that nobody else (other users on the system) can do anything with the directory. man is for manual : and it will be your best friend! Manual pages include a lot of detail about a command and its available flags/options. It should be your first (or second) port of call when you are trying to work out what a command or option does. You can scroll up and down in the man page using the arrow keys. You can search in the man page using the forward slash followed by the search text followed by the ENTER key. e.g. type /hello and press ENTER to search for the word hello . Press n key to find next occurance of hello etc. You can quit the man page by pressing q .","title":"2.6) Who has permission to read, write and execute your home directory?"},{"location":"tutorials/unix/unix/#27-use-the-man-command-to-find-out-what-the-h-flag-does-for-ls","text":"Hint Give *ls* as an option to *man* command. Additional Hint *man ls* Answer Use the following command to view the *man* page for *ls*: 1 $ man ls **Answer**: You should discover that the *-h* option prints file sizes in human readable format 1 2 -h, --human-readable with -l, print sizes in human readable format ( e.g., 1K 234M 2G )","title":"2.7) Use the man command to find out what the -h flag does for ls"},{"location":"tutorials/unix/unix/#28-use-the-h-how-did-the-output-change-of-musclefq","text":"Hint Don't forget the *-l* option too. Additional Hint Run *ls -lh* Answer 1 2 3 $ ls -lh ... -rw-r----- 1 training01 training 2 .5K Jun 14 11 :28 muscle.fq **Answer**: it changed the output so the *filesize* of *muscle.fq* is now *2.5K* instead of *2461*","title":"2.8) Use the -h, how did the output change of muscle.fq?"},{"location":"tutorials/unix/unix/#topic-3-exploring-the-file-system","text":"In this topic we will learn how to move around the filesystem and see what is there. Duration : 30 minutes. Relevant commands : pwd , cd , ls , file","title":"Topic 3: Exploring the file system"},{"location":"tutorials/unix/unix/#31-print-the-value-of-your-current-working-directory","text":"Answer The *pwd* command prints the value of your current working directory. 1 2 $ pwd /home/training01","title":"3.1) Print the value of your current working directory."},{"location":"tutorials/unix/unix/#32-list-the-contents-of-the-root-directory-called-forward-slash","text":"Hint *ls* expects one or more anonymous options which are the files/directories to list. Answer 1 2 3 4 5 6 $ ls / applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys Here we see that *ls* can take a filepath as its argument, which allows you to list the contents of directories other than your current working directory.","title":"3.2) List the contents of the root directory, called '/' (forward slash)."},{"location":"tutorials/unix/unix/#33-use-the-cd-command-to-change-your-working-directory-to-the-root-directory-did-your-prompt-change","text":"Hint *cd* expects a single option which is the directory to change to Answer The *cd* command changes the value of your current working directory. To change to the root directory use the following command: 1 $ cd / **Answer**: Yes, it now says the CWD is */* instead of *~*. Some people imagine that changing the working directory is akin to moving your focus within the file system. So people often say \"move to\", \"go to\" or \"charge directory to\" when they want to change the working directory. The root directory is special in Unix. It is the topmost directory in the whole file system. Output on ERROR only : Many Unix commands will not produce any output if everything went well; cd is one such command. However, it will get grumpy if something went wrong by way of an error message on-screen.","title":"3.3) Use the cd command to change your working directory to the root directory.  Did your prompt change?"},{"location":"tutorials/unix/unix/#34-list-the-contents-of-the-cwd-and-verify-it-matches-the-list-in-32","text":"Hint *ls* Answer Assuming you have changed to the root directory then this can be achieved with *ls*, or *ls -a* (for all files) or *ls -la* for a long listing of all files. If you are not currently in the root directory then you can list its contents by passing it as an argument to ls: 1 2 3 4 5 6 $ ls applications-merged etc media root tmp bin home mnt sbin usr boot lib oldhome selinux var data lib64 opt srv dev lost+found proc sys **Answer**: Yes, we got the same output as exercise 3.2","title":"3.4) List the contents of the CWD and verify it matches the list in 3.2"},{"location":"tutorials/unix/unix/#35-change-your-current-working-directory-back-to-your-home-directory-what-is-the-simplest-unix-command-that-will-get-you-back-to-your-home-directory-from-anywhere-else-in-the-file-system","text":"Hint The answer to exercise 2.6 might give some hints on how to get back to the home directory Additional Hint *$HOME*, *~*, */vlsci/TRAINING/trainXX* are all methods to name your home directory. Yet there is a simpler method; the answer is buried in *man cd* however *cd* doesn't have its own manpage so you will need to search for it. Answer Use the *cd* command to change your working directory to your home directory. There are a number of ways to refer to your home directory: 1 cd $HOME is equivalent to: 1 cd ~ The simplest way to change your current working directory to your home directory is to run the *cd* command with no arguments: **Answer**: the simplest for is cd with NO options. 1 cd This is a special-case behaviour which is built into *cd* for convenience.","title":"3.5) Change your current working directory back to your home directory. What is the simplest Unix command that will get you back to your home directory from anywhere else in the file system?"},{"location":"tutorials/unix/unix/#36-change-your-working-directory-to-the-following-directory","text":"/vlsci/TRAINING/shared/Intro_to_Unix Answer **Answer**: *cd /vlsci/TRAINING/shared/Intro_to_Unix*","title":"3.6) Change your working directory to the following directory:"},{"location":"tutorials/unix/unix/#37-list-the-contents-of-that-directory-how-many-files-does-it-contain","text":"Hint *ls* Answer You can do this with *ls* 1 2 $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy **Answer**: 7 files (expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy)","title":"3.7) List the contents of that directory. How many files does it contain?"},{"location":"tutorials/unix/unix/#38-what-kind-of-file-is-vlscitrainingsharedintro_to_unixsleepy","text":"Hint Take the word *file* quite literally. Additional Hint *file sleepy* Answer Use the *file* command to get extra information about the contents of a file: Assuming your current working directory is */vlsci/TRAINING/shared/Intro_to_Unix* 1 2 $ file sleepy Bourne-Again shell script text executable Otherwise specify the full path of sleepy: 1 2 $ file /vlsci/TRAINING/shared/Intro_to_Unix/sleepy Bourne-Again shell script text executable **Answer**: Bourne-Again shell script text executable The \"Bourne-Again shell\" is more commonly known as BASH. The *file* command is telling us that sleepy is (probably) a shell script written in the language of BASH. The file command uses various heuristics to guess the \"type\" of a file. If you want to know how it works then read the Unix manual page like so: 1 man file 3.9) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/hi ? Hint Take the word *file* quite literally. Answer Use the file command again. If you are in the same directory as *hi* then: 1 2 3 $ file hi ELF 64 -bit LSB executable, x86-64, version 1 ( SYSV ) , dynamically linked ( uses shared libs ) , for GNU/Linux 2 .6.9, not stripped **Answer**: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux This rather complicated output is roughly saying that the file called *hi* contains a binary executable program (raw instructions that the computer can execute directly).","title":"3.8) What kind of file is /vlsci/TRAINING/shared/Intro_to_Unix/sleepy?"},{"location":"tutorials/unix/unix/#310-what-are-the-file-permissions-of-the-following-file-and-what-do-they-mean","text":"/vlsci/TRAINING/shared/Intro_to_Unix/sleepy Hint Remember the *ls* command, and don't forget the *-l* flag Answer You can find the permissions of *sleepy* using the *ls* command with the *-l* flag. If you are in the same directory as *sleepy* then: 1 2 $ ls -l sleepy -rw-r--r-- 1 arobinson common 183 Feb 9 16 :36 sleepy **Answer**: The Answer is dependent on the computer you are connected too however will follow something like above. We can see that this particular instance of sleepy is owned by the user arobinson, and is part of the common user group. It is 183 bytes in size, and was last modified on the 9th of February at 4:36pm. The file is readable to everyone, and write-able only to arobinson. The digit '1' between the file permission string and the owner indicates that there is one link to the file. The Unix file system allows files to be referred to by multiple \"links\". When you create a file it is referred to by one link, but you may add others later. For future reference: links are created with the *ln* command.","title":"3.10) What are the file permissions of the following file and what do they mean?"},{"location":"tutorials/unix/unix/#311-change-your-working-directory-back-to-your-home-directory-ready-for-the-next-topic","text":"Hint *cd* Answer You should know how to do this with the cd command: 1 cd","title":"3.11) Change your working directory back to your home directory ready for the next topic."},{"location":"tutorials/unix/unix/#topic-4-working-with-files-and-directories","text":"In this topic we will start to read, create, edit and delete files and directories. Duration : 50 minutes. Relevant commands : mkdir , cp , ls , diff , wc , nano , mv , rm , rmdir , head , tail , grep , gzip , gunzip Hint : Look at the commands above; you will need them roughly in order for this topic. Use the man command find out what they do, in particular the NAME, SYNOPSIS and DESCRIPTION sections.","title":"Topic 4: Working with files and directories"},{"location":"tutorials/unix/unix/#41-in-your-home-directory-make-a-sub-directory-called-test","text":"Hint You are trying to *make a directory*, which of the above commands looks like a shortened version of this? Additional Hint *mkdir* Answer Make sure you are in your home directory first. If not *cd* to your home directory. Use the *mkdir* command to make new directories: 1 $ mkdir test Use the *ls* command to check that the new directory was created. 1 2 $ ls exp01 file01 muscle.fq test","title":"4.1) In your home directory make a sub-directory called test."},{"location":"tutorials/unix/unix/#42-copy-all-the-files-from-the-following-directory-into-the-newly-created-test-directory","text":"/vlsci/TRAINING/shared/Intro_to_Unix Hint You are trying to *copy*, which of the above commands looks like a shortened version of this? Additional Hint 1 2 3 4 5 6 7 $ man cp ... SYNOPSIS cp [ OPTION ] ... [ -T ] SOURCE DEST ... DESCRIPTION Copy SOURCE to DEST, or multiple SOURCE ( s ) to DIRECTORY. which means *cp* expects zero or more flags, a SOURCE file followed by a DEST file or directory Answer Use the *cp* command to copy files. Wildcards : You could copy them one-by-one, but that would be tedious, so use the * wildcard to specify that you want to copy all the files. There are a number of ways you could do this depending on how you specify the source and destination paths to *cp*. You only need to perform one of these ways, but we show multiple ones for your reference. **Answer 1**: From your home directory: 1 $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test **Answer 2**: Change to the test directory and then copy (assuming you started in your home directory): 1 2 $ cd test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* . In the example above the '*.*' (dot) character refers to the current working directory. It should be the test subdirectory of your home directory. **Answer 3**: Change to the \\end{UNIX_TRAINING_FILES_PATH} directory and then copy: 1 2 cd /vlsci/TRAINING/shared/Intro_to_Unix/ cp * ~/test Remember that ~ is a shortcut reference to your home directory. Note : This exercise assumes that the copy command from the previous exercise was successful.","title":"4.2) Copy all the files from the following directory into the newly created test directory:"},{"location":"tutorials/unix/unix/#43-check-that-the-file-size-of-expectationstxt-is-the-same-in-both-the-directory-that-you-copied-it-from-and-the-directory-that-you-copied-it-to","text":"Hint Remember *ls* can show you the file size (with one of its flags) Additional Hint *ls -l* Answer Use *ls -l* to check the size of files. You could do this in many ways depending on the value of your working directory. We just show one possible way for each file: 1 2 3 $ ls -l /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt $ ls -l ~/test/expectations.txt From the output of the above commands you should be able to see the size of each file and check that they are the same. **Answer**: They should each be *1033773* bytes **Alternate**: Sometimes it is useful to get file sizes reported in more \"human friendly\" units than bytes. If this is true then try the *-h* option for ls: 1 2 $ ls -lh /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt -rw-r--r-- 1 arobinson common 1010K Mar 26 2012 /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt In this case the size is reported in kilobytes as *1010K*. Larger files are reported in megabytes, gigabytes etcetera. Note : this exercise assumes your working directory is ~/test ; if not run cd ~/test","title":"4.3) Check that the file size of expectations.txt is the same in both the directory that you copied it from and the directory that you copied it to."},{"location":"tutorials/unix/unix/#44-check-that-the-contents-of-expectationstxt-are-the-same-in-both-the-directory-that-you-copied-it-from-and-the-directory-that-you-copied-it-to","text":"Hint What is the opposite of *same*? Additional Hint *diff*erence Answer Use the *diff* command to compare the contents of two files. 1 $ diff /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt expectations.txt If the two files are identical the *diff* command will NOT produce any output) **Answer**: Yes, they are the same since no output was given.","title":"4.4) Check that the contents of expectations.txt are the same in both the directory that you copied it from and the directory that you copied it to."},{"location":"tutorials/unix/unix/#45-how-many-lines-words-and-characters-are-in-expectationstxt","text":"Hint Initialisms are key Additional Hint *w*ord *c*ount Answer Use the *wc* (for \"word count\") to count the number of characters, lines and words in a file: 1 2 $ wc expectations.txt 20415 187465 1033773 expectations.txt **Answer**: There are *20415* lines, *187465* words and *1033773* characters in expectations.txt. To get just the line, word or character count: 1 2 3 4 5 6 $ wc -l expectations.txt 20415 expectations.txt $ wc -w expectations.txt 187465 expectations.txt $ wc -c expectations.txt 1033773 expectations.txt","title":"4.5) How many lines, words and characters are in expectations.txt?"},{"location":"tutorials/unix/unix/#46-open-testexpectationstxt-in-the-nano-text-editor-delete-the-first-line-of-text-and-save-your-changes-to-the-file-exit-nano","text":"Hint *nano FILENAME* Once *nano* is open it displays some command hints along the bottom of the screen. Additional Hint *^O* means hold the *Control* (or CTRL) key while pressing the *o*. Despite what it displays, you need to type the lower-case letter that follows the *^* character. WriteOut is another name for Save. Answer Take some time to play around with the *nano* text editor. *Nano* is a very simple text editor which is easy to use but limited in features. More powerful editors exist such as *vim* and *emacs*, however they take a substantial amount of time to learn.","title":"4.6) Open ~/test/expectations.txt in the nano text editor, delete the first line of text, and save your changes to the file. Exit nano."},{"location":"tutorials/unix/unix/#47-did-the-changes-you-made-to-testexpectationstxt-have-any-effect-on-vlscitrainingsharedintro_to_unix","text":"How can you tell if two files are the same or different in their contents? Hint Remember exercise 4.4 Additional Hint Use *diff* Answer Use *diff* to check that the two files are different after you have made the change to the copy of *expectations.txt* in your *~/test* directory. 1 2 diff ~/test/expectations.txt \\ /vlsci/TRAINING/shared/Intro_to_Unix/expectations.txt You could also use *ls* to check that the files have different sizes.","title":"4.7) Did the changes you made to ~/test/expectations.txt have any effect on /vlsci/TRAINING/shared/Intro_to_Unix?"},{"location":"tutorials/unix/unix/#48-in-your-test-subdirectory-rename-expectationstxt-to-footxt","text":"Hint Another way to think of it is *moving* it from *expectations.txt* to *foo.txt* Additional Hint *mv* Use *man mv* if you need to work out how to use it. Answer Use the *mv* command to rename the file: 1 2 3 $ mv expectations.txt foo.txt $ ls foo.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy","title":"4.8) In your test subdirectory, rename expectations.txt to foo.txt."},{"location":"tutorials/unix/unix/#49-rename-footxt-back-to-expectationstxt","text":"Answer Use the *mv* command to rename the file: 1 2 3 $ mv foo.txt expectations.txt $ ls expectations.txt hello.c hi jude.txt moby.txt sample_1.fastq sleepy Use *ls* to check that the file is in fact renamed.","title":"4.9) Rename foo.txt back to expectations.txt."},{"location":"tutorials/unix/unix/#410-remove-the-file-expectationstxt-from-your-test-directory","text":"Hint We are trying to *remove* a file, check the commands at the top of this topic. Additional Hint *rm* Answer Use the *rm* command to remove files (carefully): 1 2 3 $ rm expectations.txt $ ls hello.c hi jude.txt moby.txt sample_1.fastq sleepy","title":"4.10) Remove the file expectations.txt from your test directory."},{"location":"tutorials/unix/unix/#411-remove-the-entire-test-directory-and-all-the-files-within-it","text":"Hint We are trying to *remove a directory*. Additional Hint You could use *rmdir* but there is an easier way using just *rm* and a flag. Answer You could use the *rm* command to remove each file individually, and then use the *rmdir* command to remove the directory. Note that *rmdir* will only remove directories that are empty (i.e. do not contain files or subdirectories). A faster way is to pass the *-r* (for recursive) flag to *rm* to remove all the files and the directory in one go: **Logical Answer**: 1 2 3 cd ~ rm test/* rmdir test **Easier Answer**: 1 2 cd ~ rm -r test Warning : Be very careful with rm -r , it will remove all files and all subdirectories underneath the specified directory. This could be catastrophic if you do it in the wrong location! Now is a good moment to pause and think about file backup strategies.","title":"4.11) Remove the entire test directory and all the files within it."},{"location":"tutorials/unix/unix/#412-recreate-the-test-directory-in-your-home-directory-and-copy-all-the-files-from-vlscitrainingsharedintro_to_unix-back-into-the-test-directory","text":"Hint See exercises 4.1 and 4.2 Answer Repeat exercises 4.1 and 4.2. 1 2 3 $ cd ~ $ mkdir test $ cp /vlsci/TRAINING/shared/Intro_to_Unix/* test","title":"4.12) Recreate the test directory in your home directory and copy all the files from /vlsci/TRAINING/shared/Intro_to_Unix back into the test directory."},{"location":"tutorials/unix/unix/#413-change-directories-to-test-and-use-the-cat-command-to-display-the-entire-contents-of-the-file-helloc","text":"Hint Use *man* if you can't guess how it might work. Answer 1 2 3 4 5 6 7 $ cd ~/test $ cat hello.c #include <stdio.h> int main ( void ) { printf ( \"Hello World\\n\" ) ; return 0 ; } *hello.c* contains the source code of a C program. The compiled executable version of this code is in the file called *hi*, which you can run like so: 1 2 $ ./hi Hello World","title":"4.13) Change directories to ~/test and use the cat command to display the entire contents of the file hello.c"},{"location":"tutorials/unix/unix/#414-use-the-head-command-to-view-the-first-20-lines-of-the-file-sample_1fastq","text":"Hint Remember your *best* friend! Additional Hint Use *man* to find out what option you need to add to display a given number of *lines*. Answer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ head -20 sample_1.fastq @IRIS:7:1:17:394#0/1 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG +IRIS:7:1:17:394#0/1 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ @IRIS:7:1:17:800#0/1 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG +IRIS:7:1:17:800#0/1 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT @IRIS:7:1:17:1757#0/1 TTTTCTCGACGATTTCCACTCCTGGTCNACGAATCC +IRIS:7:1:17:1757#0/1 aaaaaa `` aaa ` aaaa_^a ``` ]][ Z [ DY^XYV^_Y @IRIS:7:1:17:1479#0/1 CATATTGTAGGGTGGATCTCGAAAGATATGAAAGAT +IRIS:7:1:17:1479#0/1 abaaaaa ` a ``` ^aaaaa ` _ ] aaa ` aaa__a_X ] `` @IRIS:7:1:17:150#0/1 TGATGTACTATGCATATGAACTTGTATGCAAAGTGG +IRIS:7:1:17:150#0/1 abaabaa ` aaaaaaa^ba_ ]] aaa^aaaaa_^ ][ aa","title":"4.14) Use the head command to view the first 20 lines of the file sample_1.fastq"},{"location":"tutorials/unix/unix/#415-use-the-tail-command-to-view-the-last-8-lines-of-the-file-sample_1fastq","text":"Hint It's very much like *head*. Answer 1 2 3 4 5 6 7 8 9 tail -8 sample_1.fastq @IRIS:7:32:731:717#0/1 TAATAATTGGAGCCAAATCATGAATCAAAGGACATA +IRIS:7:32:731:717#0/1 ababbababbab ] abbaa ` babaaabbb ` bbbabbb @IRIS:7:32:731:1228#0/1 CTGATGCCGAGGCACGCCGTTAGGCGCGTGCTGCAG +IRIS:7:32:731:1228#0/1 ` aaaaa `` aaa ` a `` a ` ^a ` a ` a_ [ a_a ` a ` aa ` __","title":"4.15) Use the tail command to view the last 8 lines of the file sample_1.fastq"},{"location":"tutorials/unix/unix/#416-use-the-grep-command-to-find-out-all-the-lines-in-mobytxt-that-contain-the-word-ahab","text":"Hint One might say we are 'looking for the *pattern* \"Ahab\"' Additional Hint 1 2 3 4 5 $ man grep ... SYNOPSIS grep [ OPTIONS ] PATTERN [ FILE... ] ... Answer 1 2 3 4 5 $ grep Ahab moby.txt \"Want to see what whaling is, eh? Have ye clapped eye on Captain Ahab?\" \"Who is Captain Ahab, sir?\" \"Aye, aye, I thought so. Captain Ahab is the Captain of this ship.\" ... AND MUCH MUCH MORE ... If you want to know how many lines are in the output of the above command you can \"pipe\" it into the *wc -l* command: 1 2 $ grep Ahab moby.txt | wc -l 491 which shows that there are *491* lines in *moby.txt* that contain the word Ahab.","title":"4.16) Use the grep command to find out all the lines in moby.txt that contain the word \"Ahab\""},{"location":"tutorials/unix/unix/#417-use-the-grep-command-to-find-out-all-the-lines-in-expectationstxt-that-contain-the-word-the-with-a-case-insensitive-search-it-should-count-the-the-the-the-etcetera","text":"Hint One might say we are *ignoring case*. Additional Hint 1 2 3 4 5 $ man grep ... -i, --ignore-case Ignore case distinctions in both the PATTERN and the input files. ( -i is specified by POSIX. ) ... Answer Use the *-i* flag to *grep* to make it perform case insensitive search: 1 2 3 4 5 6 $ grep -i the expectations.txt The Project Gutenberg EBook of Great Expectations, by Charles Dickens This eBook is for the use of anyone anywhere at no cost and with re-use it under the terms of the Project Gutenberg License included [ Project Gutenberg Editor ' s Note: There is also another version of ... AND MUCH MUCH MORE ... Again, \"pipe\" the output to *wc -l* to count the number of lines: 1 2 $ grep -i the expectations.txt | wc -l 8165","title":"4.17) Use the grep command to find out all the lines in expectations.txt that contain the word \"the\" with a case insensitive search (it should count \"the\" \"The\" \"THE\" \"tHe\" etcetera)."},{"location":"tutorials/unix/unix/#418-use-the-gzip-command-to-compress-the-file-sample_1fastq-use-gunzip-to-decompress-it-back-to-the-original-contents","text":"Hint Use the above commands along with *man* and *ls* to see what happens to the file. Answer Check the file size of sample_1.fastq before compressing it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # check filesize $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20 :03 sample_1.fastq # compress it (takes a few seconds) $ gzip sample_1.fastq # check filesize (Note: its name changed) $ ls -l sample_1.fastq.gz -rw-r--r-- 1 training01 training 26997595 Jun 14 20 :03 sample_1.fastq.gz # decompress it $ gunzip sample_1.fastq.gz $ ls -l sample_1.fastq -rw-r--r-- 1 training01 training 90849644 Jun 14 20 :03 sample_1.fastq You will see that when it was compressed it is *26997595* bytes in size, making it about *0.3* times the size of the original file. **Note**: in the above section the lines starting with *#* are comments so don't need to be copied but if you do then they wont do anything.","title":"4.18) Use the gzip command to compress the file sample_1.fastq. Use gunzip to decompress it back to the original contents."},{"location":"tutorials/unix/unix/#topic-5-pipes-output-redirection-and-shell-scripts","text":"In this section we will cover a lot of the more advanced Unix concepts; it is here where you will start to see the power of Unix. I say start because this is only the \u201ctip of the iceberg\u201d. Duration : 50 minutes. Relevant commands : wc , paste , grep , sort , uniq , nano , cut","title":"Topic 5: Pipes, output redirection and shell scripts"},{"location":"tutorials/unix/unix/#51-how-many-reads-are-contained-in-the-file-sample_1fastq","text":"Hint Examine some of the file to work out how many lines each *read* takes up. Additional Hint Count the number of lines Answer We can answer this question by counting the number of lines in the file and dividing by 4: 1 2 $ wc -l sample_1.fastq 3000000 **Answer**: There are *3000000* lines in the file representing *750000* reads. If you want to do simple arithmetic at the command line then you can use the \"basic calculator\" called *bc*: 1 2 $ echo \"3000000 / 4\" | bc 750000 Note : that the vertical bar character \"|\" is the Unix pipe (and is often called the \"pipe symbol\"). It is used for connecting the output of one command into the input of another command. We'll see more examples soon. *bc* is suitable for small calculations, but it becomes cumbersome for more complex examples. If you want to do more sophisticated calculations then we recommend to use a more general purpose programming language (such as Python etcetera).","title":"5.1) How many reads are contained in the file sample_1.fastq?"},{"location":"tutorials/unix/unix/#52-how-many-reads-in-sample_1fastq-contain-the-sequence-gattaca","text":"Hint Check out exercise 4.16 Answer Use *grep* to find all the lines that contain *GATTACA* and \"pipe\" the output to *wc -l* to count them: 1 2 $ grep GATTACA sample_1.fastq | wc -l 1119 **Answer**: *1119* If you are unsure about the possibility of upper and lower case characters then consider using the *-i* (ignore case option for grep).","title":"5.2) How many reads in sample_1.fastq contain the sequence GATTACA?"},{"location":"tutorials/unix/unix/#53-on-what-line-numbers-do-the-sequences-containing-gattaca-occur","text":"Hint We are looking for the *line numbers*. Additional Hint Check out the manpage for *grep* and/or *nl* Answer You can use the *-n* flag to grep to make it prefix each line with a line number: **Answer 1**: 1 2 3 4 5 $ grep -n GATTACA sample_1.fastq 5078 :AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 :AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 :ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Answer 2**: Or you can use the *nl* command to number each line of sample_1.fastq and then search for *GATTACA* in the numbered lines: 1 2 3 4 5 $ nl sample_1.fastq | grep GATTACA 5078 AGGAAGATTACAACTCCAAGACACCAAACAAATTCC 7170 AACTACAAAGGTCAGGATTACAAGCTCTTGCCCTTC 8238 ATAGTTTTTTCGATTACATGGATTATATCTGTTTGC ... AND MUCH MUCH MORE ... **Just the line numbers**: If you just want to see the line numbers then you can \"pipe\" the output of the above command into *cut -f 1*: 1 2 3 4 5 $ nl sample_1.fastq | grep GATTACA | cut -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ... *cut* will remove certain columns from the input; in this case it will remove all except column 1 (a.k.a. field 1, hence the *-f 1* option) 1 2 3 4 5 $ grep -n GATTACA sample_1.fastq | cut -d: -f 1 5078 7170 8238 ... AND MUCH MUCH MORE ...","title":"5.3) On what line numbers do the sequences containing GATTACA occur?"},{"location":"tutorials/unix/unix/#54-use-the-nl-command-to-print-each-line-of-sample_1fastq-with-its-corresponding-line-number-at-the-beginning","text":"Hint Check answer to 5.3. Answer 1 2 3 4 5 6 7 8 9 10 $ nl sample_1.fastq 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT ... AND MUCH MUCH MORE ... There are a lot of lines in that file so this command might take a while to print all its output. If you get tired of looking at the output you can kill the command with *control-c* (hold the *control* key down and simultaneously press the \"*c*\" character).","title":"5.4) Use the nl command to print each line of sample_1.fastq with its corresponding line number at the beginning."},{"location":"tutorials/unix/unix/#55-redirect-the-output-of-the-previous-command-to-a-file-called-sample_1fastqnl","text":"Check the first 20 lines of sample_1.fastq.nl with the head command. Use the less command to interactively view the contents of sample_1.fastq.nl (use the arrow keys to navigate up and down, q to quit and \u2018 / \u2018 to search). Use the search facility in less to find occurrences of GATTACA . Hint Ok that one was tough, *> FILENAME* is how you do it if you didn't break out an internet search for \"redirect the output in Unix\" Answer 1 $ nl sample_1.fastq > sample_1.fastq.nl The greater-than sign \"*>*\" is the file redirection operator. It causes the standard output of the command on the left-hand-side to be written to the file on the right-hand-side. You should notice that the above command is much faster than printing the output to the screen. This is because writing to disk can be performed much more quickly than rendering the output on a terminal. To check that the first 20 lines of the file look reasonable you can use the *head* command like so: 1 2 3 4 5 6 7 8 9 10 $ head -20 sample_1.fastq.nl 1 @IRIS:7:1:17:394#0/1 2 GTCAGGACAAGAAAGACAANTCCAATTNACATTATG 3 +IRIS:7:1:17:394#0/1 4 aaabaa ` ] baaaaa_aab ] D^^ ` b ` aYDW ] abaa ` ^ 5 @IRIS:7:1:17:800#0/1 6 GGAAACACTACTTAGGCTTATAAGATCNGGTTGCGG 7 +IRIS:7:1:17:800#0/1 8 ababbaaabaaaaa ` ] ` ba ` ] ` aaaaYD \\\\ _a `` XT ... The *less* command allows you to interactively view a file. The arrow keys move the page up and down. You can search using the '*/*' followed by the search term. You can quit by pressing \"*q*\". Note that the *less* command is used by default to display man pages. 1 $ less sample_1.fastq.nl","title":"5.5) Redirect the output of the previous command to a file called sample_1.fastq.nl."},{"location":"tutorials/unix/unix/#56-the-four-lines-per-read-format-of-fastq-is-cumbersome-to-deal-with-often-it-would-be-preferable-if-we-could-convert-it-to-tab-separated-value-tsv-format-such-that-each-read-appears-on-a-single-line-with-each-of-its-fields-separated-by-tabs-use-the-following-command-to-convert-sample_1fastq-into-tsv-format","text":"1 $ cat sample_1.fastq | paste - - - - > sample_1.tsv Answer The *'-'* (dash) character has a special meaning when used in place of a file; it means use the standard input instead of a real file. Note: while it is fairly common in most Unix programs, not all will support it. The *paste* command is useful for merging multiple files together line-by-line, such that the *Nth* line from each file is joined together into one line in the output, separated by default with a *tab* character. In the above example we give paste 4 copies of the contents of *sample_1.fastq*, which causes it to join consecutive groups of 4 lines from the file into one line of output.","title":"5.6) The four-lines-per-read format of FASTQ is cumbersome to deal with. Often it would be preferable if we could convert it to tab-separated-value (TSV) format, such that each read appears on a single line with each of its fields separated by tabs. Use the following command to convert sample_1.fastq into TSV format:"},{"location":"tutorials/unix/unix/#57-do-you-expect-the-output-of-the-following-command-to-produce-the-same-output-as-above-and-why","text":"1 $ paste sample_1.fastq sample_1.fastq sample_1.fastq sample_1.fastq > sample_1b.tsv Try it, see what ends up in sample_1b.tsv (maybe use less ) Hint Use *less* to examine it. Answer **Answer**: No, in the second instance we get 4 copies of each line. **Why**: In the first command *paste* will use the input file (standard input) 4 times since the *cat* command will only give one copy of the file to *paste*, where as, in the second command *paste* will open the file 4 times. Note: this is quite confusing and is not necessory to remember; its just an interesting side point.","title":"5.7) Do you expect the output of the following command to produce the same output as above? and why?"},{"location":"tutorials/unix/unix/#58-check-that-sample_1tsv-has-the-correct-number-of-lines-use-the-head-command-to-view-the-first-20-lines-of-the-file","text":"Hint Remember the *wc* command. Answer We can count the number of lines in *sample_1.tsv* using *wc*: 1 $ wc -l sample_1.tsv The output should be *750000* as expected (1/4 of the number of lines in sample_1.fastq). To view the first *20* lines of *sample_1.tsv* use the *head* command: 1 $ head -20 sample_1.tsv","title":"5.8) Check that sample_1.tsv has the correct number of lines. Use the head command to view the first 20 lines of the file."},{"location":"tutorials/unix/unix/#59-use-the-cut-command-to-print-out-the-second-column-of-sample_1tsv-redirect-the-output-to-a-file-called-sample_1dnatxt","text":"Hint See exercise 5.3 (for cut) and 5.5 (redirection) Answer The file sample_1.tsv is in column format. The cut command can be used to select certain columns from the file. The DNA sequences appear in column 2, we select that column using the -f 2 flag (the f stands for \"field\"). 1 cut -f 2 sample_1.tsv > sample_1.dna.txt Check that the output file looks reasonable using *head* or *less*.","title":"5.9) Use the cut command to print out the second column of sample_1.tsv. Redirect the output to a file called sample_1.dna.txt."},{"location":"tutorials/unix/unix/#510-use-the-sort-command-to-sort-the-lines-of-sample_1dnatxt-and-redirect-the-output-to-sample_1dnasortedtxt-use-head-to-look-at-the-first-few-lines-of-the-output-file-you-should-see-a-lot-of-repeated-sequences-of-as","text":"Hint Use *man* (sort) and see exercise 5.5 (redirection) Answer 1 $ sort sample_1.dna.txt > sample_1.dna.sorted.txt Running *head* on the output file reveals that there are duplicate DNA sequences in the input FASTQ file.","title":"5.10) Use the sort command to sort the lines of sample_1.dna.txt and redirect the output to sample_1.dna.sorted.txt. Use head to look at the first few lines of the output file. You should see a lot of repeated sequences of As."},{"location":"tutorials/unix/unix/#511-use-the-uniq-command-to-remove-duplicate-consecutive-lines-from-sample_1dnasortedtxt-redirect-the-result-to-sample_1dnauniqtxt-compare-the-number-of-lines-in-sample1_dnatxt-to-the-number-of-lines-in-sample_1dnauniqtxt","text":"Hint I am pretty sure you have already used *man* (or just guessed how to use *uniq*). You're also a gun at redirection now. Answer 1 $ uniq sample_1.dna.sorted.txt > sample_1.dna.uniq.txt Compare the outputs of: 1 2 3 4 $ wc -l sample_1.dna.sorted.txt 750000 $ wc -l sample_1.dna.uniq.txt 614490 View the contents of *sample_1.dna.uniq.txt* to check that the duplicate DNA sequences have been removed.","title":"5.11) Use the uniq command to remove duplicate consecutive lines from sample_1.dna.sorted.txt, redirect the result to sample_1.dna.uniq.txt. Compare the number of lines in sample1_dna.txt to the number of lines in sample_1.dna.uniq.txt."},{"location":"tutorials/unix/unix/#512-can-you-modify-the-command-from-above-to-produce-only-those-sequences-of-dna-which-were-duplicated-in-sample_1dnasortedtxt","text":"Hint Checkout the *uniq* manpage Additional Hint Look at the man page for uniq. Answer Use the *-d* flag to *uniq* to print out only the duplicated lines from the file: 1 $ uniq -d sample_1.dna.sorted.txt > sample_1.dna.dup.txt","title":"5.12) Can you modify the command from above to produce only those sequences of DNA which were duplicated in sample_1.dna.sorted.txt?"},{"location":"tutorials/unix/unix/#513-write-a-shell-pipeline-which-will-print-the-number-of-duplicated-dna-sequences-in-sample_1fastq","text":"Hint That is, *piping* most of the commands you used above instead of redirecting to file Additional Hint i.e. 6 commands (*cat*, *paste*, *cut*, *sort*, *uniq*, *wc*) Answer Finally we can 'pipe' all the pieces together into a sophisticated pipeline which starts with a FASTQ file and ends with a list of duplicated DNA sequences: **Answer**: 1 2 $ cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l 56079 The output file should have *56079* lines.","title":"5.13) Write a shell pipeline which will print the number of duplicated DNA sequences in sample_1.fastq."},{"location":"tutorials/unix/unix/#514-advanced-write-a-shell-script-which-will-print-the-number-of-duplicated-dna-sequences-in-sample_1fastq","text":"Hint Check out the *sleepy* file (with *cat* or *nano*); there is a bit of magic on the first line that you will need. You also need to tell bash that this file can be executed (check out *chmod* command). Answer Put the answer to *5.13* into a file called *sample_1_dups.sh* (or whatever you want). Use *nano* to create the file. **Answer**: the contents of the file will look like this: 1 2 3 #!/bin/bash cat sample_1.fastq | paste - - - - | cut -f 2 | sort | uniq -d | wc -l Note : the first line has special meaning. If it starts with ' #! ' (Hash then exclamation mark) then it tells bash this file is a script that can be interpreted. The command (including full path) used to intepret the script is placed right after the magic code. Give everyone execute permissions on the file with chmod: 1 $ chmod +x sample_1_dups.sh You can run the script like so: 1 $ ./sample_1_dups.sh If all goes well the script should behave in exactly the same way as the answer to 5.13.","title":"5.14) (Advanced) Write a shell script which will print the number of duplicated DNA sequences in sample_1.fastq."},{"location":"tutorials/unix/unix/#515-advanced-modify-your-shell-script-so-that-it-accepts-the-name-of-the-input-fastq-file-as-a-command-line-parameter","text":"Hint Shell scripts can refer to command line arguments by their position using special variables called *$0*, *$1*, *$2* and so on. Additional Hint *$0* refers to the name of the script as it was called on the command line. *$1* refers to the first command line argument, and so on. Answer Copy the shell script from *5.14* into a new file: 1 $ cp sample_1_dups.sh fastq_dups.sh Edit the new shell script file and change it to use the command line parameters: 1 2 3 #!/bin/bash cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l You can run the new script like so: 1 $ ./fastq_dups.sh sample_1.fastq In the above example the script takes *sample_1.fastq* as input and prints the number of duplicated sequences as output. **A better Answer**: Ideally we would write our shell script to be more robust. At the moment it just assumes there will be at least one command line argument. However, it would be better to check and produce an error message if insufficient arguments were given: 1 2 3 4 5 6 7 #!/bin/bash if [ $# -eq 1 ] ; then cat $1 | paste - - - - | cut -f 2 | sort | uniq -d | wc -l else echo \"Usage: $0 <fastq_filename>\" exit 1 fi The '*if ...; then*' line means: do the following line(s) ONLY if the *...* (called condition) bit is true. The '*else*' line means: otherwise do the following line(s) instead. Note: it is optional. The '*fi*' line means: this marks the end of the current *if* or *else* section. The '*[ $# -eq 1 ]*' part is the condition: * *$#*: is a special shell variable that indicates how many command line arguments were given. * *-eq*: checks if the numbers on either side of it are equal. * *1*: is a number one Spaces in conditions : Bash is VERY picky about the spaces within the conditions; if you get it wrong it will just behave strangely (without warning). You MUST put a space near the share brackets and between each part of the condition! So in words our script is saying \"if user provided 1 filename, then count the duplicates, otherwise print an error\". Exit-status : It is a Unix standard that when the user provides incorrect commandline arguments we print a usage message and return a *non-zero* exit status. The *exit status* is a standard way for other programs to know if our program ran correctly; 0 means everything went as expected, any other number is an error. If you don't provide an *exit ..* line then it automatically returns a 0 for you.","title":"5.15) (Advanced) Modify your shell script so that it accepts the name of the input FASTQ file as a command line parameter."},{"location":"tutorials/unix/unix/#516-advanced-modify-your-shell-script-so-that-it-accepts-zero-or-more-fastq-files-on-the-command-line-argument-and-outputs-the-number-of-duplicated-dna-sequences-in-each-file","text":"Answer We can add a loop to our script to accept multiple input FASTQ files: 1 2 3 4 5 #!/bin/bash for file in $@ ; do dups = $( cat $file | paste - - - - | cut -f 2 | sort | uniq -d | wc -l ) echo \" $file $dups \" done There's a lot going on in this script. The *$@* is a sequence of all command line arguments. The '*for ...; do*' (a.k.a. for loop) iterates over that sequence one argument at a time, assigning the current argument in the sequence to the variable called *file*. The *$(...)* allow us to capture the output of another command (in-place of the *...*). In this case we capture the output of the pipeline and save it to the variable called *dups*. If you had multiple FASTQ files available you could run the script like so: 1 ./fastq_dups.sh sample_1.fastq sample_2.fastq sample_3.fastq And it would produce output like: 1 2 3 sample_1.fastq 56079 sample_2.fastq XXXXX sample_3.fastq YYYYY","title":"5.16) (Advanced) Modify your shell script so that it accepts zero or more FASTQ files on the command line argument and outputs the number of duplicated DNA sequences in each file."},{"location":"tutorials/unix/unix/#finished","text":"Well done, you learnt a lot over the last 5 topics and you should be proud of your achievement; it was a lot to take in. From here you should be comfortable around the Unix command line and ready to take on the HPC Workshop. You will no-doubt forget a lot of what you learnt here so I encourage you to save a link to this workshop for later reference. Thank you for your attendance, please don\u2019t forget to complete the training survey and give it back to the workshop facilitators.","title":"Finished"},{"location":"tutorials/using_git/","text":"PR reviewers and advice: Juan Nunez-Iglesias Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/using_git/Using_Git/","text":"Using Git and Github for revision control \u00b6 What is Git? \u00b6 Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly. Getting help \u00b6 There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough. A simple workflow \u00b6 Step 1, create a github account. \u00b6 Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics . Step 2, sign into github and create a repository. \u00b6 Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning) Step 3, clone your repository to your local computer. \u00b6 Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). 1 2 3 4 5 $ git clone https://github.com/bjpop/test.git Cloning into 'test' ... remote: Counting objects: 3 , done . remote: Total 3 ( delta 0 ) , reused 0 ( delta 0 ) Unpacking objects: 100 % ( 3 /3 ) , done . This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README . md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). 1 2 3 4 5 $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs Step 4, commit a file to the repository. \u00b6 Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: 1 $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: 1 2 $ python hello.py hello world Check the status of your repository: 1 2 3 4 5 6 7 $ git status # On branch master # Untracked files : # ( use \" git add <file>... \" to include in what will be committed ) # # hello . py nothing added to commit but untracked files present ( use \" git add \" to track ) Notice that git tells you that the new file hello . py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: 1 $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \u201cstage\u201d your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: 1 2 3 4 5 6 7 $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello . py have been staged and are ready to be committed. Notice that hello . py is no longer untracked. Commit your changes with a commit message: 1 2 3 4 $ git commit - m \"A little greeting program\" [ master b1cce11 ] A little greeting program 1 files changed , 1 insertions ( + ), 0 deletions ( - ) create mode 100644 hello . py Re-check the status of your repository: 1 2 3 4 5 $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit ( working directory clean ) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin / master ). Step 5, push your changes to github. \u00b6 Push the commit in your local repository to github (thus synchronising them). 1 2 3 4 5 6 7 8 9 10 $ git push origin Username for ' https://github.com ' : < type your github username > Password for ' https://<your github username>@github.com ' : Counting objects : 4 , done . Delta compression using up to 16 threads . Compressing objects : 100 % ( 2 / 2 ) , done . Writing objects : 100 % ( 3 / 3 ) , 305 bytes , done . Total 3 ( delta 0 ) , reused 0 ( delta 0 ) To https : // github . com / bjpop / test . git 71 a771a .. b1cce11 master -> master Now if you look at your repository on github you should see the file hello . py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name: Step 6, create a branch in your local repository. \u00b6 You can ask git to tell you about the names of the current branches: 1 2 $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). 1 2 3 4 $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: 1 2 3 4 5 $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello . py file: 1 $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): 1 2 3 4 5 6 7 8 9 10 $ git status # On branch documentation # Changes not staged for commit : # ( use \" git add <file>... \" to update what will be committed ) # ( use \" git checkout -- <file>... \" to discard changes in working directory ) # # modified : hello . py # no changes added to commit ( use \" git add \" and / or \" git commit -a \" ) Stage the new changes and commit them, and check the status again: 1 2 3 4 5 6 7 $ git add hello.py $ git commit -m \"Added a comment\" [ documentation 9bbe430 ] Added a comment 1 files changed, 1 insertions ( + ) , 0 deletions ( - ) $ git status # On branch documentation nothing to commit ( working directory clean ) Now we can push the new \u201cdocumentation\u201d branch to github: 1 2 3 4 5 6 7 8 9 10 $ git push origin documentation Username for ' https : //github.com': <your github username> Password for ' https : //<your github username>@github.com': Counting objects : 5 , done . Delta compression using up to 16 threads . Compressing objects : 100 % ( 2 / 2 ), done . Writing objects : 100 % ( 3 / 3 ), 314 bytes , done . Total 3 ( delta 0 ), reused 0 ( delta 0 ) To https : //github.com/bjpop/test.git * [ new branch ] documentation -> documentation On github you should be able to see the new branch: Step 7, merge the changes back into the master \u00b6 branch. To go back to the master branch you must check it out: 1 2 $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: 1 2 $ cat hello.py print ( \"hello world\" ) Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: 1 2 3 4 5 $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions ( + ) , 0 deletions ( - ) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: 1 2 3 $ cat hello.py print ( \"hello world\" ) #this is a comment Check the status of the master branch: 1 2 3 4 5 $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit ( working directory clean ) Push the changes in the master branch back to github: 1 2 3 4 5 6 $ git push origin master Username for ' https://github.com ' : bjpop Password for ' https://bjpop@github.com ' : Total 0 ( delta 0 ) , reused 0 ( delta 0 ) To https : // github . com / bjpop / test . git b1cce11 .. 9 bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: 1 2 3 4 5 6 7 8 9 10 11 12 $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12 :30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12 :08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17 :29:02 2013 -0700","title":"Introduction to Git and Github"},{"location":"tutorials/using_git/Using_Git/#using-git-and-github-for-revision-control","text":"","title":"Using Git and Github for revision control"},{"location":"tutorials/using_git/Using_Git/#what-is-git","text":"Git is a revision control system. It is designed to help you keep track of collections of files which stem from a common source and undergo modifications over time. The files tend to be human generated text. It is very good at managing source code repositories, but it can also be used to manage other things, such as configuration files and text documents. It is not, however, a file backup system. Git encourages a distributed style of project development. Each contributor to a project has their own complete repository. Changes are shared between repositories by pushing changes to, or pulling changes from other repositories. Collaboration between developers is greatly enhanced by websites such as github , bitbucket and gitorious which provide convenient interfaces to managing multiple repositories. There are many alternatives to git which each have their pros and cons. Two of the more popular alternatives are: Subversion is particularly suited to a centralised model of development. Mercurial is very similar to Git, but is sometimes considered more user friendly.","title":"What is Git?"},{"location":"tutorials/using_git/Using_Git/#getting-help","text":"There are lots of resources on the web for learning how to use Git. A popular reference is Pro Git , which is freely available online ( http://git-scm.com/book ). Another good reference is the book Version Control with Git , by Loeliger and McCullough.","title":"Getting help"},{"location":"tutorials/using_git/Using_Git/#a-simple-workflow","text":"","title":"A simple workflow"},{"location":"tutorials/using_git/Using_Git/#step-1-create-a-github-account","text":"Create a github account ( https://github.com/ ). Do this step once only (unless you need multiple accounts). You get unlimited numbers of (world readable) public repositories for free. Private repositories (that can be shared with selected users) cost money (see https://github.com/plans ), but discounts are available for academics .","title":"Step 1, create a github account."},{"location":"tutorials/using_git/Using_Git/#step-2-sign-into-github-and-create-a-repository","text":"Sign in to your github account and create a new repository. Do this once for every new project you have. You will need to provide some information: the repository name a description of the repository choose whether it is public (free) or private (costs money) whether to initialise with a dummy README file (it is useful) whether to provide an initial .gitignore file (probably leave this > out in the beginning)","title":"Step 2, sign into github and create a repository."},{"location":"tutorials/using_git/Using_Git/#step-3-clone-your-repository-to-your-local-computer","text":"Clone your new repository from github to your local computer. Each repository on github is identified by a URL, which will look like the one below: Run the command below on your development machine in the directory where you want to keep the repository (of course you should use the actual URL of your own repository, not the one in the example). 1 2 3 4 5 $ git clone https://github.com/bjpop/test.git Cloning into 'test' ... remote: Counting objects: 3 , done . remote: Total 3 ( delta 0 ) , reused 0 ( delta 0 ) Unpacking objects: 100 % ( 3 /3 ) , done . This will create a directory with the same name as your repository (in this example it is called test ). If you change into that directory and list its contents you will see a .git subdirectory, which is where Git keeps all the data for your repository. You will also see working copies of the files in the project. In this example the only such file is README . md which was created automatically by github when the repository was first created. (The .md extension on the file suggests that it uses the Markdown syntax, see https://help.github.com/articles/github-flavored-markdown ). 1 2 3 4 5 $ cd test $ ls -a . .. .git README.md $ ls .git branches config description HEAD hooks index info logs objects packed-refs refs","title":"Step 3, clone your repository to your local computer."},{"location":"tutorials/using_git/Using_Git/#step-4-commit-a-file-to-the-repository","text":"Create a new file in the repository on your local computer and commit it to your local repository. How you create the file is immaterial. You could copy it from somewhere else, create it in a text editor. In this case we\u2019ll make a little python program: 1 $ echo 'print(\"hello world\")' > hello.py Test that your new file is satisfactory, in this case we test our code: 1 2 $ python hello.py hello world Check the status of your repository: 1 2 3 4 5 6 7 $ git status # On branch master # Untracked files : # ( use \" git add <file>... \" to include in what will be committed ) # # hello . py nothing added to commit but untracked files present ( use \" git add \" to track ) Notice that git tells you that the new file hello . py is not tracked (not in the repository). When you are happy with your file, you can stage it (this is not a commit), but it will cause the file to be tracked: 1 $ git add hello.py Note that git uses a two-stage process for committing changes. The first stage is to \u201cstage\u201d your changes. Staged changes appear in the repository index, but are not committed. You can stage many changes together, and even amend or undo previously staged (but not committed) changes. The second stage is to commit the current staged changes to the repository. Committing causes the changes to be reflected in the state of the repository. Re-check the status of your repository: 1 2 3 4 5 6 7 $ git status # On branch master # Changes to be committed: # (use \"git reset HEAD <file>...\" to unstage) # # new file: hello.py # Now we can see that the changes to hello . py have been staged and are ready to be committed. Notice that hello . py is no longer untracked. Commit your changes with a commit message: 1 2 3 4 $ git commit - m \"A little greeting program\" [ master b1cce11 ] A little greeting program 1 files changed , 1 insertions ( + ), 0 deletions ( - ) create mode 100644 hello . py Re-check the status of your repository: 1 2 3 4 5 $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit ( working directory clean ) Now we see that there a no uncommitted changes in the repository, however git tells us that our local repository is one commit ahead of the github version (which it calls origin / master ).","title":"Step 4, commit a file to the repository."},{"location":"tutorials/using_git/Using_Git/#step-5-push-your-changes-to-github","text":"Push the commit in your local repository to github (thus synchronising them). 1 2 3 4 5 6 7 8 9 10 $ git push origin Username for ' https://github.com ' : < type your github username > Password for ' https://<your github username>@github.com ' : Counting objects : 4 , done . Delta compression using up to 16 threads . Compressing objects : 100 % ( 2 / 2 ) , done . Writing objects : 100 % ( 3 / 3 ) , 305 bytes , done . Total 3 ( delta 0 ) , reused 0 ( delta 0 ) To https : // github . com / bjpop / test . git 71 a771a .. b1cce11 master -> master Now if you look at your repository on github you should see the file hello . py has been uploaded, along with its commit time and commit message. You can inspect the contents of the file on github by clicking on its name:","title":"Step 5, push your changes to github."},{"location":"tutorials/using_git/Using_Git/#step-6-create-a-branch-in-your-local-repository","text":"You can ask git to tell you about the names of the current branches: 1 2 $ git branch * master By default your repository starts with a branch called master. The asterisk next to the branch name tells you which is the current branch (at the moment there is only one branch). 1 2 3 4 $ git branch documentation $ git branch documentation * master The first command above creates a new branch called documentation . The second command shows us that the new branch has been created, but the current branch is still master . To switch to another branch you must check it out: 1 2 3 4 5 $ git checkout documentation Switched to branch 'documentation' $ git branch * documentation master Let\u2019s add a change to our existing hello . py file: 1 $ echo '#this is a comment' >> hello.py Check the status of the repository (now in the documentation branch): 1 2 3 4 5 6 7 8 9 10 $ git status # On branch documentation # Changes not staged for commit : # ( use \" git add <file>... \" to update what will be committed ) # ( use \" git checkout -- <file>... \" to discard changes in working directory ) # # modified : hello . py # no changes added to commit ( use \" git add \" and / or \" git commit -a \" ) Stage the new changes and commit them, and check the status again: 1 2 3 4 5 6 7 $ git add hello.py $ git commit -m \"Added a comment\" [ documentation 9bbe430 ] Added a comment 1 files changed, 1 insertions ( + ) , 0 deletions ( - ) $ git status # On branch documentation nothing to commit ( working directory clean ) Now we can push the new \u201cdocumentation\u201d branch to github: 1 2 3 4 5 6 7 8 9 10 $ git push origin documentation Username for ' https : //github.com': <your github username> Password for ' https : //<your github username>@github.com': Counting objects : 5 , done . Delta compression using up to 16 threads . Compressing objects : 100 % ( 2 / 2 ), done . Writing objects : 100 % ( 3 / 3 ), 314 bytes , done . Total 3 ( delta 0 ), reused 0 ( delta 0 ) To https : //github.com/bjpop/test.git * [ new branch ] documentation -> documentation On github you should be able to see the new branch:","title":"Step 6, create a branch in your local repository."},{"location":"tutorials/using_git/Using_Git/#step-7-merge-the-changes-back-into-the-master","text":"branch. To go back to the master branch you must check it out: 1 2 $ git checkout master Switched to branch 'master' You can confirm that the master branch does not yet have the changes made in the documentation branch: 1 2 $ cat hello.py print ( \"hello world\" ) Notice that the comment is missing. You can pull the changes in the documentation branch back into the master branch with the merge command: 1 2 3 4 5 $ git merge documentation Updating b1cce11..9bbe430 Fast-forward hello.py | 1 + 1 files changed, 1 insertions ( + ) , 0 deletions ( - ) In this case the merge was easy because there were no conflicts between master and documentation. In this case git automatically updates the tracked files in the current branch. We can test that the changes have taken place by looking at the contents of hello.py: 1 2 3 $ cat hello.py print ( \"hello world\" ) #this is a comment Check the status of the master branch: 1 2 3 4 5 $ git status # On branch master # Your branch is ahead of 'origin/master' by 1 commit. # nothing to commit ( working directory clean ) Push the changes in the master branch back to github: 1 2 3 4 5 6 $ git push origin master Username for ' https://github.com ' : bjpop Password for ' https://bjpop@github.com ' : Total 0 ( delta 0 ) , reused 0 ( delta 0 ) To https : // github . com / bjpop / test . git b1cce11 .. 9 bbe430 master -> master Again you can verify on github that the changes have taken place. To get an idea of the history of a project you can ask for a log of the commit messages: 1 2 3 4 5 6 7 8 9 10 11 12 $ git log commit 9bbe430f6e8b70187927b4a70a8402f71b17b426 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12 :30:39 2013 +1100 Added a comment commit b1cce115fb40a9b11917db7eb73c8295e276bb09 Author: Bernie <florbitous@gmail.com> Date: Fri Mar 15 12 :08:01 2013 +1100 A little greeting program commit 71a771a86b8116c3f93c99db5416bfa371a6f772 Author: Bernie Pope <florbitous@gmail.com> Date: Thu Mar 14 17 :29:02 2013 -0700","title":"Step 7, merge the changes back into the master"},{"location":"tutorials/var_detect_advanced/","text":"PR reviewers and advice: Clare Sloggett, Khalid Mahmood, Simon Gladman, Jessica Chung Current slides: TBD Other slides: None yet","title":"Home"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/","text":"Variant Detection - Advanced Workshop \u00b6 Tutorial Overview \u00b6 In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information Background \u00b6 Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/ Preparation \u00b6 Make sure you have an instance of Galaxy ready to go. If you don\u2019t have your own - go to our Galaxy-Tut or Galaxy Australia server. Log in so that your work will be saved. If you don\u2019t already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can\u2019t be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won\u2019t need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf . Section 1: Quality Control \u00b6 The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30). Section 2: Alignment and depth of coverage \u00b6 In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \u201cTutorial_readgroup\u201d. This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn\u2019t matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \u201cNA12878\u201d Set the platform to ILLUMINA Set the library name to \u201cTutorial_library\u201d. Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it\u2019s still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download . If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene\u2026) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It\u2019s not practical to go through this by hand, but you\u2019ll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it. Section 3. Local realignment \u00b6 Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data\u2026 (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant. Section 4. Calling variants with FreeBayes \u00b6 FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \u201cSimple diploid calling with filtering and coverage\u201d. This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset\u2019s display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Section 5. Calling variants with GATK Unified Genotyper \u00b6 For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378. Section 6. Evaluate variants \u00b6 How can we evaluate our variants? We\u2019ve called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data\u2026 (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \u201c^#\u201d. ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \u201c#\u201d character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \u201cFreeBayes vs UnifiedGenotyper\u201d. As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \u201cFreeBayes\u201d. As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \u201cUnifiedGenotyper\u201d. Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \u201cFreeBayes UnifiedGenotyper\u201d indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \u201cFreeBayes \u2229 UnifiedGenotyper\u201d indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We\u2019d expect this to remove many false positives, but also a few true positives. Section 7. Annotation \u00b6 The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated \u2026 and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \u201chg19\u201d. This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF\u2026 header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \u201cintergenic region\u201d. Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \u201cmissense_variant\u201d. Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you\u2019d expect?","title":"Variant Calling part 2 (Galaxy)"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#variant-detection-advanced-workshop","text":"","title":"Variant Detection - Advanced Workshop"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#tutorial-overview","text":"In this tutorial, we will look further at variant calling from sequence data. We will: Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop Carry out local realignment on our aligned reads Compare the performance of different variant calling tools Annotate our called variants with reference information","title":"Tutorial Overview"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#background","text":"Some background reading and reference material can be found here . The slides used in this workshop can be found here . Where is the data in this tutorial from? The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#preparation","text":"Make sure you have an instance of Galaxy ready to go. If you don\u2019t have your own - go to our Galaxy-Tut or Galaxy Australia server. Log in so that your work will be saved. If you don\u2019t already have an account on this server, select from the menu User -> Register and create one. Import data for the tutorial. We will import a pair of FASTQ files containing paired-end reads, and a VCF file of known human variants to use for variant evaluation. Method 1: Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Get the FASTQ files: click Paste/Fetch data and enter these URLs into the text box. If you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 Select Type as fastqsanger and click Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can\u2019t be sure which is which. Get the VCF file: click Paste/Fetch data again to open a new text box, and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf This time, you can leave the Type on Auto-detect. Click Start . Once the upload status for both sets of files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Method 2: Upload local data to Galaxy. (In most cases, you won\u2019t need this for the tutorial) Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial. In the Galaxy tools panel (left), under BASIC TOOLS , click on Get Data and choose Upload File . Click Choose local file and select the downloaded FASTQ files. Select Type as fastqsanger and click Start . Click Choose local file again and select the downloaded VCF file. Click Start . Once the upload status for all files turns green , you can click Close . You should now be able to see all three files in the Galaxy history panel (right). Rename the datasets You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names. Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click Save . Similarly, rename the second dataset to NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 . Similarly, rename the third dataset to dbSNP135_excludingsitesafter129.chr20.vcf .","title":"Preparation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-1-quality-control","text":"The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome! Analyse the quality of the reads in the FASTQ file. From the left hand tool panel in Galaxy, under NGS ANALYSIS , select NGS: QC and manipulation -> FastQC Select one of the FASTQ files as input and Execute the tool. When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics. Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).","title":"Section 1: Quality Control"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-2-alignment-and-depth-of-coverage","text":"In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome. Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects. We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment. Map/align the reads with Bowtie2 to the human reference genome. We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under NGS ANALYSIS in the tools panel, select the tool NGS: Mapping -> Bowtie2 . We have paired-end reads in two FASTQ files, so select paired-end . Select the two FASTQ files as inputs. Under Select reference genome select the human genome hg19 . Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under Set read groups information? select Set read groups (SAM/BAM specification) . (Picard-style should also work.) Set the read group identifier to \u201cTutorial_readgroup\u201d. This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn\u2019t matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier. Set the sample name to \u201cNA12878\u201d Set the platform to ILLUMINA Set the library name to \u201cTutorial_library\u201d. Normally we would set this to identify the DNA library from our DNA extraction. You can leave other read group information blank, and use default Bowtie2 settings. Execute the tool. When the alignment has finished, you should rename the BAM file to something more convenient, such as NA12878.chr20_2mb.30xPE.bam . Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents. Visualise the aligned BAM file with IGV. The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server). Note: if you are already familiar with IGV, you may want to go through this section quickly, but it\u2019s still a good idea to launch IGV for use in later steps. In the green dataset box for your BAM file in the history panel, you will see some display with IGV links. Launch IGV by clicking the web current link. If IGV is already running on your computer, instead click the local link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download . If your BAM file was not automatically loaded, download and open it: Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory. In IGV, select the correct reference genome, hg19 , in the top-left drop-down menu. In IGV, open the BAM file using File -> Load from File . Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box). Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome. Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene\u2026) Restrict the genomic region considered. Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region. Under BASIC TOOLS , select the tool Text manipulation -> Create single interval . Enter these values: Chromosome: chr20 Start position: 0 End position: 2000000 Name: chr20_2mb Strand: plus Execute this tool. This will create a small BED file specifying just one genomic region. When the file is created, rename it to chr20_2mb.bed . Have a look at the contents of this BED file. Evaluate the depth of coverage of the aligned region. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Depth of Coverage . Select the BAM file you just generated as the input BAM file. Make sure the reference genome we aligned to is selected under Using reference genome . Set Output format to table . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of: \u2018Depth of Coverage on data.... (output summary sample)\u2019: this file will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution. Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling. Also have a quick look at the (per locus coverage) file. It\u2019s not practical to go through this by hand, but you\u2019ll see that it gives coverage statistics for every site in the genome. The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.","title":"Section 2: Alignment and depth of coverage"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-3-local-realignment","text":"Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome. This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels. Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels. If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2. However performing local realignment will improve the accuracy of our variant calls. Generate the list of indel regions to be realigned. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels. Select your BAM file as input. Select the correct reference genome (the genome used for alignment). Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step. Realign the subsets of reads around the target indel areas. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file. Select your BAM file as input. Select the correct reference genome. Under Restrict realignment to provided intervals , select the intervals file generated in the previous step. Execute . Rename the file to something easier to recognise, e.g. NA12878.chr20_2mb.30xPE.realigned.bam To keep your history clean, you may want to delete the log files generated by GATK in the last two steps. Compare the realigned BAM file to original BAM around an indel. Open the realigned BAM file in IGV. You can use the local link to open it in your already-running IGV session and compare it to the pre-realignment BAM file. Generally, the new BAM should appear identical to the old BAM except in the realigned regions. Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data\u2026 (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions). If you can\u2019t find anything obvious, check region chr20:1163914-1163954; you should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.","title":"Section 3. Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-4-calling-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome. You can read more about FreeBayes here . Call variants with FreeBayes. Under NGS ANALYSIS , select the tool NGS: Variant Analysis -> FreeBayes . Select your realigned BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \u201cSimple diploid calling with filtering and coverage\u201d. This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. When it has run, rename the resulting VCF file to something shorter, such as NA12878.FreeBayes.chr20_2mb.vcf . Check the generated list of variants . Click the eye icon to examine the VCF file contents. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? Open the VCF file in IGV using the dataset\u2019s display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.","title":"Section 4. Calling variants with FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-5-calling-variants-with-gatk-unified-genotyper","text":"For comparison, we will call variants with a second variant caller. The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step. The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants. You can read more about the GATK here . Call variants using Unified Genotyper. Under NGS COMMON TOOLSETS , select the tool NGS: GATK Tools 2.8 -> Unified Genotyper . Select your realigned BAM file as input, and select the correct reference genome. UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under dbSNP ROD file , select the dataset dbsnp135_excludingsitesafter129_chr20.vcf . Set Genotype likelihoods calculation model to employ to SNP . Restrict the analysis to only the region of interest: Set Basic or Advanced GATK options to Advanced . Click Insert Operate on Genomic intervals to add a new region and select the chr20_2mb.bed file from your history. Execute . Rename the file to something useful eg NA12878.GATK.chr20_2mb.vcf . The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files. Check the generated list of variants. Roughly how many variants are there in your VCF file (how many lines in the dataset?) Click the eye icon to examine the contents of the VCF file. Notice that the ID column (the third column) is populated with known SNP IDs from the VCF file we provided. Notice that the VCF header rows, and the corresponding information in the INFO column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the INFO column, so long as it adds header rows to describe the fields it uses. Open the VCF file in IGV, using the link in the history panel. Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.","title":"Section 5. Calling variants with GATK Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-6-evaluate-variants","text":"How can we evaluate our variants? We\u2019ve called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample: We expect to see true variations at the rate of about 1 per 1000bp against the reference genome 85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP) A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions. You can read more about SNP call set properties here . You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs). We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on? Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool. Under NGS COMMON TOOLSETS , select NGS: GATK Tools 2.8 -> Eval Variants . Under Input variant file , select the VCF file you generated with FreeBayes. Click Insert Variant to add a second input file, and under Input variant file , select the VCF file you generated with UnifiedGenotyper. Set the reference genome to hg19 . Provide our list of known variants: make sure Provide a dbSNP Reference-Ordered Data (ROD) file is set to Set dbSNP and under dbSNP ROD file select the input reference variant file, dbSNP135_excludingsitesafter129.chr20.vcf . We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set Basic or Advanced Analysis options to Advanced . Then set the following: Eval modules to apply on the eval track(s) : CompOverlap TiTvVariantEvaluator Do not use the standard eval modules by default : check this option Execute . Interpret the dbSNP concordance section of the evaluation report. Examine the contents of the \u2018Eval Variants on data\u2026 (report)\u2019 The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP. The EvalRod column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively. The CompRod column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp. Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing all variants are the most informative summaries. nEvalVariants : number of variant sites in EvalRod (i.e. all called variants). novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP). CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP). compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants). This metric is important, and it\u2019s what people generally refer to as dbSNP concordance . nConcordant and concordantRate : number and percentage of overlapping variants that have the same genotype. Interpret the TiTv section of the evaluation report. The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets. It generally follows the table format above. The most interesting metric is in the column labelled tiTvRatio . The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation. In this table, it can be useful to compare not just the rows labelled all , but also those labelled known and novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual? How much overlap is there in the call sets? One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates. Remove the header lines from a VCF file: select the tool BASIC TOOLS -> Filter and Sort ->Select . As an input file, in Select lines from , select the VCF file you generated using FreeBayes. Select NOT Matching . As the pattern, enter \u201c^#\u201d. ^ in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \u201c#\u201d character at the start of the line. Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have! Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper. Create a Venn diagram: select the tool STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn . Enter any title you like, e.g. \u201cFreeBayes vs UnifiedGenotyper\u201d. As Input file 1 , select the first of the filtered files you just generated. As Column index , enter 1. This is the second column, i.e. the column containing the position coordinate. Under as name , enter a name for this input file, e.g. just \u201cFreeBayes\u201d. As Input file 2 , select the second of the filtered VCF files. Again as Column index , enter 1. Under as name , enter a name for this input file, e.g. just \u201cUnifiedGenotyper\u201d. Execute . You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram: \u201cFreeBayes UnifiedGenotyper\u201d indicates the difference, i.e. how many variants were called by FreeBayes and not UnifiedGenotyper. \u201cFreeBayes \u2229 UnifiedGenotyper\u201d indicates the intersection, i.e. how many variants were called by both variant callers. You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We\u2019d expect this to remove many false positives, but also a few true positives.","title":"Section 6. Evaluate variants"},{"location":"tutorials/var_detect_advanced/var_detect_advanced/#section-7-annotation","text":"The variants we have detected can be annotated with information from known reference data. This can include, for instance whether the variant corresponds to a previously-observed variant in other samples, or across the population whether the variant is inside or near a known gene whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc) whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated \u2026 and lots of other information! Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant. For this workshop we will annotate our variants with the SnpEff tool, which has its own prebuilt annotation databases. Annotate the detected variants. Select the tool NGS ANALYSIS -> NGS: Annotation -> SnpEff . Choose any of your generated lists of variants as the input VCF file in the first field. Select VCF as the output format as well. This will add the annotated information to the INFO column of the VCF file. As Genome source , select Named on demand . Then as the genome, enter \u201chg19\u201d. This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed. Make sure Produce Summary Stats is set to Yes . Execute SnpEff. Examine the annotated information. Click the eye icon on the resulting VCF file to view it. You should see more information in the INFO column for each variant. You should also see a few extra VCF header rows. In particular notice the new INFO=<ID=EFF\u2026 header row, listing the information added on the predicted effects of each variant. Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short EFF field listing the variant as being in an \u201cintergenic region\u201d. Variants in known functional regions of the genome will be annotated with much more information. Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool BASIC TOOLS -> Filter and Sort -> Select on your annotated VCF file and filtering for lines matching the string \u201cmissense_variant\u201d. Examine the annotation summary stats. The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file. You will see summaries of types of variants, impact of variants, functional regions etc. Go through these tables and graphs and see if you understand what they represent. Where are most of the variants found (in exons, introns etc)? Does this match what you\u2019d expect?","title":"Section 7. Annotation"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/","text":"Introduction to Variant detection \u00b6 Background \u00b6 A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV) Variant Calling vs genotyping \u00b6 Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants. Process of variant calling \u00b6 Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position. Homozygous or Heterozygous mutations: \u00b6 What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events. Variant Calling Software: \u00b6 There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods Variant Calling using Samtools (Mpileup + bcftools) \u00b6 Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information Variant Calling using GATK-Unified Genotyper \u00b6 GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: 1 P ( Genotype | Data ) = ( P ( Data | Genotype ) * P ( Genotype )) / P ( Data ) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information Variant Calling using FreeBayes \u00b6 FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information Evaluation of detected variants using Variant Eval \u00b6 The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file. Quality Matrix: \u00b6 1 2 3 TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance. Local realignment \u00b6 In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml The Galaxy workflow platform \u00b6 Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right. Data Format used in the tutorial \u00b6 Sequence Alignment Map format \u00b6 SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example 1 2 3 SRR017937 .312 16 chr20 43108717 37 76 M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ? ,@ A = A \\ < 5 = ,@ == A : BB @ = B9 (.; A @ B ; \\ > @ ABBB @ @9 BB @ : @5 \\ < BBBB9 ) \\ > BBB2 \\ < BBB @ BBB ? ;; BABBBBBBB @ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that \u2018aligns\u2019 can mean \u2018aligns with mismatches\u2019 - mismatches that don\u2019t affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \u201csense\u201d strand of the reference, and bases are listed in 5\u2019 -> 3\u2019 order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy Binary Sequence Alignment Map format \u00b6 BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can\u2019t be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV VCF file format \u00b6 What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one \u2018#\u2018, showing the name of each field, and then the sample names starting at the 10 th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. VCF format in Galaxy: \u00b6 Bcf file format: \u00b6 \u00b6 BCF format: BCF, or the binary variant call format, is the binary version of VCF. It keeps the same information in VCF, while much more efficient to process especially for many samples. The relationship between BCF and VCF is similar to that between BAM and SAM.","title":"Introduction to Variant detection"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#introduction-to-variant-detection","text":"","title":"Introduction to Variant detection"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#background","text":"A variant is something that is different from a standard or type. The aim of variation detection is to detect how many bases out of the total are different to a reference genome. In Craig Venter\u2019s genome 4.1 million DNA variants were reported. What sort of variation could we find in the DNA sequencing? Single nucleotide variations (SNVs) Single nucleotide polymorphisms (SNPs) Small insertions and deletions (INDELs) Large Chromosome rearrangements-Structural variations (SV) Copy number variations (CNV)","title":"Background"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-vs-genotyping","text":"Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms. Variant callers estimate the probability of a particular genotype given the observed data. The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position. The result of variant calling is a list of probable variants.","title":"Variant Calling vs genotyping"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#process-of-variant-calling","text":"Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list The number of reads that stack up on each other is called read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.","title":"Process of variant calling"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#homozygous-or-heterozygous-mutations","text":"What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.","title":"Homozygous or Heterozygous mutations:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-software","text":"There a number of software available for variant calling some of which are as follows: SAMtools (mpileup and bcftools): Li 2009 Bioinformatics GATK: McKenna et al. 2010 Genome Res FreeBayes: MIT DiBayes: SOLiD software http://www.lifetechnologies.com InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. Bioinformatics MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. Nature Methods","title":"Variant Calling Software:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-samtools-mpileup-bcftools","text":"Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods. Mpileup: Input: BAM file Output: Pileuped up reads under the reference bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes Further information","title":"Variant Calling using Samtools (Mpileup + bcftools)"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-gatk-unified-genotyper","text":"GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form. GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods. Inputs: BAM file Output: VCF file with sites and genotypes. The probability of a variant genotype for a given sequence of data is calculated using the Bayes Theorem as follows: 1 P ( Genotype | Data ) = ( P ( Data | Genotype ) * P ( Genotype )) / P ( Data ) P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype. P(Data | Genotype) is the probability of the data (the reads) given the genotype P(Data) is the probability of seeing the reads. GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well. Further information","title":"Variant Calling using GATK-Unified Genotyper"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-freebayes","text":"FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files). Further information","title":"Variant Calling using FreeBayes"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#evaluation-of-detected-variants-using-variant-eval","text":"The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set. The results will have: True Positives (TP): The variants called by the software which are also a known variant in the known variants file. False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file. True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file. False Negatives (FN): The variants not called by the software which are known as variants in the known variants file.","title":"Evaluation of detected variants using Variant Eval"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#quality-matrix","text":"1 2 3 TP | FP ---|---- TN | FN Sensitivity: TP/(TP+FN) Specificity: TN/(TN+FP) Note: Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity. DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping. Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions. Transversion: a substitute of a purine for a pyrimidine. Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations. For more details on variant eval visit: http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html Notes: An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.","title":"Quality Matrix:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#local-realignment","text":"In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: http://samtools.sourceforge.net/mpileup.shtml","title":"Local realignment"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#the-galaxy-workflow-platform","text":"Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results. Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline. Galaxy uses the concept of \u2018histories\u2019. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account Learn more about Galaxy here The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.","title":"The Galaxy workflow platform"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#data-format-used-in-the-tutorial","text":"","title":"Data Format used in the tutorial"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#sequence-alignment-map-format","text":"SAM format Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format: 11 mandatory fields (+ variable number of optional fields) 1 QNAME: Query name of the read 2 FLAG 3 RNAME: Reference sequence name 4 POS: Position of alignment in reference sequence 5 MAPQ: Mapping quality (Phred-scaled) 6 CIGAR: String that describes the specifics of the alignment against the reference 7 MRNM 8 MPOS 9 ISIZE 10 SEQQuery: Sequence on the same strand as the reference 11 QUAL: Query quality (ASCII-33=Phred base quality) SAM example 1 2 3 SRR017937 .312 16 chr20 43108717 37 76 M * 0 0 TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG ? ,@ A = A \\ < 5 = ,@ == A : BB @ = B9 (.; A @ B ; \\ > @ ABBB @ @9 BB @ : @5 \\ < BBBB9 ) \\ > BBB2 \\ < BBB @ BBB ? ;; BABBBBBBB @ For this example: QNAME = SRR017937.312 - this is the name of this read FLAG = 16 - see the format description below RNAME = chr20 - this read aligns to chromosome 20 POS = 43108717 - this read aligns the sequence on chr20 at position 43108717 MAPQ = 37 - this is quite a high quality score for the alignment (b/w 0 and 90) CIGAR = 76M - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that \u2018aligns\u2019 can mean \u2018aligns with mismatches\u2019 - mismatches that don\u2019t affect the alignment are not recorded in this field) MRNM = * - see the format description below MPOS = 0 as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*. ISIZE = 0 as there is no mate for this read SEQQuery = the 76bp sequence of the reference segment QUAL = per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file SAM format is described more fully here NOTE: reads are shown mapped to the \u201csense\u201d strand of the reference, and bases are listed in 5\u2019 -> 3\u2019 order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn for an overview of paired-end sequencing. SAM file in Galaxy","title":"Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#binary-sequence-alignment-map-format","text":"BAM format SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM. Data in a BAM file is binary and therefore can\u2019t be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file BAM file in IGV","title":"Binary Sequence Alignment Map format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-file-format","text":"What is VCF file: The Variant Call Format (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations. VCF consists of a header section and a data section. The header must contain a line starting with one \u2018#\u2018, showing the name of each field, and then the sample names starting at the 10 th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official VCF spec for a more rigorous description of the format. Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier. Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s). 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT.","title":"VCF file format"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-format-in-galaxy","text":"","title":"VCF format in Galaxy:"},{"location":"tutorials/var_detect_advanced/var_detect_advanced_background/#bcf-file-format","text":"","title":"Bcf file format:"},{"location":"tutorials/variant_calling_galaxy_1/","text":"PR reviewers and advice: Clare Sloggett, Khalid Mahmood, Jessica Chung, Simon Gladman Current slides: https://docs.google.com/presentation/d/18rjOlb3Q_i_IY65_4cA2UWp76uzAyrrcS6YJS2AzquE (these are Clare\u2019s from May 2017 workshop round - fairly cut-down) Other slides: Original main folder: https://drive.google.com/drive/u/0/folders/0B-gCKa6V4E0lelJGVDFGaHBLSlk","title":"Home"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/","text":"Introduction to Variant Calling using Galaxy \u00b6 Overview \u00b6 This tutorial is designed to introduce the tools, data types and workflow of variant detection. We will align reads to the genome, look for differences between reads and reference genome sequence, and filter the detected genomic variation manually to understand the computational basis of variant calling. We cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22. Learning Objectives \u00b6 At the end of the course, you will be able to: Work with the FASTQ format and base quality scores Align reads to generate a BAM file and subsequently generate a pileup file Run the FreeBayes variant caller to find SNVs and indels Visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye Requirements \u00b6 This workshop uses Galaxy as a platform. It is recommended that participants who have not used Galaxy before either sign up for our Intro to GVL workshop, or work through this tutorial themselves beforehand. This is a hands-on workshop and attendees are required to bring their own laptops. Background \u00b6 Some background reading material - background Where is the data in this tutorial from? The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project. 1. Preparation \u00b6 Make sure you have an instance of Galaxy ready to go. For example, you can use the Galaxy Australia server . Create a new history for this tutorial. In the history pane, click on the cog icon at the top right. Click Create New . Click on Unnamed history and re-name it. Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it\u2019s named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis. 2. Quality Control \u00b6 The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome! 1. Take a look at the FASTQ file \u00b6 Click on the eye icon to the top right of the FASTQ file to view the a snippet of the file. Note that each read is represented by 4 lines: * read identifier * short read sequence * separator * short read sequence quality scores 1 2 3 4 5 e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ . 2. Assessing read quality from the FASTQ files \u00b6 From the Galaxy tools panel, in the search box at the top, type in \u201cFastQC\u201d. Click on FastQC The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). This will be a file called FastQC on data 1: Web page . Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30). 3. Alignment to the reference - (FASTQ to BAM) \u00b6 The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome. 1. Align the reads with BWA \u00b6 We will map (align) the reads with the BWA tool to the human reference genome. For this tutorial, use Human reference genome 19 (hg19) - this is hg19 from UCSC . Align the reads [3-5mins]: from the Galaxy tools panel, search for Map with BWA-MEM From the options: Will you selection a reference genome\u2026: Use a built-in genome index Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file: from the Galaxy tools panel, search for SortSam From the options: Set the input file to be the output BAM file from the previous step. Sort by: set to Coordinate Keep other options as default and click execute 2. Examine the alignment \u00b6 To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, search for BAM-to-SAM From the options: BAM File to Convert: select your sorted BAM file Keep all options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon in the History pane next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc. 3. Assess the alignment data \u00b6 We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, search for the tool IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, search for Flagstat Select the sorted BAM file as input. Keep other options as default and click execute. Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant. 4. Visualise the BAM file with IGV \u00b6 To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \u201cDisplay with IGV web current \u201d. This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \u201c local \u201d instead of \u201cweb current\u201d, and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22 : 36 , 006 , 744 - 36 , 007 , 406 Can you see a few variants? Don\u2019t close IGV yet as we\u2019ll be using it later. 5. Generate a pileup file \u00b6 A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don\u2019t need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence. 1. Generate a pileup file \u00b6 From the Galaxy tools panel, search for Generate pileup From the options: Call consensus according to MAQ model: set to Yes This generates a called \u2018consensus base\u2019 for each chromosomal position. This would allow us to use this pileup directly for variant detection, if we wanted. Keep other options as default and click execute Galaxy tries to assign a datatype attribute to every output file. In this case, you\u2019ll need to manually set the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you\u2019re editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+\u2019 = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+\u2019 = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores 2. Filter the pileup file \u00b6 If you click the eye icon to view the contents of your pileup file, you\u2019ll see that the visible rows of the file aren\u2019t very interesting as they are outside chromosome 22 and have very low coverage. Let\u2019s filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, search for Filter Pileup From the options: which contains: set to Pileup with ten columns (with consensus). This is to match the Call consensus according to MAQ model option we selected earlier - the filter tool needs to know to expect the 10-column rather than 6-column format. Do not report positions with coverage lower than: set to 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row. 6. Call variants with FreeBayes \u00b6 FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial. 1. Call variants with FreeBayes. \u00b6 In the tool panel search for FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \u201cSimple diploid calling with filtering and coverage\u201d. This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. (This may take a while.) 2. Check the generated list of variants \u00b6 Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant\u2019s genomic location, REF and ALT describe the variant\u2019s nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes\u2019 confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification . 3. Visualise the variants and compare files \u00b6 Open the VCF file in IGV using the dataset\u2019s display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22 : 36 , 006 , 744 - 36 , 007 , 406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1 == \"chr22\" and c2 > 36006744 and c2 < 36007406 . 4. Optional: filter variants \u00b6 See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort : Filter tool we used above. 7. Further steps \u00b6 We\u2019ve seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"Variant Calling part 1 (Galaxy)"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#introduction-to-variant-calling-using-galaxy","text":"","title":"Introduction to Variant Calling using Galaxy"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#overview","text":"This tutorial is designed to introduce the tools, data types and workflow of variant detection. We will align reads to the genome, look for differences between reads and reference genome sequence, and filter the detected genomic variation manually to understand the computational basis of variant calling. We cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.","title":"Overview"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#learning-objectives","text":"At the end of the course, you will be able to: Work with the FASTQ format and base quality scores Align reads to generate a BAM file and subsequently generate a pileup file Run the FreeBayes variant caller to find SNVs and indels Visualise BAM files using the Integrative Genomics Viewer (IGV) and identify likely SNVs and indels by eye","title":"Learning Objectives"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#requirements","text":"This workshop uses Galaxy as a platform. It is recommended that participants who have not used Galaxy before either sign up for our Intro to GVL workshop, or work through this tutorial themselves beforehand. This is a hands-on workshop and attendees are required to bring their own laptops.","title":"Requirements"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#background","text":"Some background reading material - background Where is the data in this tutorial from? The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the 1000 genomes Genomes project.","title":"Background"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-preparation","text":"Make sure you have an instance of Galaxy ready to go. For example, you can use the Galaxy Australia server . Create a new history for this tutorial. In the history pane, click on the cog icon at the top right. Click Create New . Click on Unnamed history and re-name it. Import data for the tutorial. In this case, we are uploading a FASTQ file. Method 1 Paste/Fetch data from a URL to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . Click Paste/Fetch data and paste the following URL into the box https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). The dataset will have a very long name, as it\u2019s named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the Name box and give the file a shorter name by removing the URL. Then click Save . Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ): Method 2 Upload data to Galaxy. In the Galaxy tools panel (left), click on Get Data and choose Upload File . From Choose local file select the downloaded FASTQ file. Select Type as fastqsanger and click Start . Once the upload status turns green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right). Summary: So far, we have started a Galaxy instance, got hold of our data and uploaded it to the Galaxy instance. Now we are ready to perform our analysis.","title":"1. Preparation"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-quality-control","text":"The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!","title":"2. Quality Control"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-take-a-look-at-the-fastq-file","text":"Click on the eye icon to the top right of the FASTQ file to view the a snippet of the file. Note that each read is represented by 4 lines: * read identifier * short read sequence * separator * short read sequence quality scores 1 2 3 4 5 e.g. identifier: @61CC3AAXX100125:7:72:14903:20386/1 read sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT separator: + quality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6 For more details see FASTQ .","title":"1. Take a look at the FASTQ file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-assessing-read-quality-from-the-fastq-files","text":"From the Galaxy tools panel, in the search box at the top, type in \u201cFastQC\u201d. Click on FastQC The input FASTQ file will be selected by default. Keep the other defaults and click execute. Tip: Note the batch processing interface of Galaxy: grey = waiting in queue yellow = running green = finished red = tried to run and failed When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data). This will be a file called FastQC on data 1: Web page . Look at the various quality scores. The data looks pretty good - high Per base sequence quality (avg. above 30).","title":"2. Assessing read quality from the FASTQ files"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-alignment-to-the-reference-fastq-to-bam","text":"The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.","title":"3. Alignment to the reference - (FASTQ to BAM)"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-align-the-reads-with-bwa","text":"We will map (align) the reads with the BWA tool to the human reference genome. For this tutorial, use Human reference genome 19 (hg19) - this is hg19 from UCSC . Align the reads [3-5mins]: from the Galaxy tools panel, search for Map with BWA-MEM From the options: Will you selection a reference genome\u2026: Use a built-in genome index Using reference genome: set to hg19 Single or Paired-end reads: set to Single Make sure your fastq file is the input file. Keep other options as default and click execute. Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings Sort the BAM file: from the Galaxy tools panel, search for SortSam From the options: Set the input file to be the output BAM file from the previous step. Sort by: set to Coordinate Keep other options as default and click execute","title":"1. Align the reads with BWA"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-examine-the-alignment","text":"To examine the output sorted BAM file, we need to first convert it into readable SAM format. From the Galaxy tools panel, search for BAM-to-SAM From the options: BAM File to Convert: select your sorted BAM file Keep all options as default and click execute Examine the generated Sequence Alignment Map (SAM) file. Click the eye icon in the History pane next to the newly generated file Familiarise yourself with the SAM format Note that some reads have mapped to non-chr22 chromosomes (see column 3). This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant? Tip: Galaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs. This can be done as follows: Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.","title":"2. Examine the alignment"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-assess-the-alignment-data","text":"We can generate some mapping statistics from the BAM file to assess the quality of our alignment. Run IdxStats. From the Galaxy tools panel, search for the tool IdxStats Select the sorted BAM file as input. Keep other options as default and click execute. IdxStats generates a tab-delimited output with four columns. Each line consists of a reference sequence name (e.g. a chromosome), reference sequence length, number of mapped reads and number of placed but unmapped reads. We can see that most of the reads are aligning to chromosome 22 as expected. Run Flagstat. From the Galaxy tools panel, search for Flagstat Select the sorted BAM file as input. Keep other options as default and click execute. Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.","title":"3. Assess the alignment data"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-visualise-the-bam-file-with-igv","text":"To visualise the alignment data: Click on the sorted BAM file dataset in the History panel. Click on \u201cDisplay with IGV web current \u201d. This should download a .jnlp Java Web Start file to your computer. Open this file to run IGV. (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \u201c local \u201d instead of \u201cweb current\u201d, and this will open the BAM file in your current IGV session. Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded. Our reads for this tutorial are from chromosome 22, so select chr22 from the second drop box under the toolbar. Zoom in to view alignments of reads to the reference genome. Try looking at region chr22 : 36 , 006 , 744 - 36 , 007 , 406 Can you see a few variants? Don\u2019t close IGV yet as we\u2019ll be using it later.","title":"4. Visualise the BAM file with IGV"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#5-generate-a-pileup-file","text":"A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view. The current generation of variant calling tools do not output pileup files, and you don\u2019t need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.","title":"5. Generate a pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-generate-a-pileup-file","text":"From the Galaxy tools panel, search for Generate pileup From the options: Call consensus according to MAQ model: set to Yes This generates a called \u2018consensus base\u2019 for each chromosomal position. This would allow us to use this pileup directly for variant detection, if we wanted. Keep other options as default and click execute Galaxy tries to assign a datatype attribute to every output file. In this case, you\u2019ll need to manually set the datatype to pileup . First, rename the output to something more meaningful by clicking on the pencil icon To change the datatype, click on the Datatype link from the top tab while you\u2019re editing attributes. For downstream processing we want to tell Galaxy that this is a pileup file. From the drop-down, select Pileup and click save. Tip: The pileup file we generated has 10 columns: * 1. chromosome * 2. position * 3. current reference base * 4. consensus base from the mapped reads * 5. consensus quality * 6. SNV quality * 7. maximum mapping quality * 8. coverage * 9. bases within reads * 10. quality values Further information on (9): Each character represents one of the following (the longer this string, higher the coverage): . = match on forward strand for that base , = match on reverse strand ACGTN = mismatch on forward acgtn = mismatch on reverse +[0-9]+[ACGTNacgtn]+\u2019 = insertion between this reference position and the next -[0-9]+[ACGTNacgtn]+\u2019 = deletion between this reference position and the next ^ = start of read $ = end of read BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores","title":"1. Generate a pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-filter-the-pileup-file","text":"If you click the eye icon to view the contents of your pileup file, you\u2019ll see that the visible rows of the file aren\u2019t very interesting as they are outside chromosome 22 and have very low coverage. Let\u2019s filter to regions with coverage of at least 10 reads. From the Galaxy tools panel, search for Filter Pileup From the options: which contains: set to Pileup with ten columns (with consensus). This is to match the Call consensus according to MAQ model option we selected earlier - the filter tool needs to know to expect the 10-column rather than 6-column format. Do not report positions with coverage lower than: set to 10 Try this filtering step two ways: First, set the parameter Only report variants? to No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location. Then, repeat the step but set Only report variants? to Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores. Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.","title":"2. Filter the pileup file"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#6-call-variants-with-freebayes","text":"FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the Advanced Variant Calling tutorial.","title":"6. Call variants with FreeBayes"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-call-variants-with-freebayes","text":"In the tool panel search for FreeBayes . Select your sorted BAM file as input, and select the correct reference genome. Under Choose parameter selection level , select \u201cSimple diploid calling with filtering and coverage\u201d. This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on Galaxy-specific options . Execute FreeBayes. (This may take a while.)","title":"1. Call variants with FreeBayes."},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-check-the-generated-list-of-variants","text":"Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns. How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.) What sort of quality scores do your variants have? FreeBayes, like most variant callers, produces a Variant Call Format (VCF) file. VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers. The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant\u2019s genomic location, REF and ALT describe the variant\u2019s nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes\u2019 confidence in the correctness of this variant call. The columns in more detail are: Col Field Description 1 CHROM Chromosome name 2 POS 1-based position. For an indel, this is the position preceding the indel. 3 ID Variant identifier (optional). Usually the dbSNP rsID. 4 REF Reference sequence at POS involved in the variant. For a SNP, it is a single base. 5 ALT Comma delimited list of alternative sequence(s) seen in our reads. 6 QUAL Phred-scaled probability of all samples being homozygous reference. 7 FILTER Semicolon delimited list of filters that the variant fails to pass. 8 INFO Semicolon delimited list of variant information. 9 FORMAT Colon delimited list of the format of individual genotypes in the following fields. 10+ Sample(s) Individual genotype information defined by FORMAT. For even more detail on VCF files, you can look at the VCF format specification .","title":"2. Check the generated list of variants"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-visualise-the-variants-and-compare-files","text":"Open the VCF file in IGV using the dataset\u2019s display in IGV local link (using the web current link will open IGV again, and using local should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file. Take a look again at the same region as earlier: chr22 : 36 , 006 , 744 - 36 , 007 , 406 Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to c1 == \"chr22\" and c2 > 36006744 and c2 < 36007406 .","title":"3. Visualise the variants and compare files"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-optional-filter-variants","text":"See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the Filter and Sort : Filter tool we used above.","title":"4. Optional: filter variants"},{"location":"tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#7-further-steps","text":"We\u2019ve seen how to: Align the raw data (sequence reads) to a reference genome Generate variant calls from aligned reads Interpret the various file formats used in storing reads, alignments and variant calls Visualise the data using IGV For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants. We look at some further steps in the Advanced Variant Calling tutorial.","title":"7. Further steps"}]}